<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>The way to freedom</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-04-29T08:12:31.280Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Vent Lam</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[译]AMD 2.0 – MI450X 超越 Nvidia 的机会</title>
    <link href="http://example.com/2025/04/29/AMD%202.0%20%E2%80%93%20MI450X%20%E8%B6%85%E8%B6%8A%20Nvidia%20%E7%9A%84%E6%9C%BA%E4%BC%9A/"/>
    <id>http://example.com/2025/04/29/AMD%202.0%20%E2%80%93%20MI450X%20%E8%B6%85%E8%B6%8A%20Nvidia%20%E7%9A%84%E6%9C%BA%E4%BC%9A/</id>
    <published>2025-04-29T08:05:33.854Z</published>
    <updated>2025-04-29T08:12:31.280Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Nvidia-的新护城河-快速改进、开发者优先的方法、低-AMD-AI-软件工程师薪资、Python-DSL、UALink-灾难、MI325x、MI355x、MI430X-UL4、MI450X-架构、IF64-IF128、灵活的-IO、UALink、IFoE"><a href="#Nvidia-的新护城河-快速改进、开发者优先的方法、低-AMD-AI-软件工程师薪资、Python-DSL、UALink-灾难、MI325x、MI355x、MI430X-UL4、MI450X-架构、IF64-IF128、灵活的-IO、UALink、IFoE" class="headerlink" title="Nvidia 的新护城河 快速改进、开发者优先的方法、低 AMD AI 软件工程师薪资、Python DSL、UALink 灾难、MI325x、MI355x、MI430X UL4、MI450X 架构、IF64&#x2F;IF128、灵活的 IO、UALink、IFoE"></a>Nvidia 的新护城河 快速改进、开发者优先的方法、低 AMD AI 软件工程师薪资、Python DSL、UALink 灾难、MI325x、MI355x、MI430X UL4、MI450X 架构、IF64&#x2F;IF128、灵活的 IO、UALink、IFoE</h2><p>由迪伦·帕特尔、邓洪·陈、丹尼尔·尼什巴尔、韦加·朱和伊凡·恰姆撰写</p><p><img src="https://semianalysis.com/wp-content/uploads/2025/04/image-19-1.png"></p><p><em><sub>SemiAnalysis is expanding the AI engineering team! If you have an experience in PyTorch, training, inferencing, system modelling, SLURM&#x2F;Kubernetes, send us your resume and 5 bullet points demonstrating your engineering excellence to <a href="https://semianalysis.com/2025/04/23/amd-2-0-new-sense-of-urgency-mi450x-chance-to-beat-nvidia-nvidias-new-moat/">letsgo@semianalysis.com</a>.</sub></em></p><p>Ever since <a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/">SemiAnalysis published an article in December 2024 detailing mediocre AMD software and the lack of usability</a>, AMD has kicked into a higher gear and has made rapid progress in the past four months on many items we laid out. We view AMD’s new sense of urgency as a massive positive in its journey to catch up to Nvidia. AMD is now in a wartime stance, but there are still many battles ahead of it.<br>自从 SemiAnalysis 在 2024 年 12 月发布了一篇关于 AMD 软件平庸和可用性缺乏的文章以来，AMD 已经进入了更高的档次，并在过去四个月中在我们列出的许多项目上取得了快速进展。我们认为 AMD 的新紧迫感是其追赶 Nvidia 旅程中的一个巨大积极因素。AMD 现在处于战时状态，但仍然面临许多战斗。</p><p>In this report, we will discuss the many positive changes AMD has made. They are on the right track but need to increase the R&amp;D budget for GPU hours and make further investments in AI talent. We will provide additional recommendations and elaborate on AMD management’s blind spot: how they are uncompetitive in the race for AI Software Engineers due to compensation structure benchmarking to the wrong set of companies.<br>在本报告中，我们将讨论 AMD 所做的许多积极变化。他们走在正确的轨道上，但需要增加 GPU 小时的研发预算，并进一步投资于人工智能人才。我们将提供额外的建议，并详细阐述 AMD 管理层的盲点：由于薪酬结构基准错误的公司，他们在争夺人工智能软件工程师的竞争中处于不利地位。</p><p>We will also discuss how AMD’s product launch cadence has put their current generation products against Nvidia’s next-gen products. Launching the MI325X at the same time as B200 has led to mediocre customer interest. Customers are now comparing the 8-GPU MI355X to the rack-scale 72-GPU GB200 NVL72 solution. <a href="https://semianalysis.com/accelerator-industry-model/">Our demand view in the Accelerator Model tracked Microsoft’s disappointment early in 2024 and lack of follow-on orders for AMD GPUs</a>.<br>我们还将讨论 AMD 的产品发布节奏如何将他们当前一代产品与 Nvidia 的下一代产品进行对比。MI325X 与 B200 同时发布导致客户兴趣平平。客户现在将 8-GPU 的 MI355X 与机架规模的 72-GPU GB200 NVL72 解决方案进行比较。我们在加速器模型中的需求视图追踪了微软在 2024 年初的失望以及对 AMD GPU 缺乏后续订单的情况。</p><p>We now believe that, there is renewed interest in AMD GPUs from <a href="https://semianalysis.com/accelerator-industry-model/">OpenAI via Oracle</a> and <a href="https://semianalysis.com/accelerator-industry-model/">a few other major customers</a>, but still not Microsoft, on the condition that they reach a sweet-heart pricing with AMD. <strong>We will also outline how AMD’s window for fully catching up to NVIDIA could open in H2 2026, when AMD will finally bring a rack-scale solution to production. These SKUs, the MI450X IF64 and MI450X IF128, could be competitive to NVIDIA’s H2 2026 production rack-scale solution (VR200 NVL144).</strong><br>我们现在相信，OpenAI 通过 Oracle 和其他一些主要客户对 AMD GPU 的兴趣重新燃起，但仍然没有微软，前提是他们与 AMD 达成优惠定价。我们还将概述 AMD 在 2026 年下半年完全赶上 NVIDIA 的窗口何时会打开，当时 AMD 将最终将机架级解决方案投入生产。这些 SKU，MI450X IF64 和 MI450X IF128，可能会与 NVIDIA 在 2026 年下半年的生产机架级解决方案（VR200 NVL144）具有竞争力。</p><p>SemiAnalysis is actively working with NVIDIA and AMD on inference benchmarking on Hopper and CDNA3 class GPUs and will publish an article on comprehensive benchmarks and comparisons within the upcoming months.<br>SemiAnalysis 正在与 NVIDIA 和 AMD 积极合作，进行 Hopper 和 CDNA3 类 GPU 的推理基准测试，并将在未来几个月内发布一篇关于全面基准测试和比较的文章。</p><h2 id="Executive-Summary-执行摘要"><a href="#Executive-Summary-执行摘要" class="headerlink" title="Executive Summary 执行摘要"></a>Executive Summary 执行摘要</h2><ol><li>We met with Lisa Su to present our findings from the <a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training">December AMD article</a> and she acknowledged the many gaps in the ROCm software stack &amp; expressed a strong desire to improve.<br> 我们与 Lisa Su 会面，展示了我们在 12 月 AMD 文章中的发现，她承认了 ROCm 软件栈中的许多缺口，并表达了强烈的改进愿望。</li><li>Over the past four months, AMD has made rapid progress on its AI software stack.<br> 在过去的四个月里，AMD 在其 AI 软件栈上取得了快速进展。</li><li>In January 2025, AMD launched its developer relations (devrel) function, mainly led by Anush Elangovan, AMD’s AI Software Czar. His focus is interacting with external developers on Tech Twitter and In Real Life events.<br> 在 2025 年 1 月，AMD 推出了其开发者关系（devrel）职能，主要由 AMD 的人工智能软件负责人 Anush Elangovan 领导。他的重点是与外部开发者在技术推特和现实生活活动中互动。</li><li>In January 2025, AMD recognized that external developers community are what made CUDA great and has since adopted a Developer First strategy.<br> 在 2025 年 1 月，AMD 认识到外部开发者社区是使 CUDA 伟大的原因，因此采用了以开发者为先的战略。</li><li>Before SemiAnalysis published our AMD article in December, <a href="https://x.com/AnushElangovan/status/1877342554842345479">AMD had zero MI300X in PyTorch Continuous Integration&#x2F;Continuous Delivery (CI&#x2F;CD)</a>. AMD has since added MI300 into PyTorch CI&#x2F;CD. AMD has made great progress over the past four months on CI&#x2F;CD.<br> 在 SemiAnalysis 于 12 月发布我们的 AMD 文章之前，AMD 在 PyTorch 持续集成&#x2F;持续交付（CI&#x2F;CD）中没有 MI300X。自那时以来，AMD 已将 MI300 添加到 PyTorch CI&#x2F;CD 中。在过去四个月中，AMD 在 CI&#x2F;CD 方面取得了重大进展。</li><li>AMD plans to take a page out of <a href="https://sites.research.google/trc/about/">Google’s TPU Research Cloud (TRC) book</a> and launch a developer cloud at their upcoming Advancing AI event in June. The metric for success is if an <a href="https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/">GPT-J moment</a> happens on their AMD’s community developer cloud. <br> AMD 计划借鉴谷歌的 TPU 研究云（TRC）模式，并在他们即将于 6 月举行的“推进人工智能”活动中推出一个开发者云。成功的标准是 AMD 的社区开发者云上是否会出现一个 GPT-J 时刻。</li><li>AI Software Engineering compensation is AMD’s management’s blind spot. Their total compensation is significantly worse than companies that are great at AI software, such as NVIDIA and AI Labs.<br> AI 软件工程的薪酬是 AMD 管理层的盲点。他们的总薪酬明显低于在 AI 软件方面表现出色的公司，如 NVIDIA 和 AI Labs。</li><li>AMD’s internal development clusters have seen significant improvements over the past four months, yet these enhancements still fall short of what is needed to compete effectively in the long-term GPU development landscape.<br> AMD 的内部开发集群在过去四个月中取得了显著改善，但这些提升仍然不足以在长期 GPU 开发领域有效竞争。</li><li>AMD should considerably increase &amp; prioritize allocations of its investments into R+D CapEx and OpEx initiatives to give their teams significantly more GPU resources for software development. The current short-sighted focus on quarterly earnings compromises its capacity for long-term competitiveness. AMD needs to invest significantly more GPUs, they have less than 1&#x2F;20th of Nvidia’s total GPU count.<br> AMD 应该大幅增加并优先分配其在研发资本支出和运营支出项目上的投资，以为其团队提供更多的 GPU 资源用于软件开发。目前对季度收益的短视关注妨碍了其长期竞争力。AMD 需要显著投资更多的 GPU，他们的总 GPU 数量不到 Nvidia 的 1&#x2F;20。</li><li>Making the entire Cuda ecosystem first class on Python has been a top priority for Jensen. NVIDIA now has a pythonic interface at every level of the stack. ROCm does not have this. This is a serious threat to AMD’s usability long term with developers.<br>让整个 Cuda 生态系统在 Python 上成为一流一直是 Jensen 的首要任务。NVIDIA 现在在堆栈的每个层级都有一个 Python 接口。ROCm 没有这个。这对 AMD 在长期与开发者的可用性构成了严重威胁。</li><li>Although RCCL has made some decent progress, the delta between NCCL and RCCL continues to significantly widen due to the <a href="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72583/">new NCCL improvements and features announced at GTC 2025</a>.<br>尽管 RCCL 取得了一些不错的进展，但由于在 GTC 2025 上宣布的新 NCCL 改进和功能，NCCL 与 RCCL 之间的差距仍在显著扩大。</li><li>AMD has made some progress in the last four months on its software infrastructure layer (i.e., Kubernetes, SDC detectors, health checks, SLURM, Docker, metrics exporters), but its rate of progress is nowhere near keeping up with the rate of progress of AMD’s ML libraries.<br>在过去四个月中，AMD 在其软件基础设施层（即 Kubernetes、SDC 探测器、健康检查、SLURM、Docker、指标导出器）上取得了一些进展，但其进展速度远远跟不上 AMD 的 ML 库的进展速度。</li><li>AMD is currently lacking support for many inference features, such as good support for disaggregated prefill, Smart Routing, and NVMe KV Cache Tiering. NVIDIA <a href="https://github.com/ai-dynamo/dynamo">open-sourced Dynamo</a>, a distributed inference framework, further democratizing disaggregated serving for NVIDIA GPUs.<br>AMD 目前缺乏对许多推理功能的支持，例如对分散预填充、智能路由和 NVMe KV 缓存分层的良好支持。NVIDIA 开源了 Dynamo，一个分布式推理框架，进一步使 NVIDIA GPU 的分散服务民主化。</li><li>The MI355X is still not competitive with NVIDIA’s rack scale GB200 NVL72 solution. Instead, AMD is pitching MI355X as being competitive to NVIDIA’s air-cooled HGX solutions, but this is not the purchasing comparison most customers are making.<br>MI355X 仍然无法与 NVIDIA 的机架规模 GB200 NVL72 解决方案竞争。相反，AMD 将 MI355X 定位为与 NVIDIA 的风冷 HGX 解决方案竞争，但这并不是大多数客户所做的购买比较。</li><li>By 2H 2026, AMD’s MI450X rack-scale solution, if executed properly, could be competitive against Nvidia’s VR200 NVL144.<br>到 2026 年下半年，AMD 的 MI450X 机架级解决方案如果执行得当，可能会与 Nvidia 的 VR200 NVL144 竞争。</li><li>SemiAnalysis is expanding the AI engineering team! If you have an experience in PyTorch, training, inferencing, system modelling, SLURM&#x2F;Kubernetes, send us your resume and 5 bullet points demonstrating your engineering excellence to <a href="https://semianalysis.com/2025/04/23/amd-2-0-new-sense-of-urgency-mi450x-chance-to-beat-nvidia-nvidias-new-moat/">letsgo@semianalysis.com</a>.<br>SemiAnalysis 正在扩展 AI 工程团队！如果您在 PyTorch、训练、推理、系统建模、SLURM&#x2F;Kubernetes 方面有经验，请将您的简历和 5 个展示您工程卓越的要点发送至 <a href="mailto:&#108;&#x65;&#116;&#115;&#103;&#x6f;&#x40;&#x73;&#101;&#x6d;&#x69;&#x61;&#x6e;&#97;&#x6c;&#x79;&#x73;&#x69;&#x73;&#46;&#99;&#x6f;&#x6d;">&#108;&#x65;&#116;&#115;&#103;&#x6f;&#x40;&#x73;&#101;&#x6d;&#x69;&#x61;&#x6e;&#97;&#x6c;&#x79;&#x73;&#x69;&#x73;&#46;&#99;&#x6f;&#x6d;</a>。</li><li>AMD is hiring engineers across the whole software stack. Drop Anush a note at <a href="https://semianalysis.com/2025/04/23/amd-2-0-new-sense-of-urgency-mi450x-chance-to-beat-nvidia-nvidias-new-moat/">anush+letsgo@amd.com</a>.<br>AMD 正在招聘整个软件栈的工程师。请给 Anush 发个邮件，地址是 <a href="mailto:&#x61;&#x6e;&#x75;&#115;&#104;&#43;&#x6c;&#101;&#x74;&#x73;&#103;&#111;&#x40;&#97;&#109;&#100;&#46;&#99;&#111;&#x6d;">&#x61;&#x6e;&#x75;&#115;&#104;&#43;&#x6c;&#101;&#x74;&#x73;&#103;&#111;&#x40;&#97;&#109;&#100;&#46;&#99;&#111;&#x6d;</a>。</li></ol><h2 id="What’s-New-Since-our-December-AMD-Article-自我们-12-月的-AMD-文章以来有什么新变化？"><a href="#What’s-New-Since-our-December-AMD-Article-自我们-12-月的-AMD-文章以来有什么新变化？" class="headerlink" title="What’s New Since our December AMD Article?自我们 12 月的 AMD 文章以来有什么新变化？"></a>What’s New Since our December AMD Article?自我们 12 月的 AMD 文章以来有什么新变化？</h2><p>Hours after we dropped the AMD article, Lisa Su reached out to us to arrange a call with our engineering team to discuss in detail each of our findings and recommendations. The <a href="https://x.com/dylan522p/status/1871287937268383867">very next day at 7am PT</a>, we presented our findings to Lisa and walked her through our experience during the prior five months working with the AMD team to try to fix their software to carry out various workload benchmarks.<br>在我们发布 AMD 文章后的几个小时，Lisa Su 与我们联系，安排与我们的工程团队进行电话会议，详细讨论我们的发现和建议。第二天早上 7 点（太平洋时间），我们向 Lisa 展示了我们的发现，并向她介绍了在过去五个月与 AMD 团队合作修复他们的软件以进行各种工作负载基准测试的经历。</p><p>We showed her dozens of bug reports our team submitted to our AMD engineering contacts. She was sympathetic to end users facing an unpleasant experience on ROCm and acknowledged the many gaps in the ROCm software stack. Furthermore, Lisa Su and her team expressed a strong desire for AMD to do better. To this end, for the next hour and a half, Lisa asked her engineering team and our engineers numerous detailed questions regarding our experience and our key recommendations.<br>我们向她展示了我们团队提交给 AMD 工程联系人的数十个错误报告。她对面临 ROCm 不愉快体验的最终用户表示同情，并承认 ROCm 软件堆栈中存在许多缺陷。此外，Lisa Su 及其团队表达了 AMD 做得更好的强烈愿望。为此，在接下来的一个半小时里，Lisa 向她的工程团队和我们的工程师提出了许多关于我们的经验和关键建议的详细问题。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-1.-AMD-tweet-with-Lisa.png?resize=952,1024&ssl=1"></p><p>Source: X 来源：X</p><p>This tone shift from the top has resonated across the organization. <strong>AMD is now in wartime mode</strong>, it is addressing software gaps and is working hard on closing them. This is a huge change from when AMD’s PR department in 2024 <em>would not publicly acknowledge</em> any major issues with software.<br>这种来自高层的语气转变在整个组织中产生了共鸣。AMD 现在处于战时模式，正在解决软件差距，并努力弥补这些差距。这与 AMD 在 2024 年时公关部门不会公开承认任何重大软件问题的情况相比，发生了巨大的变化。</p><p>In 2025 thus far, AMD has acknowledged that their software has way more bugs are than Nvidia currently but are rapidly improving and are engaging the community to bring ROCm to parity. In particular, Anush Elangovan, the AMD AI Software Tzar, has been active in tackling the issues at AMD.<br>截至 2025 年，AMD 已承认他们的软件存在比 Nvidia 更多的错误，但正在迅速改进，并积极与社区合作以使 ROCm 达到平衡。特别是，AMD AI 软件负责人 Anush Elangovan 一直在积极解决 AMD 的问题。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-2.-Anush-Tweet.png?fit=1024,632&ssl=1"></p><p>Source: X 来源：X</p><h2 id="AMD’s-Culture-Shift-–-A-Renewed-Sense-of-UrgencyAMD-的文化转变-重新焕发的紧迫感"><a href="#AMD’s-Culture-Shift-–-A-Renewed-Sense-of-UrgencyAMD-的文化转变-重新焕发的紧迫感" class="headerlink" title="AMD’s Culture Shift – A Renewed Sense of UrgencyAMD 的文化转变 - 重新焕发的紧迫感"></a>AMD’s Culture Shift – A Renewed Sense of UrgencyAMD 的文化转变 - 重新焕发的紧迫感</h2><p>Acceptance is the final stage of grief. AMD has finally accepted its massive software gap and can now address its issues for its chance to beat NVIDIA in the software and hardware game. AMD has caught a second wind, <a href="https://nypost.com/2024/08/26/business/nvidia-employees-can-work-7-days-a-week-until-2-a-m-but-few-leave-because-of-ai-chip-giants-lavish-pay-report/">but Nvidia is still at an all-out sprint</a>, and AMD must match them and even outpace them to catch up. Nvidia’s staff clearly recognize that sometimes <a href="https://nypost.com/2024/08/26/business/nvidia-employees-can-work-7-days-a-week-until-2-a-m-but-few-leave-because-of-ai-chip-giants-lavish-pay-report/">extra hours are needed for Nvidia</a> to continue to be a leader in a competitive market. For AMD to win, it needs to work at least as hard and smart as Nvidia if not harder and smarter. We see clear signs of this starting to happen.<br>接受是悲伤的最后阶段。AMD 终于接受了其巨大的软件差距，现在可以解决其问题，以便在软件和硬件领域有机会击败 NVIDIA。AMD 重新焕发了活力，但 Nvidia 仍在全力冲刺，AMD 必须与他们匹敌，甚至超越他们以赶上。Nvidia 的员工显然认识到，有时需要额外的工作时间，以便 Nvidia 在竞争激烈的市场中继续保持领先地位。为了赢得胜利，AMD 需要至少与 Nvidia 一样努力和聪明，甚至更努力和更聪明。我们看到这一切开始发生的明确迹象。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-3.-Anush-Tweet.png?fit=1024,929&ssl=1"></p><p>Source: X 来源：X</p><p>There plenty of examples of the focused and hungry teams catching up. xAI vs OpenAI is an example of how a shift in culture to adopt a sense of urgency and take a wartime stance can lead to a company catching up with the competition at a ludicrous speed.<br>有很多专注且渴望的团队追赶上来的例子。xAI 与 OpenAI 就是一个例子，说明文化的转变以采纳紧迫感并采取战时立场如何能使公司以惊人的速度赶上竞争对手。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-4.-Grok-AI.png?fit=1024,530&ssl=1"></p><p>Source: xAI 来源：xAI</p><p>We see many concrete examples that show this dynamic starting to play out at AMD. One area where AMD was executing is their product roadmap to achieve parity with Nvidia on rack-scale solutions. We explain our estimates of their rack-scale MI450X solution later in the article.<br>我们看到许多具体例子表明这种动态在 AMD 开始显现。AMD 执行的一个领域是他们的产品路线图，以在机架规模解决方案上与 Nvidia 实现平价。我们将在文章后面解释他们的机架规模 MI450X 解决方案的估计。</p><p>Another example is AMD’s rapid progress on its AI software stack over the past four months. They have been showing significant improvements in both their <a href="https://rocm.blogs.amd.com/artificial-intelligence/training_rocm_pt/README.html">training</a> and <a href="https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1_Perf/README.html">inference performance</a>, as well as their out-of-the-box experience. <a href="https://www.linkedin.com/feed/update/urn:li:activity:7292299941761187840">They even adopted SemiAnalysis code for their training benchmark</a>.<br>另一个例子是 AMD 在过去四个月中其 AI 软件堆栈的快速进展。他们在训练和推理性能以及开箱体验方面都显示出了显著的改善。他们甚至采用了 SemiAnalysis 代码作为他们的训练基准。</p><p>In January 2025, AMD launched its developer relations (devrel) function as it has finally understood that developers are what made Nvidia’s CUDA great and now acknowledges that winning over developers will be critical to the success of ROCm. Currently, the developer relations team is led by Anush Elangovan, who is acting as the sole devrel for AMD at in-person events as well as on <a href="https://x.com/AnushElangovan">social media forums such as Tech Twitter</a>.<br>在 2025 年 1 月，AMD 推出了其开发者关系（devrel）职能，因为它终于明白开发者是让 Nvidia 的 CUDA 伟大的关键，现在承认赢得开发者对 ROCm 的成功至关重要。目前，开发者关系团队由 Anush Elangovan 领导，他在面对面活动以及社交媒体论坛如 Tech Twitter 上担任 AMD 的唯一 devrel。</p><p>AMD is going even further on the developer front. In June, AMD will launch a developer cloud focused on engaging with the overarching community. This is a direct result of SemiAnalysis’s suggestion for AMD to close the gap.<br>AMD 在开发者方面更进一步。AMD 将于六月推出一个专注于与整体社区互动的开发者云。这是 SemiAnalysis 建议 AMD 缩小差距的直接结果。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-5.-Vamsi-quote.png?fit=1024,396&ssl=1"></p><p>Source: SemiAnalysis, AMD 来源：SemiAnalysis，AMD</p><p>AMD now beats Nvidia in the reproducibility of benchmarks and performance claims. AMD has started posting easy-to-follow instructions that allow users and developers to replicate their benchmarking runs instead of publishing unrealistic or biased performance claims that can’t be verified. A great example is how <a href="https://rocm.blogs.amd.com/artificial-intelligence/reproducing-amd-mlperf-inference-submission/README.html">AMD published a great blog on how to reproduce their MLPerf Inference 5.0 submission</a>. NVIDIA did not provide such instructions as part of the latest MLPerf exercise.<br>AMD 现在在基准测试和性能声明的可重复性方面超过了 Nvidia。AMD 开始发布易于遵循的说明，允许用户和开发者复制他们的基准测试运行，而不是发布无法验证的不切实际或有偏见的性能声明。一个很好的例子是 AMD 发布了一篇关于如何重现他们的 MLPerf Inference 5.0 提交的优秀博客。NVIDIA 在最新的 MLPerf 练习中没有提供这样的说明。</p><h2 id="What-Makes-CUDA-Great-CUDA-有什么优点？"><a href="#What-Makes-CUDA-Great-CUDA-有什么优点？" class="headerlink" title="What Makes CUDA Great?CUDA 有什么优点？"></a>What Makes CUDA Great?CUDA 有什么优点？</h2><p>CUDA’s greatest advantage isn’t just its internal software developers but also the ecosystem that includes 4 million external developers building on the CUDA platform, thousands of enterprises, and numerous AI Labs and startups. This creates a self‑reinforcing flywheel of tools, tutorials, and ready‑made kernels that lowers the barrier to adoption for every newcomer and keeps veterans moving fast. Due to this massive developer base, tribal knowledge is quickly shared with newcomers.<br>CUDA 的最大优势不仅在于其内部软件开发人员，还在于包括 400 万外部开发人员在内的生态系统，这些开发人员在 CUDA 平台上进行开发，还有成千上万的企业以及众多的人工智能实验室和初创公司。这创造了一个自我强化的飞轮，提供工具、教程和现成的内核，降低了每个新手的采用门槛，并使老手能够快速前进。由于这个庞大的开发者基础，部落知识迅速与新手分享。</p><p>The result of this thriving ecosystem is that breakthrough ideas—whether new attention algorithms, state‑space models or high‑throughput serving engines—almost always appear on CUDA first, receive feedback sooner and get tuned harder on CUDA, which in turn attracts the next wave of developers.<br>这个蓬勃发展的生态系统的结果是，突破性的想法——无论是新的注意力算法、状态空间模型还是高吞吐量服务引擎——几乎总是首先出现在 CUDA 上，获得更快的反馈，并在 CUDA 上进行更严格的调整，这反过来又吸引了下一波开发者。</p><p>The pay‑off from that collective energy is clear: when researchers publish a game‑changing kernel, the CUDA version typically launches day‑one. Tri Dao’s FlashAttention shipped reference CUDA code, and it took multiple quarters for ROCm to implement their own optimized attention. The selective‑state‑space model was the same, the authors only released a CUDA implementation but the authors themselves did not support or port it over to ROCm. The port of ROCm mamba was from AMD internal engineers and not the original authors. On the serving side, UC Berkeley’s vLLM and SGLang have their maintainers develop mainly on NVIDIA GPUs, and only after the CUDA path is stable do the maintainers then help AMD internal developers port to ROCm.<br>这种集体能量的回报是显而易见的：当研究人员发布一个改变游戏规则的内核时，CUDA 版本通常会在第一天推出。Tri Dao 的 FlashAttention 发布了参考 CUDA 代码，而 ROCm 实现他们自己的优化注意力花费了多个季度。选择性状态空间模型也是如此，作者只发布了 CUDA 实现，但作者本人并没有支持或移植到 ROCm。ROCm mamba 的移植是由 AMD 内部工程师完成的，而不是原作者。 在服务端，UC Berkeley 的 vLLM 和 SGLang 的维护者主要在 NVIDIA GPU 上开发，只有在 CUDA 路径稳定后，维护者才会帮助 AMD 内部开发人员移植到 ROCm。</p><p>Another example is how bugs are discovered and patched faster thanks to the millions of external CUDA ecosystem developers. In contrast, on ROCm, it may take months before a bug is discovered, such as was the case with numerous bugs we discovered and reported last year. For example, ROCm’s torch.scaled_dot_product_attention API in 2024. Attention is the most important layer in start of the art transformer models.<br>另一个例子是，由于数百万外部 CUDA 生态系统开发者，错误被发现和修复的速度更快。相比之下，在 ROCm 上，发现一个错误可能需要几个月的时间，例如我们去年发现并报告的众多错误。比如，ROCm 的 torch.scaled_dot_product_attention API 在 2024 年。注意力是最先进的变换器模型中最重要的层。</p><h2 id="Developers-Developers-Developers开发者，开发者，开发者"><a href="#Developers-Developers-Developers开发者，开发者，开发者" class="headerlink" title="Developers, Developers, Developers开发者，开发者，开发者"></a>Developers, Developers, Developers开发者，开发者，开发者</h2><p>Since January 2025 AMD has been vocal about a developers first approach, echoing Steve Ballmer’s famous mantra and mirroring Jensen’s approach too. At <a href="https://www.youtube.com/watch?v=RAK3Ce0RXgM&ab_channel=TensorWave">TensorWave’s “Beyond CUDA 2025” summit</a>, Anush, AMD’s AI Software Tzar, framed ROCm’s future with three words—“ <a href="https://www.youtube.com/watch?v=Vhh_GeBPOhs&ab_channel=MrWueb007"><strong>developers, developers, developers”</strong></a> We believe this developers-first approach and messaging will be amplified on a broader stage at AMD’s June keynote event. AMD has finally understood that what made CUDA unbeatable was not just great silicon but a swarm of external developers. We feel extremely positive about AMD’s new developers-first approach.<br>自 2025 年 1 月以来，AMD 一直在强调以开发者为先的方法，呼应了史蒂夫·巴尔默的著名口号，并且也反映了詹森的做法。在 TensorWave 的“超越 CUDA 2025”峰会上，AMD 的 AI 软件负责人 Anush 用三个词框定了 ROCm 的未来——“开发者，开发者，开发者”。我们相信这种以开发者为先的方法和信息将在 AMD 6 月的主题演讲中得到更广泛的传播。AMD 终于明白，CUDA 之所以无与伦比，不仅仅是因为出色的硅芯片，还有一大批外部开发者。我们对 AMD 的新开发者优先方法感到非常积极。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-6.-developers.gif?resize=240,180&ssl=1"></p><p>Source: Microsoft 来源：微软</p><p>In January 2025, Anush took this developers-first approach by engaging with external ROCm developers on Tech Twitter &amp; GitHub collecting feedback, being customer support whenever ROCm craps out (which is often), and personally answering questions. This hands‑on engagement is real progress, but AMD’s developer relations still runs on a skeleton crew; aside from Anush, AMD has basically zero full‑time dev‑rel engineers. AMD has started hiring <a href="https://careers.amd.com/careers-home/jobs/63017?lang=en-us">full time developer relations engineers</a>, but to close the gap with NVIDIA’s army of evangelists, the company will need at least 20+ devrel engineers that host regular in‑person hackathons and meetups.<br>在 2025 年 1 月，Anush 采取了这种以开发者为先的方式，通过在 Tech Twitter 和 GitHub 上与外部 ROCm 开发者互动，收集反馈，成为 ROCm 出现问题时的客户支持（这种情况经常发生），并亲自回答问题。这种亲身参与是真正的进展，但 AMD 的开发者关系仍然由一支骨干团队运作；除了 Anush，AMD 基本上没有全职的开发者关系工程师。AMD 已经开始招聘全职开发者关系工程师，但为了缩小与 NVIDIA 的宣传团队之间的差距，公司需要至少 20 名以上的开发者关系工程师，定期举办面对面的黑客马拉松和聚会。</p><p>NVIDIA’s annual GTC developer conference, <em>“Super Bowl of AI”</em> packs more than 500 developer focused deep‑dive sessions and hands‑on labs into a single week. Those tracks—covering every layer of the stack from PyTorch to JAX to CUTLASS to CUDA C++ to Assembly to profiling tools —give external developers a reliable place to learn and push the frontier.<br>NVIDIA 每年的 GTC 开发者大会，“人工智能的超级碗”，在一周内安排了超过 500 个面向开发者的深度研讨会和实践实验室。这些课程涵盖了从 PyTorch 到 JAX，再到 CUTLASS、CUDA C++、汇编语言和性能分析工具的每一层堆栈，为外部开发者提供了一个可靠的学习和推动前沿的地方。</p><p>AMD, by contrast, still lacks a GTC style developer conference that has many developer focused sessions. The company’s June “Advancing AI” event will be great for roadmap reveals, but the event is essentially a few product keynotes plus a handful of prerecorded talks—nowhere near the multi‑track developer sessions, code‑lab depth developers get at GTC. If AMD is serious about its new developer‑first stance, it should launch an annual, in‑person ROCm Developer Conference: three to four days of parallel tracks that cover kernel authoring, graph compilers, HIP&#x2F;Triton migration, MI300 cluster tuning, and real‑time debugging with the ROCm toolchain. Pair those talks with on‑site hackathons run by a beefed‑up (20‑plus‑strong) devrel team and follow‑up regional roadshows, and ROCm users would finally have a venue to share war stories, surface blocking bugs, and build the social fabric that made GTC indispensable to the CUDA community.<br>与此相比，AMD 仍然缺乏一个 GTC 风格的开发者大会，后者有许多以开发者为中心的会议。该公司的 6 月“推进 AI”事件将非常适合路线图的揭示，但该事件本质上只是几个产品主题演讲加上一些预录的演讲——远不及 GTC 上多轨道的开发者会议和代码实验室的深度。如果 AMD 认真对待其新的以开发者为首的立场，它应该启动一个年度的、面对面的 ROCm 开发者大会：为期三到四天的平行会议，涵盖内核编写、图形编译器、HIP&#x2F;Triton 迁移、MI300 集群调优以及使用 ROCm 工具链的实时调试。将这些演讲与由增强的（20 人以上）开发者关系团队主办的现场黑客马拉松和后续的区域路演结合起来，ROCm 用户最终将有一个分享战斗故事、发现阻塞错误以及建立社交网络的场所，这使得 GTC 对 CUDA 社区不可或缺。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-7.-CUDA-developer-session.png?fit=1024,562&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>Although George Hotz could have settled for AMD’s earlier offer of cloud‑hosted MI300X systems with full BMC access, he insisted on physical hardware so he could <em>“hack the metal”</em> directly. AMD initially balked—even though Hotz’s goal was to help open‑source tooling on their GPUs. The stalemate turned into a public spectacle when the widely respected PyTorch co‑creator, Soumith Chintala, tweeted in support of Geohotz receiving the physical boxes.<br>尽管乔治·霍茨本可以接受 AMD 早期提供的具有完整 BMC 访问权限的云托管 MI300X 系统，但他坚持要物理硬件，以便能够“直接破解金属”。AMD 最初对此感到犹豫——尽管霍茨的目标是帮助在他们的 GPU 上开源工具。僵局在广受尊敬的 PyTorch 联合创始人 Soumith Chintala 在推特上支持 Geohotz 获得物理设备时变成了一场公众盛事。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-8.-Chintala-Tweet.png?fit=1024,519&ssl=1"></p><p>Source: X 来源：X</p><p>We believe that this nudge worked, and a <a href="https://geohot.github.io/blog/jekyll/update/2025/03/08/AMD-YOLO.html">Geohotz March 8 blog</a> revealed that AMD had relented, sending him the two MI300X boxes. With this, AMD finally passed <em>Geohotz’s “cultural test.”</em> For AMD, this is arguably a bigger reputational coup than a technical one—shipping real silicon to a high‑profile hacker signals a newfound, developer‑first ethos that marketing dollars alone can’t buy, and it finally turns a bruising Twitter saga into a story demonstrating AMD’s new developer-first ethos.<br>我们相信这个推动有效，Geohotz 3 月 8 日的博客透露，AMD 已经让步，向他发送了两个 MI300X 盒子。通过这一点，AMD 终于通过了 Geohotz 的“文化测试”。对 AMD 来说，这无疑是一个比技术上的更大的声誉胜利——向一位高知名度的黑客发货真实的硅片，传达了一种新的以开发者为先的理念，这不是仅靠营销资金所能买到的，并且它最终将一场痛苦的 Twitter 传奇转变为展示 AMD 新开发者优先理念的故事。</p><p>In addition to sending Geohotz boxes, we believe that AMD can also score another easy reputation and marketing win by donating physical AMD GPU boxes to academic labs. <a href="https://x.com/haozhangml/status/1914439713332863348?s=46">Jensen &amp; Ian Buck has had a long history of donating GPUs to academics labs even going back as far as 2014</a>. This year, Jensen continues to supporting academics labs such as <a href="https://x.com/scsatcmu/status/1912910889566490821?s=46">CMU’s Catalyst Labs</a>, <a href="https://x.com/vllm_project/status/1893001644037566610">Berekely’s Sky labs</a>, <a href="https://x.com/haoailab">UCSD’s HaoAI Lab</a> and others for some time by donating physical gold plated B200 boxes to them in addition to providing free cloud access to NVIDIA GPUs.<br>除了发送 Geohotz 盒子，我们相信 AMD 还可以通过向学术实验室捐赠实体 AMD GPU 盒子来获得另一个轻松的声誉和市场营销胜利。Jensen 和 Ian Buck 在向学术实验室捐赠 GPU 方面有着悠久的历史，甚至可以追溯到 2014 年。今年，Jensen 继续支持学术实验室，如 CMU 的 Catalyst Labs、Berekely 的 Sky Labs、UCSD 的 HaoAI Lab 等，向他们捐赠实体镀金 B200 盒子，并提供免费的 NVIDIA GPU 云访问。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-9.-CMU-Tweet.png?fit=1020,932&ssl=1"></p><p>Source: X 来源：X</p><h2 id="Continuous-Integration-Continuous-Deployment-CI-CD-持续集成-持续部署-CI-CD"><a href="#Continuous-Integration-Continuous-Deployment-CI-CD-持续集成-持续部署-CI-CD" class="headerlink" title="Continuous Integration &#x2F; Continuous Deployment (CI&#x2F;CD)持续集成 &#x2F; 持续部署 (CI&#x2F;CD)"></a>Continuous Integration &#x2F; Continuous Deployment (CI&#x2F;CD)持续集成 &#x2F; 持续部署 (CI&#x2F;CD)</h2><p>Before SemiAnalysis published the AMD article in December, <a href="https://x.com/AnushElangovan/status/1877342554842345479">AMD had zero MI300X participating in PyTorch CI&#x2F;CD</a>. AMD has since added MI300 into PyTorch CI&#x2F;CD. AMD had been well known for providing buggy software – adding MI300 into PyTorch CI will go a long way towards this effort of continuing to rid AMD’s software of bugs!<br>在 SemiAnalysis 于 12 月发布 AMD 文章之前，AMD 在 PyTorch CI&#x2F;CD 中没有参与 MI300X。此后，AMD 已将 MI300 添加到 PyTorch CI&#x2F;CD 中。AMD 一直以提供有缺陷的软件而闻名——将 MI300 添加到 PyTorch CI 将大大推动消除 AMD 软件中的错误的努力！</p><p>Previously, AMD’s did not want to spend money on investing in CI&#x2F;CD resources, but we believe that this stance has changed over the past four months. At the ROCm SF developer meetup events, an AMD software engineer walked up to us, thanked us, and told us that they now have CI resources due to our efforts.<br>之前，AMD 不想在 CI&#x2F;CD 资源上花钱，但我们相信这种立场在过去四个月中发生了变化。在 ROCm SF 开发者见面会上，一位 AMD 软件工程师走到我们面前，感谢我们，并告诉我们他们现在有了 CI 资源，这要归功于我们的努力。</p><p>In addition to unit test CI, AMD has also enabled MI300 on TorchInductor Performance CI such that <a href="https://x.com/AnushElangovan/status/1884727132477382915">performance is tracked in inductor &#x2F;torch.compile commit</a>. NVIDIA has only provided the A100 in this CI and has not even provided any H100s or B200s for torchinductor perf CI. For this specific compile CI, AMD is ahead of NVIDIA. However – there is still much progress to be had as AMD’s dynamic shapes torch.compile is only at a 77% pass rate compared to Nvidia’s 90%+.<br>除了单元测试 CI，AMD 还在 TorchInductor 性能 CI 上启用了 MI300，以便在 inductor&#x2F;torch.compile 提交中跟踪性能。NVIDIA 在此 CI 中仅提供了 A100，甚至没有提供任何 H100 或 B200 用于 torchinductor 性能 CI。在这个特定的编译 CI 中，AMD 领先于 NVIDIA。然而，仍然有很大的进步空间，因为 AMD 的动态形状 torch.compile 的通过率仅为 77%，而 Nvidia 的通过率超过 90%。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-12.-PyTorch.png?fit=1024,559&ssl=1"></p><p>Source: PyTorch 来源：PyTorch</p><p>AMD should build on this progress by open sourcing and making all of their CI&#x2F;CD and dashboards publicly accessible such that anyone can see the pass rate of AMD’s software across all ROCm libraries (HipBLASLt, Sglang, vLLM,TransformerEngine, etc). <a href="https://hud.pytorch.org/benchmark/compilers">Currently, the only publicly accessible ROCm CI for their ML libraries is PyTorch</a>.<br>AMD 应该在此基础上进一步发展，开源并使其所有的 CI&#x2F;CD 和仪表板公开可访问，以便任何人都可以查看 AMD 软件在所有 ROCm 库（HipBLASLt、Sglang、vLLM、TransformerEngine 等）中的通过率。目前，唯一公开可访问的 ROCm CI 是他们的 ML 库 PyTorch。</p><h2 id="Upcoming-Community-Developer-Cloud即将推出的社区开发者云"><a href="#Upcoming-Community-Developer-Cloud即将推出的社区开发者云" class="headerlink" title="Upcoming Community Developer Cloud即将推出的社区开发者云"></a>Upcoming Community Developer Cloud即将推出的社区开发者云</h2><p>One of the reasons that Google’s TPU was able to gain external developer adoption was due to TPU’s free Colab access and because it provided large cluster access via <a href="https://sites.research.google/trc/about/">TPU Research Cloud</a> (TRC). This allowed the community quick and free access, enabling many interesting projects as shown in <a href="https://sites.research.google/trc/spotlight/">TRC’s spotlight</a> and <a href="https://sites.research.google/trc/publications/">the many papers published as part of TRC</a>. In fact, in 2020, well before the ChatGPT moment, a high schooler was able to train a model that was competitive with GPT-2 on a relatively giant TPU pod for free. In addition to providing plenty of smaller 8-16 chip pods, TRC also regularly provides free 1-2-week access to 1000+ chip pods to researchers.<br>谷歌的 TPU 能够获得外部开发者采用的原因之一是 TPU 提供了免费的 Colab 访问，并且通过 TPU Research Cloud (TRC)提供了大型集群访问。这使得社区能够快速且免费地访问，从而启用了许多有趣的项目，如 TRC 的聚光灯和作为 TRC 一部分发布的许多论文所示。事实上，在 2020 年，远在 ChatGPT 时刻之前，一名高中生能够在一个相对巨大的 TPU pod 上免费训练出与 GPT-2 竞争的模型。除了提供大量较小的 8-16 芯片 pod 外，TRC 还定期向研究人员提供 1000+芯片 pod 的免费 1-2 周访问。</p><p>The <a href="https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/">famous open source GPT-J model</a> was also trained on TPUs for free leading to an complete open repo about how to use TPUs with JAX and furthering external community adoption of TPUs. TRC has been wildly successful at promoting the TPU and supporting the open-source community.<br>著名的开源 GPT-J 模型也在 TPU 上免费训练，导致了一个关于如何使用 TPU 与 JAX 的完整开放仓库，并进一步推动了外部社区对 TPU 的采用。TRC 在推广 TPU 和支持开源社区方面取得了巨大的成功。</p><p>AMD’s developer cloud plans essentially take a page out of Google’s book. We believe that if AMD properly invests in enough GPUs for this developer cloud program such that its GPUs easily and freely accessible, AMD’s developer cloud will be able to help it broaden adoption of its GPUs. This is a key battle AMD must win in its race against NVIDIA.<br>AMD 的开发者云计划基本上借鉴了谷歌的做法。我们相信，如果 AMD 为这个开发者云项目适当投资足够的 GPU，使其 GPU 易于且自由地访问，AMD 的开发者云将能够帮助其扩大 GPU 的采用。这是 AMD 在与 NVIDIA 竞争中必须赢得的关键战役。</p><p><strong>The metric for success is if an GPT-J moment happens on their AMD’s community developer cloud.<br>成功的标准是是否在他们的 AMD 社区开发者云上发生 GPT-J 时刻。</strong></p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-12.1-Vamsi-quote.png?fit=1024,396&ssl=1"></p><p>Source: SemiAnalysis, AMD 来源：SemiAnalysis，AMD</p><h2 id="AMD-Management’s-Blind-Spot-–-AMD-AI-Software-Engineer-CompensationAMD-管理层的盲点-AMD-AI-软件工程师薪酬"><a href="#AMD-Management’s-Blind-Spot-–-AMD-AI-Software-Engineer-CompensationAMD-管理层的盲点-AMD-AI-软件工程师薪酬" class="headerlink" title="AMD Management’s Blind Spot – AMD AI Software Engineer CompensationAMD 管理层的盲点 - AMD AI 软件工程师薪酬"></a>AMD Management’s Blind Spot – AMD AI Software Engineer CompensationAMD 管理层的盲点 - AMD AI 软件工程师薪酬</h2><p>AMD faces a critical challenge in its AI software division due to non-competitive compensation, significantly impacting its ability to attract and retain top talent. Other companies that are known for developing great AI software pay significantly better than AMD.<br>AMD 在其人工智能软件部门面临着一个关键挑战，原因是薪酬不具竞争力，这严重影响了其吸引和留住顶尖人才的能力。其他以开发优秀人工智能软件而闻名的公司支付的薪酬远高于 AMD。</p><p>While compensation isn’t everything, it remains a significant factor influencing engineers’ decisions. Engineers often evaluate career opportunities based on multiple factors, including technical challenges, workplace culture, and growth opportunities. However, competitive compensation remains critical, particularly in highly specialized fields such as AI software engineering.<br>虽然薪酬不是一切，但它仍然是影响工程师决策的重要因素。工程师通常根据多个因素评估职业机会，包括技术挑战、工作场所文化和成长机会。然而，竞争性的薪酬仍然至关重要，特别是在像人工智能软件工程这样的高度专业化领域。</p><p>It is well known amongst AI engineers that the total compensation packages at AMD, which include base salary, RSUs (Restricted Stock Units), and bonuses, lag considerably behind competitors like NVIDIA, Tesla Dojo, <a href="https://openai.com/careers/hardwaresoftware-co-design-engineer/">OpenAI Chip Team</a>, Google TPU, and xAI, etc.<br>在人工智能工程师中，众所周知，AMD 的总薪酬方案，包括基本工资、RSU（限制性股票单位）和奖金，远远落后于竞争对手，如 NVIDIA、Tesla Dojo、OpenAI Chip Team、Google TPU 和 xAI 等。</p><p>In conversations with top AI software engineers about why they chose not to join AMD, many highlighted that working at AMD software feels like working on porting features that NVIDIA engineers developed two years earlier. In contrast, NVIDIA provides engineers with the opportunity to work on cutting-edge software and to build software for chips used in state-of-the-art models like o3, for both training and inference.<br>在与顶级人工智能软件工程师的对话中，许多人强调选择不加入 AMD 的原因是，在 AMD 软件工作感觉像是在移植 NVIDIA 工程师两年前开发的功能。相比之下，NVIDIA 为工程师提供了在前沿软件上工作的机会，并为用于最先进模型（如 o3）的芯片构建软件，涵盖训练和推理。</p><p>Additionally, engineers attracted to the “David” in a “David vs Goliath” scenario often choose Google TPU or the <a href="https://openai.com/careers/hardwaresoftware-co-design-engineer/">OpenAI Chip team</a> over AMD. These teams offer significantly better compensation and arguably have a higher probability of success in competing against NVIDIA due to those companies having giant volume of internal workloads to be their own customers. This makes them more appealing options for ambitious engineers.<br>此外，吸引那些在“戴维与歌利亚”场景中选择“戴维”的工程师，往往会选择谷歌 TPU 或 OpenAI 芯片团队而非 AMD。这些团队提供显著更好的薪酬，并且由于这些公司拥有巨大的内部工作量作为自己的客户，竞争 NVIDIA 的成功概率也更高。这使得它们对有抱负的工程师更具吸引力。</p><p>AMD’s internal benchmarking of their compensation structure appears to cherry pick comparable companies. By benchmarking against semiconductor companies that aren’t known for great software such as Juniper Networks, Cisco, and ARM, AMD may mistakenly perceive their compensation as competitive. However, when correctly benchmarked against companies renowned for AI software such as GPU kernels, GEMMs, PyTorch internals, distributed training infrastructure, and inference engines—the gap in pay becomes glaringly apparent.<br>AMD 内部对其薪酬结构的基准测试似乎选择了可比公司。通过与不以优秀软件著称的半导体公司（如 Juniper Networks、Cisco 和 ARM）进行基准测试，AMD 可能错误地认为其薪酬具有竞争力。然而，当与以 AI 软件著称的公司（如 GPU 内核、GEMMs、PyTorch 内部、分布式训练基础设施和推理引擎）进行正确的基准测试时，薪酬差距变得显而易见。</p><p>When conducting a proper apples-to-apples comparison, for example, comparing an NVIDIA PyTorch Lead to AMD’s PyTorch Lead or NVIDIA’s NCCL engineers to AMD’s RCCL engineers, NVIDIA pays significantly better and thus is able to attract and retain top talent.<br>在进行适当的同类比较时，例如，将 NVIDIA 的 PyTorch 负责人与 AMD 的 PyTorch 负责人进行比较，或将 NVIDIA 的 NCCL 工程师与 AMD 的 RCCL 工程师进行比较，NVIDIA 的薪酬显著更高，因此能够吸引和留住顶尖人才。</p><p>This issue represents a critical blind spot in AMD management’s strategy. We think that AMD understands how essential software engineers are to AMD’s long-term competitiveness and innovation and wants to place them at the core of their strategy, but we think the blind spot comes from inaccurate benchmarking and attempting to make comparisons in broad strokes – a fog of war, as it were. This unfortunately has resulted in a persistent undervaluation of software which risks further exacerbating the company’s weaker software capabilities relative to its direct competitors.<br>这个问题代表了 AMD 管理层战略中的一个关键盲点。我们认为 AMD 理解软件工程师对其长期竞争力和创新的重要性，并希望将他们置于战略的核心，但我们认为盲点来自于不准确的基准测试和试图进行粗略比较——可以说是一种战争迷雾。不幸的是，这导致了对软件的持续低估，这可能进一步加剧公司相对于直接竞争对手的软件能力较弱的问题。</p><p>AMD should keep AI software engineering base salaries stable but implement substantial increases in RSUs. By tying engineer compensation more closely to AMD’s future growth and, the company can more directly align the interests of their top talent with long-term organizational performance.<br>AMD 应保持 AI 软件工程师的基本工资稳定，但应大幅增加 RSU。通过将工程师的薪酬与 AMD 的未来增长更紧密地联系起来，公司可以更直接地将其顶尖人才的利益与长期组织绩效对齐。</p><p>Given AMD’s financial position with over $5 billion in cash reserves, the company possesses ample capacity to invest strategically in software talent. Leadership must now decide to prioritize retaining and attracting high-caliber engineers through meaningful compensation enhancements. Without taking such action, AMD risks perpetuating its lag behind NVIDIA, undermining its progress in the rapidly evolving AI market.<br>鉴于 AMD 的财务状况拥有超过 50 亿美元的现金储备，公司具备充足的能力在软件人才方面进行战略投资。领导层现在必须决定优先通过有意义的薪酬提升来留住和吸引高素质的工程师。如果不采取这样的行动，AMD 将面临在 NVIDIA 后面持续滞后的风险，削弱其在快速发展的 AI 市场中的进展。</p><p>AMD’s internal development clusters have seen significant improvements over the past four months, yet these enhancements still fall short of what is needed to compete effectively in the long-term GPU development landscape.<br>AMD 的内部开发集群在过去四个月中取得了显著改善，但这些提升仍然不足以在长期 GPU 开发领域有效竞争。</p><p>Currently, AMD claims to have rented an aggregate capacity of ~8,000 MI300 GPUs from CSPs distributed across several clusters and it claims that among this, the largest single cluster contains about 2,000 MI300 GPUs. However, deeper examination suggests the realistic consistent availability may be closer to 3,000 to 4,000 GPUs in aggregate across the whole company as AMD internally operates on a burst model. Internal developers now have adequate access to single-node development resources, but multi-node and comprehensive cluster development remain constrained. This limitation severely impacts larger-scale projects and collaborative development efforts, and there is still a need for a substantial increase in absolute quantity and consistency of GPU availability.<br>目前，AMD 声称已从多个集群的 CSP 租用了约 8,000 个 MI300 GPU，并声称其中最大的单个集群包含约 2,000 个 MI300 GPU。然而，深入检查表明，整个公司的实际可用数量可能更接近 3,000 到 4,000 个 GPU，因为 AMD 内部采用的是突发模型。内部开发人员现在可以充分访问单节点开发资源，但多节点和综合集群开发仍然受到限制。这一限制严重影响了大规模项目和协作开发的努力，并且仍然需要在 GPU 可用性的绝对数量和一致性上有显著增加。</p><p>Moreover, with the new industry-standard approach of datacenter-scale disaggregated prefill optimization for inferencing, even developing inference solutions now requires resources at the cluster scale. AMD’s current limited cluster level resources for individual internal developers further complicates its ability to effectively innovate and compete in this evolving landscape.<br>此外，随着数据中心规模的分散预填优化的新行业标准方法，甚至开发推理解决方案现在也需要集群规模的资源。AMD 目前对个别内部开发者的有限集群级资源进一步复杂化了其在这一不断发展的环境中有效创新和竞争的能力。</p><p>A major impediment to further expansion and innovation at AMD is the short-term, burst-oriented model for procurement of clusters for internal use, with most contracts lasting less than a year. This contrasts starkly with NVIDIA’s strategy, which employs persistent, multi-year cluster deployments, giving engineers greater freedom to pursue creative and high-risk projects without continuous oversight from financial controllers. NVIDIA, for example, operates extensive internal GPU resources including the A100 Selene cluster with thousands of GPUs, two EOS clusters (one with 4,600 H100 GPUs and another with 11,000 H100 GPUs) alongside dozens of smaller 64-1024 sized H100&#x2F;H200 clusters located both on-premises and rented from cloud providers such as OCI, Azure, CoreWeave, and Nebius, etc. The massive scale of the GB200 clusters they will get this year. The above numbers also exclude the billions of dollars of clusters for DGX Cloud that they have.<br>AMD 进一步扩展和创新的一个主要障碍是短期、突发导向的内部集群采购模式，大多数合同持续时间不到一年。这与 NVIDIA 的战略形成鲜明对比，后者采用持久的多年集群部署，使工程师能够在没有财务控制者持续监督的情况下，追求创造性和高风险的项目。例如，NVIDIA 运营着广泛的内部 GPU 资源，包括拥有数千个 GPU 的 A100 Selene 集群、两个 EOS 集群（一个有 4,600 个 H100 GPU，另一个有 11,000 个 H100 GPU），以及数十个规模在 64-1024 的 H100&#x2F;H200 集群，这些集群位于本地和从云服务提供商如 OCI、Azure、CoreWeave 和 Nebius 等租赁。它们今年将获得的 GB200 集群的规模巨大。上述数字还不包括他们为 DGX Cloud 拥有的数十亿美元的集群。</p><p>AMD’s current setup, where each GPU hour effectively carries a direct profit-and-loss consideration, discourages essential exploratory projects and strategic long-term developments.<br>AMD 目前的设置，使得每个 GPU 小时实际上都带有直接的盈亏考量，这抑制了必要的探索性项目和战略性长期发展。</p><p>AMD must urgently pivot from its current sub-year cluster strategy toward signing long-term, multi-year commitments, and it should specifically invest in a substantial cluster of 10,000+ flagship class GPUs. Such a commitment would demonstrate AMD’s dedication to each GPU generation, like NVIDIA’s robust, long-term software and hardware support spanning multiple years for every GPU generation. The existing burst model is significantly damaging AMD’s internal development efforts and limiting innovation potential. Transitioning to a sustained, multi-year investment approach will enable AMD to pursue strategic innovation and competitive advantage effectively.<br>AMD 必须紧急从当前的年度集群策略转向签署长期的、多年的承诺，并且应该特别投资于一个超过 10,000 个旗舰级 GPU 的大型集群。这种承诺将展示 AMD 对每一代 GPU 的投入，就像 NVIDIA 对每一代 GPU 提供的强大、长期的软件和硬件支持一样，跨越多个年份。现有的突发模式正在严重损害 AMD 的内部开发工作，并限制创新潜力。转向持续的、多年的投资方式将使 AMD 能够有效地追求战略创新和竞争优势。</p><p>With over $5 billion in available cash reserves, AMD clearly has the financial flexibility to shift toward a more strategic, long-term investment approach. The current short-sighted focus on quarterly earnings compromises AMD’s capacity for future innovation and development leadership. Adopting a multi-year commitment to GPU generations would significantly enhance long-term support, aligning AMD’s internal capabilities more closely with customer expectations. This strategic adjustment would also reassure customers regarding AMD’s commitment to sustained support and innovation, thereby strengthening market confidence and long-term partnerships.<br>凭借超过 50 亿美元的现金储备，AMD 显然具备向更具战略性、长期投资方法转变的财务灵活性。目前对季度收益的短视关注妨碍了 AMD 未来创新和发展领导力的能力。采取对 GPU 世代的多年承诺将显著增强长期支持，使 AMD 的内部能力与客户期望更紧密地对齐。这一战略调整还将向客户保证 AMD 对持续支持和创新的承诺，从而增强市场信心和长期合作关系。</p><h2 id="ROCm’s-Lack-of-First-Class-Python-SupportROCm-对一流-Python-支持的缺乏"><a href="#ROCm’s-Lack-of-First-Class-Python-SupportROCm-对一流-Python-支持的缺乏" class="headerlink" title="ROCm’s Lack of First-Class Python SupportROCm 对一流 Python 支持的缺乏"></a>ROCm’s Lack of First-Class Python SupportROCm 对一流 Python 支持的缺乏</h2><p>Making the entire CUDA ecosystem a first-class experience on Python has been a top priority for NVIDIA for the past 12 months. None other than Jensen himself is personally looped in and managing this effort. In the 2010s, Jensen was the first to understand that investing into making CUDA software great for AI would pay dividends. In 2025, Jensen’s key insight is to now understand that the de facto language of AI is Python, and that supporting every layer of NVIDIA’s current C++ CUDA stack into the Python world will yield a high return on investment. AT GTC this year, NVIDIA released dozens of Python libraries from GEMM libraries like nvmath-python libraries to cuda.binding cuBLASLt bindings to kernel DSLs like cuTile, Warp, Triton, CuTe Python. Unfortunately, ROCm libraries support in Python is nowhere near what NVIDIA has. <strong>NVIDIA has a python interface at every layer of the stack. AMD does not offer this.</strong><br>在过去的 12 个月里，使整个 CUDA 生态系统在 Python 上成为一流体验一直是 NVIDIA 的首要任务。连 Jensen 本人都亲自参与并管理这一工作。在 2010 年代，Jensen 是第一个意识到投资于使 CUDA 软件在 AI 领域出色将会带来回报的人。到 2025 年，Jensen 的关键洞察是现在要理解 AI 的事实标准语言是 Python，并且将 NVIDIA 当前的 C++ CUDA 堆栈的每一层支持到 Python 世界中将带来高投资回报。今年在 GTC 上，NVIDIA 发布了数十个 Python 库，从 GEMM 库如 nvmath-python 库到 cuda.binding cuBLASLt 绑定，再到像 cuTile、Warp、Triton、CuTe Python 这样的内核 DSL。不幸的是，ROCm 库在 Python 中的支持远不及 NVIDIA。NVIDIA 在堆栈的每一层都有 Python 接口，而 AMD 并没有提供这一点。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-13.-CUDA-Accelerated-Python.png?fit=1024,557&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>By supporting Python first class in CUDA, end users can spend less time to get the same performance or spend the same amount of time to get even better performance. CUDA Python effectively shifts the pareto frontier curve on “Performance of Application versus Time Spent Optimizing”<br>通过在 CUDA 中支持 Python 一流，最终用户可以花更少的时间获得相同的性能，或者花相同的时间获得更好的性能。CUDA Python 有效地移动了“应用性能与优化时间”之间的帕累托前沿曲线。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-14.-CUDA-Ecosystem.png?fit=1024,596&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>To use a simple example, previously if a developer wanted to call cuBLASLt with a custom epilogue, they would need to write a C++ extension and then Pybind it to Python which is a bit convolved and adds another layer of indirection that an ML engineer needs to worry about. Now with nvmath-python, the same task can now be carried out and automatically tuned with just 3 python lines**. The task is now turned from a 30-minute task to a 2-minute one.** These NVIDIA Python libraries aren’t just half-baked bindings, they are first-class implementations with performance top of mind.<br>为了使用一个简单的例子，以前如果开发者想要使用自定义尾声调用 cuBLASLt，他们需要编写一个 C++ 扩展，然后将其绑定到 Python，这有点复杂，并增加了一个机器学习工程师需要担心的间接层。现在使用 nvmath-python，相同的任务现在只需 3 行 Python 代码即可完成并自动调优。这个任务的时间从 30 分钟缩短到了 2 分钟。这些 NVIDIA Python 库不仅仅是半成品的绑定，它们是以性能为核心的第一流实现。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-15.-Nv-math-Python.png?fit=1024,547&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>In another example, with the cuda.cooperative device side library, one can now access speed of light CUDA prebuilt algorithms such as block reduce through a python interface. This level of performance was previously only available in C++ CUDA through <a href="https://docs.nvidia.com/cuda/cub/index.html">CUB</a>.<br>在另一个例子中，使用 cuda.cooperative 设备端库，现在可以通过 Python 接口访问光速 CUDA 预构建算法，例如块归约。这个性能水平之前仅在 C++ CUDA 中通过 CUB 可用。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-16.-CUDA-cooperative.png?fit=1024,559&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>For end users that want 1:1 Python bindings instead of higher order Pythonic libraries, NVIDIA also offers this through cuda.binding and cuda.core. <strong>NVIDIA has a python interface at every layer of the stack. AMD does not offer this.</strong><br>对于希望使用 1:1 Python 绑定而不是更高阶 Python 库的最终用户，NVIDIA 也通过 cuda.binding 和 cuda.core 提供此功能。NVIDIA 在堆栈的每一层都有 Python 接口。AMD 不提供此功能。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-17.-Python-Interfaces.png?fit=1024,614&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>AMD has launched recently python interfaces for <a href="https://github.com/ROCm/aiter">AITER</a> which is the equivalent of cuDNN-python and supports OAI triton for kernel authoring but the rest of the layers of the stack, ROCm has no comparable product and they aren’t even thinking about supporting a first-class python experience yet.<br>AMD 最近推出了 AITER 的 Python 接口，相当于 cuDNN-python，并支持 OAI triton 进行内核创作，但栈的其余层次中，ROCm 没有可比的产品，他们甚至还没有考虑支持一流的 Python 体验。</p><h2 id="Python-GPU-Kernel-Authoring-DSLsPython-GPU-内核创作-DSLs"><a href="#Python-GPU-Kernel-Authoring-DSLsPython-GPU-内核创作-DSLs" class="headerlink" title="Python GPU Kernel Authoring DSLsPython GPU 内核创作 DSLs"></a>Python GPU Kernel Authoring DSLsPython GPU 内核创作 DSLs</h2><p>At GTC 2025, in addition to debuting the overall Python CUDA libraries, NVIDIA also announced Python kernel authoring DSLs – namely Python CuTe 4.0, cuTile Python and Warp. And this is on top of Nvidia’s existing Triton DSL support! <strong>AMD is lacking and uncompetitive in the Python Kernel DSLs space to the extent that Nvidia teams are now competing against each other with multiple different NVIDIA DSLs now publicly launched</strong>. There are currently five different NVIDIA python DSLs (OAI Triton, CuTe Python, cuTile Python, Numba, Warp), with many more that are internally in the works that they haven’t announced publicly yet.<br>在 GTC 2025 上，除了首次推出整体 Python CUDA 库外，NVIDIA 还宣布了 Python 内核创作 DSL，即 Python CuTe 4.0、cuTile Python 和 Warp。这是在 Nvidia 现有的 Triton DSL 支持基础上进行的！AMD 在 Python 内核 DSL 领域缺乏竞争力，以至于 Nvidia 团队现在在多个不同的 NVIDIA DSL 之间相互竞争，这些 DSL 现在已经公开发布。目前有五种不同的 NVIDIA Python DSL（OAI Triton、CuTe Python、cuTile Python、Numba、Warp），还有许多正在内部开发中尚未公开宣布的。</p><p>Python kernel DSLs can be categorized into two types based on the abstraction unit. Programmers describe single-thread behaviors in thread-based languages, while in tile-based languages, they describe operations on partitions of matrices.<br>Python 内核 DSL 可以根据抽象单元分为两种类型。程序员在基于线程的语言中描述单线程行为，而在基于块的语言中，他们描述对矩阵分区的操作。</p><p>CuTe Python is NVIDIA’s recommended path for writing speed-of-light kernels in for thread-based Python kernel DSLs. It provides low-level primitives as building blocks for custom kernels, and it uses a powerful abstraction cuTe (CUDA Tensor) to describe data and thread layouts. CUTLASS Python’s API design is based on CUTLASS, so new users can leverage CUTLASS’s extensive documentation of concepts and usage to get up to speed. While AMD has a CUTLASS-analogous C++ library CK (Composable Kernel), its documentation on concepts and usage is comparatively sparse and unclear. CK Python interface is coming for their high-level interfaces but none in the works for their CuTe-analogous atom layer.<br>CuTe Python 是 NVIDIA 推荐的用于编写光速内核的线程基础 Python 内核 DSL 的路径。它提供了低级原语作为自定义内核的构建块，并使用强大的抽象 cuTe（CUDA 张量）来描述数据和线程布局。CUTLASS Python 的 API 设计基于 CUTLASS，因此新用户可以利用 CUTLASS 的广泛文档来了解概念和用法，以快速上手。虽然 AMD 有一个与 CUTLASS 类似的 C++ 库 CK（可组合内核），但其关于概念和用法的文档相对稀少且不清晰。CK Python 接口正在为其高级接口开发，但尚未为其 CuTe 类似的原子层开发。</p><p><strong>More importantly, AMD in general currently has no Python DSLs for thread-based kernel programming which is needed for speed of light.<br>更重要的是，AMD 目前普遍没有用于线程基础内核编程的 Python DSL，这对于光速是必需的。</strong></p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-18.-Cutlass-in-Python.png?fit=1024,400&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>For Tile Based and SIMT based as well as hybrid Tile&#x2F;SIMT mixed based kernel authoring Python DSLs, NVIDIA announced cuTile at GTC 2025. cuTile is not meant for 100% speed of light performance but is meant for 98% speed of light performance at 10% the time to write the kernel. It is relatively easy to write kernels in cuTile**. Unfortunately, AMD has no offering for hybrid SIMT&#x2F;Tile based python kernels DSLs.**<br>对于基于瓦片和 SIMT 以及混合瓦片&#x2F;SIMT 的内核编写 Python DSL，NVIDIA 在 GTC 2025 上宣布了 cuTile。cuTile 并不是为了实现 100%的光速性能，而是旨在以 10%的时间编写内核实现 98%的光速性能。在 cuTile 中编写内核相对容易。不幸的是，AMD 没有提供混合 SIMT&#x2F;瓦片的 Python 内核 DSL。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-19.-Tile-and-SIMT-Kernels.png?fit=1024,542&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>Triton popularized tile-based programming models in the age of tensor cores where the effective abstraction is at the layer of a Tile instead of on a per thread basis. Nvidia will continue to fully support Triton’s Tile based DSLs in addition to cuTile Tile based DSLs.<br>Triton 在张量核心时代普及了基于瓦片的编程模型，其中有效的抽象是在瓦片层面，而不是按线程基础。Nvidia 将继续全面支持 Triton 的基于瓦片的 DSL，以及 cuTile 基于瓦片的 DSL。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-20.-Tile-programming.png?fit=1024,637&ssl=1"></p><p>Source: Nvidia 来源：Nvidia</p><p>For differential simulation AI, NVIDIA announced Warp Python DSLs. Warp is a hybrid tile and SIMT based programming model useful for writing simulations and geometry processing. The great benefit of Warp over cuTile is that Warp is automatically differentiable which is extremely useful in simulation to automatically generate the backward pass. <strong>AMD has no offering for this hybrid SIMT&#x2F;Tile based differentiable Python kernels DSLs either.</strong><br>对于差分模拟 AI，NVIDIA 宣布了 Warp Python DSL。Warp 是一种混合的 tile 和 SIMT 基础的编程模型，适用于编写模拟和几何处理。Warp 相对于 cuTile 的一个巨大优势是 Warp 是自动可微分的，这在模拟中非常有用，可以自动生成反向传播。AMD 也没有提供这种混合的 SIMT&#x2F;Tile 基础的可微分 Python 内核 DSL。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-21.-Nvidia-Warp-1.png?fit=1024,548&ssl=1"></p><p>Source: Nvidia 来源：Nvidia</p><p>Nvidia continues to fully support OpenAI Triton Python DSL in addition to all their newly announced Python DSLs. The primarily maintainer of Triton is OpenAI whose mission is to build safe AGI. Indeed, at <a href="https://semianalysis.com/2025-hackathon-eol/">the SemiAnalysis Blackwell PTX Hackathon 2025</a>, <strong>Phil Tilet, lead OpenAI maintainer of Triton, has even said that “AGI won’t come from 10% faster matrix multiplications”,</strong> &amp; this is the ethos surrounding what features Triton will prioritize. Thus Triton isn’t the platform AI chip vendors should solely support if they want to have the fastest AI chip. We believe that AI chip vendors should still fully support Triton in addition to other Python DSLs.<br>Nvidia 继续全力支持 OpenAI Triton Python DSL，以及他们新宣布的所有 Python DSL。Triton 的主要维护者是 OpenAI，其使命是构建安全的 AGI。事实上，在 2025 年的 SemiAnalysis Blackwell PTX Hackathon 上，Triton 的首席维护者 Phil Tilet 甚至表示：“AGI 不会来自 10% 更快的矩阵乘法”，这就是围绕 Triton 将优先考虑哪些功能的理念。因此，如果 AI 芯片供应商想要拥有最快的 AI 芯片，Triton 并不是他们应该单独支持的平台。我们相信，AI 芯片供应商仍然应该在支持其他 Python DSL 的同时，全面支持 Triton。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-23.-OpenAI-quote.png?fit=1024,196&ssl=1"></p><p>Source: OpenAI, SemiAnalysis Blackwell PTX Hackathon 来源：OpenAI，SemiAnalysis Blackwell PTX 黑客马拉松</p><p>This stance has led to a misalignment of objectives where OpenAI Triton does not care about getting absolute speed of light peak performance while AI Chip vendors like Nvidia, MTIA, MAIA, AMD care a lot about getting peak performance in their kernel language.<br>这种立场导致了目标的不一致，OpenAI Triton 并不关心获得绝对的光速峰值性能，而像 Nvidia、MTIA、MAIA、AMD 这样的 AI 芯片供应商则非常关注在其内核语言中获得峰值性能。</p><p>Nvidia’s Triton performance isn’t close to speed of light while AMD’s Triton performance is even further away from speed of light. AMD needs to heavily hire and invest into making Triton performance much stronger in addition to supporting&#x2F;inventing in other python kernel DSLs.<br>英伟达的 Triton 性能远未接近光速，而 AMD 的 Triton 性能更是远离光速。AMD 需要大量招聘并投资于提升 Triton 性能，同时支持&#x2F;发明其他 Python 内核 DSL。</p><p>AMD has an experimental kernel language called <a href="https://github.com/iree-org/iree-turbine/tree/main/iree/turbine/kernel/wave">wave</a> that uses a warp based programming model but it seems to be still very early stage and doesn’t have the full backing of the company. This is a far cry from cuTile, CuTe, Warp, all of which have the full backing of Jensen and Nvidia, who are all in on making CUDA Python great. Furthermore, it is questionable if warp-based kernel DSLs add the right abstraction layer considering the industry is moving towards warp-group based MMAs hardware &amp; 2-CTA MMAs hardware instead of warp based MMAs.<br>AMD 有一种名为 wave 的实验性内核语言，采用基于 warp 的编程模型，但似乎仍处于早期阶段，并没有得到公司的全面支持。这与 cuTile、CuTe、Warp 大相径庭，后者都得到了詹森和英伟达的全力支持，他们全力以赴地致力于让 CUDA Python 变得出色。此外，考虑到行业正朝着基于 warp 组的 MMA 硬件和 2-CTA MMA 硬件发展，而不是基于 warp 的 MMA，基于 warp 的内核 DSL 是否增加了正确的抽象层也是值得怀疑的。</p><h2 id="The-Widening-Gap-Between-AMD-RCCL-and-NVIDIA-NCCLAMD-RCCL-与-NVIDIA-NCCL-之间的差距扩大"><a href="#The-Widening-Gap-Between-AMD-RCCL-and-NVIDIA-NCCLAMD-RCCL-与-NVIDIA-NCCL-之间的差距扩大" class="headerlink" title="The Widening Gap Between AMD RCCL and NVIDIA NCCLAMD RCCL 与 NVIDIA NCCL 之间的差距扩大"></a>The Widening Gap Between AMD RCCL and NVIDIA NCCLAMD RCCL 与 NVIDIA NCCL 之间的差距扩大</h2><p>Collective communication libraries are extremely important for AI training and inference as these libraries let multiple GPUs work on the same workload. Nvidia’s collective communication library is called NCCL. AMD’s library is a “ctrl+c, ctrl+v” fork of NCCL and it is called RCCL. Ever since we shared our thoughts on RCCL in <a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/">our December 2024 article</a>, RCCL team has made some decent progress. More than a year after the MI300X went into production, the RCCL team has now finally supported the <a href="https://github.com/ROCm/rccl/pull/1549">LL128 protocol</a> on MI300X. This is a great improvement but by comparison, Blackwell on day one already supports all three collective protocols (SIMPLE, LL, LL128).<br>集体通信库对于 AI 训练和推理极为重要，因为这些库允许多个 GPU 在同一工作负载上协同工作。Nvidia 的集体通信库称为 NCCL。AMD 的库是 NCCL 的一个“ctrl+c, ctrl+v”分支，称为 RCCL。自从我们在 2024 年 12 月的文章中分享了对 RCCL 的看法以来，RCCL 团队取得了一些不错的进展。在 MI300X 投入生产一年多后，RCCL 团队现在终于在 MI300X 上支持 LL128 协议。这是一个很大的改进，但相比之下，Blackwell 在第一天就已经支持所有三种集体协议（SIMPLE、LL、LL128）。</p><p>Furthermore, RCCL finally supports rail optimized trees which improves networking performance by reducing traffic away from spine switches, leading to fewer path collisions. This feature has been supported by NCCL by countless years already.<br>此外，RCCL 最终支持铁路优化树，这通过减少流量从主干交换机转移来提高网络性能，从而减少路径冲突。这个功能已经被 NCCL 支持了无数年。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-24.-Trees.png?fit=861,1024&ssl=1"></p><p>Source: Github 来源：Github</p><p><strong>Although RCCL has made some decent progress, the delta between NCCL and RCCL continues to significantly widen due to the</strong> <a href="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72583/"><strong>new NCCL improvements and features announced at GTC 2025</strong></a><strong>.</strong> TheRCCL team will need more access to proper resources like large compute clusters to catch up with NCCL. They should be given exclusive access to a persistent cluster of at least 1,024 MI300 class GPUs. Furthermore, AMD leadership needs to invest into massively increasing RCCL engineer RSUs compensation in order to attract and retain key talent in what is one of the most mission critical software libraries.<br>尽管 RCCL 取得了一些不错的进展，但由于在 GTC 2025 上宣布的新 NCCL 改进和功能，NCCL 与 RCCL 之间的差距仍在显著扩大。RCCL 团队需要更多的适当资源，如大型计算集群，以赶上 NCCL。他们应该被给予至少 1,024 个 MI300 级 GPU 的持久集群的独占访问权限。此外，AMD 领导层需要投资大幅提高 RCCL 工程师的 RSU 薪酬，以吸引和留住在这个最关键的软件库之一的关键人才。</p><p>Because AMD’s RCCL library is a carbon copy fork of Nvidia’s NCCL, NCCL 2.27 and 2.28 massive refactor will continue expanding the CUDA moat and will force AMD’s RCCL team to expend thousands of engineering hours to sync Nvidia’s major refactor into RCCL. While AMD’s engineering team is bogged down having to spend thousands of engineering hours to sync the changes, Nvidia will be using that time instead to continue advancing the frontier of collective communications software stack and algorithms. This dynamic makes it virtually impossible for AMD to sustain RCCL’s existing development efforts while working towards achieving parity with NCCL, let alone surpassing NCCL.<br>由于 AMD 的 RCCL 库是 Nvidia 的 NCCL 的完全复制分支，NCCL 2.27 和 2.28 的大规模重构将继续扩大 CUDA 的护城河，并迫使 AMD 的 RCCL 团队花费数千小时的工程时间将 Nvidia 的重大重构同步到 RCCL。与此同时，AMD 的工程团队被迫花费数千小时的工程时间来同步这些更改，而 Nvidia 将利用这段时间继续推进集体通信软件栈和算法的前沿。这种动态使得 AMD 几乎不可能在努力实现与 NCCL 的平等的同时维持 RCCL 现有的开发工作，更不用说超越 NCCL 了。</p><p>AMD has indicated that they are currently in the planning phase of rewriting RCCL from scratch to stop being a fork of NCCL.<br>AMD 表示他们目前正在规划从头开始重写 RCCL，以停止成为 NCCL 的一个分支。</p><p>At his GTC 2025 talk, as a joke, we asked NCCL chief Sylvain Jeaugey, if in the spirit of open-source development, he would lend a helping hand to RCCL as its currently mostly a copy paste library.<br>在他的 GTC 2025 演讲中，我们开玩笑地问 NCCL 负责人 Sylvain Jeaugey，是否可以本着开源开发的精神，向 RCCL 伸出援手，因为它目前主要是一个复制粘贴的库。</p><p>He rebuffed our suggestion:<br>他拒绝了我们的建议：  </p><p><em>SemiAnalysis: Will Nvidia provide support to the AMD team’s RCCL fork due to this big of a refactor in the upcoming 2.28?<br>SemiAnalysis：由于即将到来的 2.28 版本进行如此大规模的重构，Nvidia 会支持 AMD 团队的 RCCL 分支吗？<br>Sylvain: Are we going to also help RCCL move to that? I don’t think so – usually we don’t really take part in that development.<br>Sylvain：我们也会帮助 RCCL 迁移到那个吗？我认为不会——通常我们并不参与那种开发。<br>Source:</em> <a href="https://www.nvidia.com/en-us/on-demand/session/gtc25-s72583/?start=2025"><em>Nvidia</em></a> <em>– timestamp 33:48</em><br>来源：Nvidia – 时间戳 33:48</p><p>During this talk, Sylvain also announced many new NCCL features in the upcoming massive refactor. These new features include supporting symmetric memory natively in NCCL as well as new algorithms that run much faster and use fewer SMs thus allowing more SMs for compute.<br>在此次演讲中，Sylvain 还宣布了即将进行的大规模重构中的许多新 NCCL 功能。这些新功能包括在 NCCL 中原生支持对称内存，以及运行速度更快、使用更少 SM 的新算法，从而允许更多 SM 用于计算。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-24.1-New-Symmetric-Memory-Stack.png?fit=1024,609&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p><a href="https://dev-discuss.pytorch.org/t/pytorch-symmetricmemory-harnessing-nvlink-programmability-with-ease/2798">PyTorch introduced SymmetricMemory API</a> that enable users to harness multi-GPU scale up programmability with ease and <a href="https://github.com/yifuwang/symm-mem-recipes">write collectives or fused compute&#x2F;communication kernels in CUDA or Triton</a>. Previously, writing multi-GPU fused compute&#x2F;communication kernels required a ton of work, but the work needed has been reduced considerably with the use of PyTorch SymMem. Performant inference kernels like one-shot and two-shot collectives as well as all gather fused matmul kernels can be written in SymmetricMemory easily.<br>PyTorch 引入了 SymmetricMemory API，使用户能够轻松利用多 GPU 扩展编程能力，并在 CUDA 或 Triton 中编写集合或融合计算&#x2F;通信内核。之前，编写多 GPU 融合计算&#x2F;通信内核需要大量工作，但使用 PyTorch SymMem 后所需的工作量大大减少。高性能推理内核，如一次性和两次性集合，以及所有收集融合矩阵乘法内核，可以轻松地在 SymmetricMemory 中编写。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-25.-Process-0-1.jpg?fit=1024,567&ssl=1"></p><p>Source: PyTorch 来源：PyTorch</p><p>This feature has been available on NVIDIA GPUs for the past 8 months while AMD GPUs still don’t support it. AMD has indicated that they will land initial support for PyTorch SymmetricMemory API in Q2 2025.<br>此功能在过去 8 个月中已在 NVIDIA GPU 上可用，而 AMD GPU 仍不支持。AMD 已表示他们将在 2025 年第二季度首次支持 PyTorch SymmetricMemory API。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-26.-Pytorch.png?fit=831,1024&ssl=1"></p><p>Source: PyTorch, YiFu 来源：PyTorch，YiFu</p><p>In the upcoming 2.27 release, allreduce achieves a 4x lower reduction at the same message size and attains the same algorithm bandwidth at 4x smaller message sizes.<br>在即将发布的 2.27 版本中，allreduce 在相同消息大小下实现了 4 倍更低的归约，并在消息大小缩小 4 倍时达到了相同的算法带宽。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-28.-New-Algorithms.png?fit=1024,620&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>In the upcoming 2.28 release, NCCL offers device-side APIs allowing end users to easily write custom communication&#x2F;compute fusion kernels.<br>在即将发布的 2.28 版本中，NCCL 提供了设备端 API，允许最终用户轻松编写自定义通信&#x2F;计算融合内核。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-29.-Device-side-API.png?fit=1024,619&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>The upcoming NCCL 2.28 release will also support <a href="https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/">GPUDirect Async</a> (IBGDA) on both InfiniBand and RoCEv2 Ethernet. Currently, in NCCL and RCCL, a CPU proxy is used to control flow for scale-out communication. Although the dataflow doesn’t go through the CPU, sending the control flow through the CPU still limits the real world achieved performance. With NVIDIA NCCL’s 2.28 integration with IBGDA – supported in both RoCEv2 and InfiniBand – the control flow is initialized by the GPU and does not go through the CPU, leading to better performance on all2all and all2all based algorithms across small and medium message sizes.<br>即将发布的 NCCL 2.28 版本将支持在 InfiniBand 和 RoCEv2 以太网上的 GPUDirect Async (IBGDA)。目前，在 NCCL 和 RCCL 中，使用 CPU 代理来控制扩展通信的流量。尽管数据流不经过 CPU，但通过 CPU 发送控制流仍然限制了实际性能的实现。通过 NVIDIA NCCL 的 2.28 与 IBGDA 的集成——在 RoCEv2 和 InfiniBand 中均受支持——控制流由 GPU 初始化，不经过 CPU，从而在小型和中型消息大小的 all2all 和基于 all2all 的算法上实现更好的性能。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-30.-Proxy-initiated-communication.png?fit=1024,600&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>Another feature that is currently only available on Nvidia is user buffer registration. This feature avoids creating extra copies between the user’s tensor and NCCL’s internal buffers. This helps with reducing the number of collective SMs needed and with alleviating memory pressure, leading a 5-20% end to end training improvement.<br>另一个目前仅在 Nvidia 上可用的功能是用户缓冲区注册。此功能避免在用户的张量和 NCCL 的内部缓冲区之间创建额外的副本。这有助于减少所需的集体 SM 数量，并减轻内存压力，从而实现 5-20%的端到端训练改进。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-32.-User-buffer-registration.png?fit=1024,576&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>Most experienced ML engineers have seen the dreaded NCCL_TIMEOUT&#x2F;RCCL_TIMEOUT or NCCL&#x2F;RCCL stalling. NVIDIA NCCL supports <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting/ras.html">ncclras</a>, which simplifies debugging these issues. Unfortunately, RCCL does not currently include any features to help with debugging.<br>大多数经验丰富的机器学习工程师都见过可怕的 NCCL_TIMEOUT&#x2F;RCCL_TIMEOUT 或 NCCL&#x2F;RCCL 停滞。NVIDIA NCCL 支持 ncclras，这简化了调试这些问题。不幸的是，RCCL 目前不包括任何帮助调试的功能。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-33.-Large-Scale-Training.png?fit=1024,571&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><h2 id="Infrastructure-Software-Progress-Not-As-Fast基础软件进展不够快"><a href="#Infrastructure-Software-Progress-Not-As-Fast基础软件进展不够快" class="headerlink" title="Infrastructure Software Progress Not As Fast基础软件进展不够快"></a>Infrastructure Software Progress Not As Fast基础软件进展不够快</h2><p>AMD’s has made meaningful progress in the last four months on its software infrastructure layer (i.e. kubernetes, SDC detectors, health checks, SLURM, docker, metrics exporters), but the rate of progress is nowhere near keeping up with the rate of progress of AMD’s ML libraries.<br>在过去四个月中，AMD 在其软件基础设施层（即 kubernetes、SDC 探测器、健康检查、SLURM、docker、指标出口）上取得了显著进展，但进展速度远远跟不上 AMD 的机器学习库的进展速度。</p><p>Until seven months ago, AMD had no GPU metrics export function at all, meaning that cluster operators had no way to gain observability into their GPUs. Although ROCm claimed to be an open-source ecosystem, AMD’s GPU metrics exporter was not open source until SemiAnalysis took this point up with AMD executives, advocating for them to adopt a sense of urgency for AMD to live by the ethos of their claimed commitment to the “open source” ecosystem.<br>直到七个月前，AMD 根本没有 GPU 指标导出功能，这意味着集群操作员无法对其 GPU 进行可观察性分析。尽管 ROCm 声称是一个开源生态系统，但 AMD 的 GPU 指标导出工具在 SemiAnalysis 与 AMD 高管提出这一点之前并不是开源的，SemiAnalysis 倡导 AMD 应对其声称的“开源”生态系统的承诺采取紧迫感。</p><p>Fortunately, after many follow-ups, AMD has finally open sourced their GPU exporter. Note that their GPU exporter is still a work in progress and many features are still missing and are not at parity with NVIDIA’s GPU open-source metrics exporter yet. For example, AMD’s GPU exporter currently has still does not support the metric for matrix core activity, CU occupancy or CU active. These are extremely important metrics to proxy how workloads are performing. The only current utilization metric available in AMD’s GPU exporter is GPU_UTIL which, as most experienced <a href="https://x.com/memorypaladin/status/1817689501113979357">ML engineers know, doesn’t actually measure util at all for both Nvidia and AMD GPUs</a>.<br>幸运的是，在多次跟进之后，AMD 终于开源了他们的 GPU 导出工具。请注意，他们的 GPU 导出工具仍在开发中，许多功能仍然缺失，尚未与 NVIDIA 的 GPU 开源指标导出工具达到同等水平。例如，AMD 的 GPU 导出工具目前仍不支持矩阵核心活动、CU 占用率或 CU 活跃度的指标。这些都是代理工作负载性能的极其重要的指标。AMD 的 GPU 导出工具中唯一可用的利用率指标是 GPU_UTIL，正如大多数经验丰富的机器学习工程师所知，这实际上并不测量 Nvidia 和 AMD GPU 的利用率。</p><p><a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/#amd%e2%80%99s-forked-libraries">As mentioned in our December AMD article</a>, the AMD Docker UX is extremely poor compared to Nvidia’s UX. AMD has acknowledged this shortcoming and has mentioned to us that they are working on this. They have indicated that they will announce a roadmap for this later this quarter.<br>正如我们在 12 月的 AMD 文章中提到的，AMD Docker 的用户体验与 Nvidia 的用户体验相比极其糟糕。AMD 已经承认了这一缺陷，并向我们表示他们正在对此进行改进。他们表示将在本季度晚些时候公布一份路线图。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-34.-Nvidia-Docker.png?fit=1024,377&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>Unlike on Nvidia’s stack, the current state of Slurm+Container on AMD’s stack is disappointing. <a href="https://github.com/NVIDIA/pyxis">On Nvidia with open source pyxis Slurm</a>, launching container through Slurm is as easy as running “srun –container-name&#x3D;pytorch”. In contrast, when working on AMD, one must through an extremely convolved process that requires the use of lots of indirections.<br>与 Nvidia 的堆栈不同，AMD 的 Slurm+Container 当前状态令人失望。在 Nvidia 的开源 pyxis Slurm 上，通过 Slurm 启动容器就像运行“srun –container-name&#x3D;pytorch”一样简单。相比之下，在 AMD 上工作时，必须经过一个极其复杂的过程，需要使用大量的间接调用。</p><p>When considering how all AMD’s internal AI engineers use SLURM with containers, it is distressing to see the amount of indirection that are needed and how poor the current AMD Slurm+Container UX is.<br>考虑到所有 AMD 内部的 AI 工程师如何使用 SLURM 与容器，看到所需的间接调用数量以及当前 AMD Slurm+Container 用户体验的糟糕程度令人感到沮丧。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-35.-Github.png?fit=1024,575&ssl=1"></p><p>Source: GitHub, AMD 来源：GitHub，AMD</p><p>We have recommended on multiple occasions that AMD prioritize fixing this and focus on supporting a first-class Slurm+Containers experience by putting some money to work and paying for SchmedMD’s (the maintainers of Slurm) consulting services. We have yet to see any concrete timelines or roadmap on when AMD plans to fix this issue.<br>我们多次建议 AMD 优先修复此问题，并专注于通过投入资金并支付 SchmedMD（Slurm 的维护者）的咨询服务来支持一流的 Slurm+Containers 体验。我们尚未看到 AMD 计划何时解决此问题的具体时间表或路线图。</p><p>Moreover, Nvidia’s datacenter-manager tool (DCGM) has directly integrated NVVS (Nvidia Validation suite) such that running diagnostics is as simple as running “sudo dcgmi diag -r &lt;diag_level&gt;”. In contrast, on AMD, the RVVS (ROCm validation Suite) is separate from their Datacenter tool (RDC), forcing the end user to download yet another library. We recommend that AMD integrate RVVS into RDC to make the user experience as simple as that of Nvidia’s DCGM.<br>此外，Nvidia 的数据中心管理工具 (DCGM) 已直接集成 NVVS (Nvidia 验证套件)，使得运行诊断变得像运行 “sudo dcgmi diag -r ” 一样简单。相比之下，AMD 的 RVVS (ROCm 验证套件) 与他们的数据中心工具 (RDC) 是分开的，这迫使最终用户下载另一个库。我们建议 AMD 将 RVVS 集成到 RDC 中，以使用户体验与 Nvidia 的 DCGM 一样简单。</p><p>Also, AMD’s UX and validation coverage is not as good as DCGM’s. <a href="https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html#run-levels-and-tests">Nvidia’s DCGM utilizes notation denoting different levels (r1,r2,r3,r4)</a> while AMD’s NVVS does not use any such notation.<br>此外，AMD 的用户体验和验证覆盖率不如 DCGM。Nvidia 的 DCGM 使用表示不同级别的符号（r1，r2，r3，r4），而 AMD 的 NVVS 则没有使用任何这样的符号。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-36.-DCGM-Diagnostics.png?fit=1024,504&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>AMD is currently lacking support for many inference features such as disaggregated prefill, Smart Routing, and NVMe KV Cache Tiering. Disaggregated serving has been an industry standard for year, and last month NVIDIA <a href="https://github.com/ai-dynamo/dynamo">open-sourced Dynamo</a>, a distributed inference framework, further democratizing disaggregated serving. Disaggregated prefill splits the prefill stage and decode stage into different GPUs. Even Google has launched their own <a href="https://cloud.google.com/ai-hypercomputer/docs/workloads/pathways-on-cloud/multihost-inference#disaggregated_inference">disaggregated inferencing framework</a>.<br>AMD 目前缺乏对许多推理功能的支持，例如分离预填充、智能路由和 NVMe KV 缓存分层。分离服务已经成为行业标准多年，上个月 NVIDIA 开源了 Dynamo，一个分布式推理框架，进一步使分离服务大众化。分离预填充将预填充阶段和解码阶段分配到不同的 GPU 上。甚至 Google 也推出了他们自己的分离推理框架。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-37.-DistServe-Runtime.png?fit=1024,649&ssl=1"></p><p>Source: Peking University 来源：北京大学</p><p>NVIDIA’s Dynamo Smart Router intelligently routes each token in a multi-GPU inference deployment to both available instances. For the prefill phase – this means ensuring that incoming tokens are equally distributed to the different GPUs serving prefill to avoid bottlenecks on any given experts in the prefill phase.<br>NVIDIA 的 Dynamo 智能路由器智能地将每个令牌在多 GPU 推理部署中路由到两个可用实例。在预填充阶段，这意味着确保传入的令牌均匀分配到不同的 GPU 上，以避免在任何给定的专家中出现瓶颈。</p><p>Similarly – in the decode phase – it is important to ensure sequence lengths and requests are well distributed and balanced across GPUs serving decode. Some experts that are more heavily trafficked can be replicated as well by the GPU Planner provided by Dynamo to help keep the load balanced.<br>同样，在解码阶段，确保序列长度和请求在服务解码的 GPU 之间良好分配和平衡也很重要。一些流量较大的专家可以通过 Dynamo 提供的 GPU 规划器进行复制，以帮助保持负载平衡。</p><p>The router also load balances across each replica serving the model which is something AMD’s vLLM and many other inference engines do not support.<br>路由器还在每个为模型提供服务的副本之间进行负载均衡，这是 AMD 的 vLLM 和许多其他推理引擎不支持的功能。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-38.-Dynamo-Smart-Router-NVcache.png?fit=1024,611&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>Dynamo’s GPU Planner is an autoscaler of both prefill and decode nodes, spinning up additional nodes with fluctuations in demand that are natural over the course of the day. It can implement a degree of load balancing among many experts in an MoE model in both prefill and decode nodes. The GPU planner spins up additional GPUs to provide additional compute to high-load experts. It can also dynamically reallocate nodes between prefill and decode nodes as needed, further maximizing resource utilization.<br>Dynamo 的 GPU 规划器是一个自动扩展器，能够根据日常需求波动自动增加预填充和解码节点。它可以在预填充和解码节点之间实现一定程度的负载均衡，适用于 MoE 模型中的多个专家。GPU 规划器会增加额外的 GPU，以为高负载专家提供额外的计算能力。它还可以根据需要动态重新分配预填充和解码节点，进一步最大化资源利用率。</p><p>This additionally supports changing the ratio of GPUs used for decoding and for prefill – this is especially useful for cases like Deep Research, where more prefill is required as opposed to decoding, as these applications need to review a huge amount of context but only generate a comparatively small amount.<br>这还支持改变用于解码和预填充的 GPU 比例——这在像深度研究这样的情况下尤其有用，因为这些应用需要审查大量上下文，但只生成相对较少的内容，因此需要更多的预填充。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-39.-Dynamo-GPU-Planner.png?fit=1024,554&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>NVIDIA Dynamo’ KV-Cache Offload Manager allows more efficient overall execution of prefill overall by saving the KVCache from prior user conversations in NVMe storage rather than discarding it.<br>NVIDIA Dynamo 的 KV-Cache Offload Manager 通过将先前用户对话中的 KVCache 保存在 NVMe 存储中，而不是丢弃它，从而实现了更高效的整体预填充执行。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-40.-Nvidia-Dynamo-Architecture.png?fit=1024,633&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>When a user engages in an ongoing multi response conversation with an LLM, the LLM needs to factor in the prior questions and responses earlier in the conversation, taking these as input tokens as well. In the naïve implementation, the inference system will have discarded the KV Cache originally used for generating those earlier questions and responses, meaning that the KV Cache will have to be computed again, repeating the same set of calculations.<br>当用户与 LLM 进行持续的多响应对话时，LLM 需要考虑对话中之前的问题和响应，将这些也作为输入标记。在简单的实现中，推理系统将会丢弃最初用于生成那些早期问题和响应的 KV 缓存，这意味着 KV 缓存必须重新计算，重复相同的计算集。</p><p>Instead, with Dynamo’s NVMe KVCache offload feature, when a user steps away, the KVCache can be offloaded to an NVMe storage system until the user returns to the conversation. When the user asks a subsequent question in the conversation, the KVCache can be quickly retrieved from the NVMe storage system, obviating the need to calculate the KVCache again.<br>相反，使用 Dynamo 的 NVMe KVCache 卸载功能，当用户离开时，KVCache 可以卸载到 NVMe 存储系统，直到用户返回对话。当用户在对话中提出后续问题时，KVCache 可以从 NVMe 存储系统快速检索，避免再次计算 KVCache 的需要。</p><p>This frees up capacity on the prefill nodes to handle more incoming volume, or alternative could reduce the size of the prefill deployment needed. The user will also have a much better experience with faster time to first token as there is now much less time needed to retrieve the KV Cache vs computing it.<br>这释放了预填充节点上的容量，以处理更多的传入流量，或者可以减少所需的预填充部署的大小。用户的体验也会更好，因为获取 KV 缓存所需的时间现在大大减少，从而使首次令牌的时间更快。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-42.-Dynamo-Distributed-KVcache.png?fit=1014,683&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>These features for KVCache offload will matter more and more as RLVR and multi-agent systems with tool use become more common.<br>随着 RLVR 和使用工具的多智能体系统变得越来越普遍，KVCache 卸载的这些功能将变得越来越重要。</p><p><strong>We genuinely want to see another effective competitor to Nvidia and want to help AMD get to that spot</strong>. AMD has made great progress over the past four months but there are still a lot of changes that AMD needs to make to be competitive with Nvidia. Earlier in the article, we have outlined in detail our recommendations to Lisa Su and the AMD Leadership Team, and we provide a summary here:<br>我们真心希望看到另一个有效的竞争者出现，以对抗 Nvidia，并希望帮助 AMD 达到那个位置。过去四个月，AMD 取得了很大进展，但仍有许多变化需要进行，以便与 Nvidia 竞争。在文章的前面部分，我们详细列出了对 Lisa Su 和 AMD 领导团队的建议，这里提供一个总结：</p><ol><li>AMD needs to maintain (if not intensify) their sense of urgency to even have a chance at being on par with NVIDIA.<br> AMD 需要保持（如果不是加大）他们的紧迫感，才能有机会与 NVIDIA 平起平坐。</li><li>AMD leadership team’s biggest blind spot is low total compensation (base + RSU + bonus) for their AI software engineers due to incorrect compensation structure benchmarking to semiconductor companies instead of companies that are great at AI software. We have discussed how our recommendation will help strengthen the alignment of engineers’ compensation to AMD’s success (or failure). <strong>We strongly believe that if AMD does not significantly increase their AI Software Engineer pay, AMD will continue losing to Nvidia</strong><strong>.</strong><br> AMD 领导团队最大的盲点是对其 AI 软件工程师的总薪酬（基本工资 + RSU + 奖金）过低，因为他们的薪酬结构基准错误地参考了半导体公司，而不是那些在 AI 软件方面表现出色的公司。我们已经讨论过我们的建议将如何帮助加强工程师薪酬与 AMD 成功（或失败）之间的对齐。我们坚信，如果 AMD 不显著提高其 AI 软件工程师的薪酬，AMD 将继续输给 Nvidia。</li><li>We recommend that AMD invest heavily in Python interfaces at every layer of the ROCm stack, and not just in Python Kernel Authoring DSLs.<br> 我们建议 AMD 在 ROCm 堆栈的每一层大力投资 Python 接口，而不仅仅是在 Python 内核编写 DSL 中。</li><li>AMD needs to invest heavily in a team of 20+ developer relations engineers hosting In Real Life events and interacting with the community at a deeper level.<br> AMD 需要在一个 20 多人的开发者关系工程师团队上进行大量投资，举办线下活动并与社区进行更深入的互动。</li><li>Unlike NVIDIA GTC, AMD doesn’t hold any developer-focused conferences and only holds product launch keynote events, for instance <em>“Advancing AI”.</em> We recommend that AMD host an in‑person “ROCm Developer Conference”.<br> 与 NVIDIA GTC 不同，AMD 并不举办任何以开发者为中心的会议，只举办产品发布主题演讲活动，例如“推进人工智能”。我们建议 AMD 举办一次线下的“ROCm 开发者大会”。</li><li>NVIDIA has launched their <a href="https://github.com/ai-dynamo/dynamo">Dynamo disaggregated prefill inference framework called</a> and <a href="https://github.com/ai-dynamo/nixl">their NIXL inference KV cache tiering library</a>. AMD does not have first-class support for disaggregated prefill or NVMe KVCache tiering. They need to rapidly make progress on this, or they will fall behind on inference.<br> NVIDIA 推出了他们的 Dynamo 分散预填推理框架和他们的 NIXL 推理 KV 缓存分层库。AMD 对分散预填或 NVMe KVCache 分层没有一流的支持。他们需要迅速在这方面取得进展，否则他们将在推理方面落后。</li><li>AMD should give ROCm collective engineers a persistent cluster of at least 1,024 MI300 class GPUs that are for the exclusive of this team. This will go a long way towards helping RCCL catch up to NCCL.<br> AMD 应该为 ROCm 集体工程师提供一个至少包含 1,024 个 MI300 级 GPU 的持久集群，专门供该团队使用。这将大大有助于 RCCL 赶上 NCCL。</li><li>Nvidia overstates TFLOP&#x2F;s in their chip specifications, but AMD overstates TFLOP&#x2F;s specifications even more. <a href="https://rocm.blogs.amd.com/software-tools-optimization/Understanding_Peak_and_Max-Achievable_FLOPS/README.html">Even in AMD’s own blog, they have admitted to a significantly larger gap between their marketed TFLOP&#x2F;s and what is achievable</a> than users might experience with NVIDIA.<br> Nvidia 在其芯片规格中夸大了 TFLOP&#x2F;s，但 AMD 对 TFLOP&#x2F;s 规格的夸大更为严重。即使在 AMD 自己的博客中，他们也承认其宣传的 TFLOP&#x2F;s 与实际可实现的之间存在显著更大的差距，用户可能会体验到与 NVIDIA 的差距。</li><li>AMD should state their Model FLOPS Utilization (MFU) and TFLOP&#x2F;s&#x2F;GPU whenever they publicly announce a new in-house model training run. <a href="https://rocm.blogs.amd.com/artificial-intelligence/introducing-instella-3B/README.html">AMD does not currently do this</a>. We have asked AMD repeatedly about their MFU but so far, we have not received a satisfactory answer. This could lead one to assume their MFU is quite low.<br> AMD 应该在公开宣布新的内部模型训练时说明他们的模型浮点运算性能利用率 (MFU) 和每个 GPU 的 TFLOP&#x2F;s。AMD 目前并没有这样做。我们已经多次询问 AMD 关于他们的 MFU，但到目前为止，我们还没有收到令人满意的答复。这可能让人假设他们的 MFU 相当低。</li><li>In contrast to <a href="https://github.com/NVIDIA/pyxis">Nvidia’s Pyxis solution</a>, first-class support for AMD SLURM for containers is non-existent. AMD’s should invest in <a href="https://www.schedmd.com/">SchedMD (maintainers of SLURM)</a> consulting to help get containers in AMD SLURM to be on par with NVIDIA.<br>与 Nvidia 的 Pyxis 解决方案相比，AMD 对容器的 SLURM 支持几乎不存在。AMD 应该投资于 SchedMD（SLURM 的维护者）咨询，以帮助使 AMD SLURM 中的容器与 NVIDIA 的水平相当。</li><li>AMD should open source and make all of their CI&#x2F;CD and dashboards publicly accessible across all ROCm libraries (HipBLASLt, Sglang, vLLM,TransformerEngine, etc). <a href="https://hud.pytorch.org/benchmark/compilers">Currently, the only publicly accessible ROCm CI for their ML libraries is PyTorch</a>.<br>AMD 应该开源并使其所有 CI&#x2F;CD 和仪表板在所有 ROCm 库（HipBLASLt、Sglang、vLLM、TransformerEngine 等）中公开可访问。目前，唯一可以公开访问的 ROCm CI 是他们的 ML 库 PyTorch。</li><li>Currently AMD has internal clusters on a short-term basis on a burst model. But since demand for compute matches the available supply of compute, that means that there is a lot of development projects and efforts that can’t be carried out. This happens often when engineers are not able to convince capacity gatekeepers to provide burst compute capacity to carry out this research. The roadblock is the fact that there is an effective P&amp;L attached to every GPU hour. The situation at Nvidia is completely different as their internal clusters are persistent and multi-year. This gives Nvidia’s engineers a large degree of freedom to be creative and work on higher risk projects on spare capacity on the cluster without an accountant hovering over them. <strong>AMD has over 5 billion dollars of cash on hand, and has the financial ability to invest more heavily into internal clusters</strong>.<br>目前，AMD 在短期内以突发模型拥有内部集群。但由于计算需求与可用计算供给相匹配，这意味着有很多开发项目和努力无法进行。当工程师无法说服容量管理者提供突发计算能力以进行研究时，这种情况经常发生。障碍在于每个 GPU 小时都有一个有效的损益表。Nvidia 的情况完全不同，因为他们的内部集群是持久的且是多年的。这使得 Nvidia 的工程师在集群的闲置容量上有很大的自由度，可以进行创造性工作和高风险项目，而不必担心会计人员的监督。AMD 目前手头有超过 50 亿美元的现金，具备更大力度投资内部集群的财务能力。</li><li>Most of AMD’s internal cluster are rented for sub-1 year basis. This means that their customers will still be using MI300 in 2027 at which point AMD’s internal MI300 volume will be very low given contract expiries, leading to poor long-term support for “vintage” generations of GPUs. AMD should change their internal cluster procurement strategy to commit to multi-year procurement so as to enable long-term support of each GPU generation. <strong>If AMD internal clusters aren’t even committing multiple years to each GPU generation, why should their customers commit to long-term ownership of AMD GPUs?</strong><br>AMD 的大部分内部集群是以不到 1 年的基础租赁的。这意味着到 2027 年，他们的客户仍将使用 MI300，而届时 AMD 内部的 MI300 数量将非常低，考虑到合同到期，这将导致对“老旧”代 GPU 的长期支持不足。AMD 应该改变其内部集群采购策略，承诺进行多年采购，以便能够对每一代 GPU 提供长期支持。如果 AMD 内部集群甚至没有对每一代 GPU 承诺多年，为什么他们的客户要承诺长期拥有 AMD 的 GPU 呢？</li><li>AMD’s software infrastructure layer (i.e. Kubernetes, SDC detectors, health checks, SLURM, docker, metrics exporters) has made some progress over the past four months but the rate of progress is much slower than the rate of progress of AMD’s ML libraries. We recommend AMD executives investigate investing more engineering resources into AMD’s AI software infrastructure layer.<br>AMD 的软件基础设施层（即 Kubernetes、SDC 探测器、健康检查、SLURM、docker、指标出口）在过去四个月中取得了一些进展，但进展速度远低于 AMD 的 ML 库的进展速度。我们建议 AMD 高管考虑投入更多工程资源到 AMD 的 AI 软件基础设施层。</li><li>Jensen has been donating DGX B200 boxes to academic labs like <a href="https://x.com/vllm_project/status/1893001644037566610">Berkeley Sky lab</a>, <a href="https://x.com/scsatcmu/status/1912910889566490821?s=46">CMU Catalyst Research Group</a> and <a href="https://x.com/haoailab/status/1914402516420440072">many other university labs</a>. We recommend that AMD also support the academic ecosystem. It is incredibly easy win for AMD marketing to ship boxes and post photos with PhD students grinning next to a shiny AMD box.<br>詹森一直在向伯克利天空实验室、CMU 催化研究小组以及许多其他大学实验室捐赠 DGX B200 盒子。我们建议 AMD 也支持学术生态系统。对 AMD 的市场营销来说，寄送盒子并发布与博士生在闪亮的 AMD 盒子旁边微笑的照片是一个非常简单的胜利。</li></ol><p>For the past couple of years, NVIDIA leadership has internally viewed <a href="https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/">Huawei</a> as the company with the <a href="https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/">highest probability of being competitive with NVIDIA</a>. Due to the rapid improvement and sense of urgency from AMD, we believe that NVIDIA should instead think of AMD as their main competitor as well. We make the following recommendations to Jensen if they want to continue being the market leader:<br>在过去的几年中，NVIDIA 的领导层内部认为华为是与 NVIDIA 竞争概率最高的公司。由于 AMD 的快速进步和紧迫感，我们认为 NVIDIA 也应该将 AMD 视为主要竞争对手。如果他们想继续保持市场领导地位，我们向 Jensen 提出以下建议：</p><ol><li>Continue to rapidly expand the API surface area with useful new features. If NVIDIA expands their API surface area faster than AMD can copy&#x2F;port to make it ROCm compatible, NVIDIA will continue to be the market leader. Recent launches across the CUDA Python stack was a great example of NVIDIA massively increasing its API surface area with useful new features.<br> 继续快速扩展 API 表面，增加有用的新功能。如果 NVIDIA 扩展其 API 表面速度快于 AMD 复制&#x2F;移植以使其兼容 ROCm，NVIDIA 将继续成为市场领导者。最近在 CUDA Python 堆栈上的发布是 NVIDIA 通过有用的新功能大幅增加其 API 表面的一个很好的例子。</li><li>For many developers, working on Nvidia consumer GPUs is a gateway to working on the broader CUDA ecosystem. Unfortunately, due to NVIDIA consumer EULA, PyTorch and most other ML libraries are unable to host consumer NVIDIA GPUs in their CI&#x2F;CD, leading to a suboptimal experience on NVIDIA GPUs. We recommend that NVIDIA explore a strategy to get NVIDIA consumer GPUs into PyTorch CI&#x2F;CD.<br> 对于许多开发者来说，使用 Nvidia 消费级 GPU 是进入更广泛的 CUDA 生态系统的一个入口。不幸的是，由于 NVIDIA 消费级 EULA，PyTorch 和大多数其他机器学习库无法在其 CI&#x2F;CD 中托管消费级 NVIDIA GPU，这导致在 NVIDIA GPU 上的体验不佳。我们建议 NVIDIA 探索一种策略，将 NVIDIA 消费级 GPU 引入 PyTorch CI&#x2F;CD。</li><li><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/bufferreg.html">NCCL User Buffer Registration</a> reduces memory pressure which allows for larger batch sizes and less activation recomputation leading to ~5-10% increase in performance. Although <a href="https://github.com/pytorch/pytorch/pull/133603">basic integration with low level PyTorch APIs</a> is supported, there is currently no integration into the common APIs such as DistributedDataParallel (DDP), DTensors, or FullySharedDataParallel (FSDP). We recommend that NVIDIA integrates the user buffer registration feature across the whole PyTorch stack.<br> NCCL 用户缓冲区注册减少了内存压力，这允许更大的批量大小和更少的激活重计算，从而导致性能提高约 5-10%。尽管支持与低级 PyTorch API 的基本集成，但目前尚未与常见 API（如 DistributedDataParallel（DDP）、DTensors 或 FullySharedDataParallel（FSDP））集成。我们建议 NVIDIA 在整个 PyTorch 堆栈中集成用户缓冲区注册功能。</li><li>Although NVIDIA’s out of box experience is better than AMD, there is still room for improvement. <a href="https://github.com/pytorch/pytorch/pull/146388">For example, for developer to get optimal performance for RMSNorm, they need to use NVIDIA&#x2F;apex library instead of having it out of the box in PyTorch</a>. RMSNorm is an extremely common layer in SOTA LLMs. We recommend NVIDIA work with the Meta PyTorch team on figuring out a strategy to integrate fast RMSNorm kernels and other cuDNN kernels directly into PyTorch.<br> 尽管 NVIDIA 的开箱体验优于 AMD，但仍有改进的空间。例如，为了让开发者获得 RMSNorm 的最佳性能，他们需要使用 NVIDIA&#x2F;apex 库，而不是在 PyTorch 中开箱即用。RMSNorm 是 SOTA LLMs 中极为常见的层。我们建议 NVIDIA 与 Meta PyTorch 团队合作，找出将快速 RMSNorm 内核和其他 cuDNN 内核直接集成到 PyTorch 中的策略。</li><li>NVIDIA needs to pay for additional CI H100s so that PyTorch can enable H100 for their <a href="https://hud.pytorch.org/benchmark/compilers">TorchInductor Benchmark CI</a>. Even AMD has enabled already MI300 for this Benchmark CI.<br> NVIDIA 需要支付额外的 CI H100，以便 PyTorch 可以为他们的 TorchInductor Benchmark CI 启用 H100。甚至 AMD 已经为这个 Benchmark CI 启用了 MI300。</li><li>Over the past four months, AMD has done a great job making most of their benchmarks easily reproducible by the community. For example, <a href="https://rocm.blogs.amd.com/artificial-intelligence/reproducing-amd-mlperf-inference-submission/README.html">AMD wrote a great blog on how to reproduce their MLPerf Inference 5.0 submission</a>. If NVIDIA wants the ML community to have confidence in the benchmarks that NVIDIA posts, we recommend that NVIDIA provide reproducible instructions and an explanatory blog post whenever they post benchmark results.<br> 在过去的四个月里，AMD 在让大多数基准测试易于被社区复现方面做得很好。例如，AMD 写了一篇很好的博客，介绍如何复现他们的 MLPerf Inference 5.0 提交。如果 NVIDIA 希望 ML 社区对 NVIDIA 发布的基准测试结果有信心，我们建议 NVIDIA 在发布基准测试结果时提供可复现的说明和解释性博客文章。</li><li>A meaningful number of NVIDIA’s open source libraries are very poor at following the “open source” ethos and do code dumps at every release. Examples include NCCL and CUTLASS. We have seen progress in some open-source libraries such as <a href="https://github.com/NVIDIA/TensorRT-LLM/pull/2980">trt-llm when they moved towards a Github first approach</a>. We recommend that NVIDIA embrace the open-source ethos across all the open libraries.<br> 许多 NVIDIA 的开源库在遵循“开源”精神方面表现很差，并在每次发布时进行代码倾倒。例子包括 NCCL 和 CUTLASS。我们已经看到一些开源库如 trt-llm 在采用 Github 优先的方法时取得了进展。我们建议 NVIDIA 在所有开源库中拥抱开源精神。</li><li>Stop promoting <a href="https://semianalysis.com/2025/03/19/nvidia-gtc-2025-built-for-reasoning-vera-rubin-kyber-cpo-dynamo-inference-jensen-math-feynman/#jensen-math-changes-every-year">marketing Jensen Math 2:4 sparsity FLOPs specifications, curtail the unannounced use of bi-directional bandwidth conventions so as to reduce confusion across the whole ecosystem</a>. Avoid overstating dense FLOP&#x2F;s specifications and instead move to publishing a FLOP&#x2F;s metric that reflects what is achievable for a real-world normal input distribution rather than for an <a href="https://github.com/NVIDIA/cutlass/blob/main/media/images/cutlass-3.8-blackwell-gemm-peak-performance.svg">unrealistic [-4, 4] uniform discrete integer distribution.</a><br> 停止推广市场营销 Jensen Math 2:4 稀疏 FLOPs 规格，限制未公告的双向带宽规范的使用，以减少整个生态系统的混淆。避免夸大密集 FLOP&#x2F;s 规格，而是转向发布一个反映实际可实现的 FLOP&#x2F;s 指标，基于现实世界的正常输入分布，而不是不切实际的[-4, 4]均匀离散整数分布。</li><li>Look into hiring AMD’s engineers that contribute to libraries such as <a href="https://github.com/ROCm/rccl/graphs/contributors">RCCL</a>, <a href="https://github.com/ROCm/composable_kernel/graphs/contributors">ComposableKernels</a>, <a href="https://github.com/ROCm/hipBLASLt/graphs/contributors">hipBLASLt</a>, <a href="https://github.com/pytorch/pytorch/pulls?q=is:pr+is:open+%5BROCm%5D">ROCm&#x2F;PyTorch</a>, etc. by <a href="https://github.com/ROCm/rccl/graphs/contributors">looking the contributor tab on Github</a> and searching on the <a href="https://github.com/pytorch/pytorch/pulls?q=is:pr+is:open+%5BROCm%5D">“[ROCm]” PR tag on Github</a>.<br> 考虑雇佣为 RCCL、ComposableKernels、hipBLASLt、ROCm&#x2F;PyTorch 等库做出贡献的 AMD 工程师，可以通过查看 Github 上的贡献者标签和在 Github 上搜索“[ROCm]” PR 标签来实现。</li></ol><h2 id="MI325X-and-MI355X-Customer-InterestMI325X-和-MI355X-客户兴趣"><a href="#MI325X-and-MI355X-Customer-InterestMI325X-和-MI355X-客户兴趣" class="headerlink" title="MI325X and MI355X Customer InterestMI325X 和 MI355X 客户兴趣"></a>MI325X and MI355X Customer InterestMI325X 和 MI355X 客户兴趣</h2><p>There has been a <a href="https://semianalysis.com/accelerator-industry-model/">lack of interest from customers in purchasing the MI325X as we’ve been saying for a year.</a> It was supposed to be a competitor to H200, but the MI325X started shipment in Q2 2025, about three quarters after the H200 and at the exact same time as Blackwell mass production. The obvious customer choice has been the much lower cost per performance Blackwell and so the release of the MI325X was too little too late and AMD was only able to sell minimal volumes of the MI325.<br>正如我们所说的，客户对购买 MI325X 的兴趣一直很低。它本应是 H200 的竞争对手，但 MI325X 在 2025 年第二季度才开始发货，比 H200 晚了大约三个季度，并且与 Blackwell 的大规模生产同时进行。显而易见，客户的选择是性价比更高的 Blackwell，因此 MI325X 的发布为时已晚，AMD 仅能销售极少量的 MI325。</p><p><a href="https://semianalysis.com/accelerator-industry-model/">Our demand view in the Accelerator Model tracked Microsoft’s disappointment early in 2024 and lack of follow on orders throughout the rest of 2024</a>. We believe that there is renewed interest in AMD GPUs from <a href="https://semianalysis.com/accelerator-industry-model/">OpenAI via Oracle</a> and <a href="https://semianalysis.com/accelerator-industry-model/">a few other major customers</a>, but still not Microsoft, on the condition that they reach a sweet-heart pricing with AMD. To be clear, MI355X is still not competitive with NVIDIA’s rack scale GB200 NVL72 solution because the MI355X’s scale-up world size is still only 8 GPUs while for NVIDIA’s GB200 NVL72 the world size is 72 GPUs.<br>我们在加速器模型中的需求视图跟踪了微软在 2024 年初的失望以及在 2024 年剩余时间内缺乏后续订单。我们相信，OpenAI 通过 Oracle 和其他一些主要客户对 AMD GPU 的兴趣重新燃起，但前提是他们与 AMD 达成优惠价格，微软仍然不在其中。明确来说，MI355X 仍然无法与 NVIDIA 的机架规模 GB200 NVL72 解决方案竞争，因为 MI355X 的扩展世界规模仍然只有 8 个 GPU，而 NVIDIA 的 GB200 NVL72 的世界规模为 72 个 GPU。</p><p>AMD’s pitch on the competitiveness of MI355X centers around the fact that it doesn’t require direct to chip liquid cooling (DLC). There certainly is merit to some point, but there is a degree of irony to the fact that AMD is still pitching the next gen MI355X as a competitor to Nvidia’s last-gen economy-class products. AMD’s MI355X cannot compete head on with NVIDIA’s flagship GB200 NVL72 for frontier reasoning inferencing due to the smaller scale-up world size mentioned above, so it is instead positioned to compete with the air-cooled HGX B200 NVL8 and the air-cooled HGX B300 NVL16.<br>AMD 对 MI355X 竞争力的宣传围绕着它不需要直接到芯片的液体冷却（DLC）这一事实。某种程度上确实有其道理，但 AMD 仍将下一代 MI355X 宣传为与 Nvidia 上一代经济型产品的竞争者，这一点有些讽刺。由于上述提到的较小的规模化世界大小，AMD 的 MI355X 无法与 NVIDIA 的旗舰 GB200 NVL72 在前沿推理推断上正面竞争，因此它被定位为与空气冷却的 HGX B200 NVL8 和空气冷却的 HGX B300 NVL16 竞争。</p><p>With that said, <a href="https://semianalysis.com/accelerator-industry-model/">this product segment will ship meaningful volumes</a> and depending on the MI355X’s software quality and the price that AMD is willing to sell at, the MI355X could be decently competitive on performance per TCO basis when compared to NVIDIA’s HGX. This is particularly true for small to medium models that do not benefit from large scale-up world sizes. However, we believe that GB200 NVL72 will win on performance and perf per TCO when it comes to reasoning models and frontier inferencing that do benefit from large disaggregated deployments or mixture of experts that best harness large scale-up networks.<br>话虽如此，这个产品细分市场将会发货有意义的数量，具体取决于 MI355X 的软件质量以及 AMD 愿意出售的价格，MI355X 在性能与 TCO 的基础上与 NVIDIA 的 HGX 相比可能会具有相当的竞争力。这对于不受大规模世界规模影响的小型到中型模型尤其如此。然而，我们相信 GB200 NVL72 在推理模型和前沿推理方面将会在性能和每 TCO 性能上胜出，因为这些模型受益于大型分散部署或最佳利用大规模网络的专家混合。</p><p>Below, we will discuss what we see on MI355X, MI420X, MI450X, UALink, Infinity Fabric over Ethernet, and pricing.<br>下面，我们将讨论我们在 MI355X、MI420X、MI450X、UALink、以太网上的 Infinity Fabric 以及定价方面看到的内容。</p><h3 id="Subscriber-Content-订阅者内容"><a href="#Subscriber-Content-订阅者内容" class="headerlink" title="Subscriber Content 订阅者内容"></a>Subscriber Content 订阅者内容</h3><h2 id="The-MI355X-Microarchitecture-CDNA4-–-SemiAnalysis-EstimatesMI355X-微架构-CDNA4-–-SemiAnalysis-估计"><a href="#The-MI355X-Microarchitecture-CDNA4-–-SemiAnalysis-EstimatesMI355X-微架构-CDNA4-–-SemiAnalysis-估计" class="headerlink" title="The MI355X Microarchitecture (CDNA4) – SemiAnalysis EstimatesMI355X 微架构 (CDNA4) – SemiAnalysis 估计"></a>The MI355X Microarchitecture (CDNA4) – SemiAnalysis EstimatesMI355X 微架构 (CDNA4) – SemiAnalysis 估计</h2><p>The MI355 microarchitecture (CDNA4) will feature a Hopper SM90 style microarchitecture with Thread Block cluster, Tensor Memory Accelerator (TMA), TMA multicasting, wgmma and async data movements and async mma. We believe that CDNA4’s TMA equivalent will be called ROCm Memory Accelerator (RMA). This will allow CDNA4 to have more efficient data movements compared to CDNA3 but unfortunately picoJoules per bit &amp; picoJoules per FLOP will still be higher than Blackwell’s SM100 uarch. This is due to CDNA4 not having tensor memory, 2 CTA mma, or preferred thread block cluster size.<br>MI355 微架构 (CDNA4) 将采用 Hopper SM90 风格的微架构，配备线程块集群、张量内存加速器 (TMA)、TMA 多播、wgmma 以及异步数据移动和异步 mma。我们相信 CDNA4 的 TMA 等效物将被称为 ROCm 内存加速器 (RMA)。这将使 CDNA4 在数据移动方面比 CDNA3 更高效，但不幸的是，单位比特的皮焦耳和单位浮点运算的皮焦耳仍将高于 Blackwell 的 SM100 微架构。这是因为 CDNA4 没有张量内存、2 CTA mma 或首选线程块集群大小。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-43.-Blackwell-TMEM.png?fit=1024,558&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>Like the MI300X and the MI325X, the MI355X also wastes too much silicon area on FP64 cores which are never used in AI. The reason AMD hasn’t been able to bring itself to pare down silicon usage on FP64 cores is that that AMD has historically done well in greybeard High-Performance Computing (HPC) applications that require FP64. The problem is that catering to this now small segment of HPC customers is now holding AMD back from capturing the now far larger AI customer segment that prefers to use number formats such as MX4&#x2F;6&#x2F;8, FP4&#x2F;FP8 or FP16.<br>与 MI300X 和 MI325X 一样，MI355X 在 FP64 核心上也浪费了太多硅面积，而这些核心在 AI 中从未被使用。AMD 未能削减 FP64 核心的硅使用量的原因在于，AMD 在历史上在需要 FP64 的老牌高性能计算（HPC）应用中表现良好。问题在于，迎合这一现在已小得多的 HPC 客户群体，正在阻碍 AMD 捕捉现在更大、偏好使用 MX4&#x2F;6&#x2F;8、FP4&#x2F;FP8 或 FP16 等数字格式的 AI 客户群体。</p><p>NVIDIA is pursuing a different strategy. When moving to Blackwell from Hopper, Nvidia has reduced the number of FP64 tensor cores by 50% and are instead using software emulation with 7-8 INT8 tensor core operations to still achieve decent FP64 performance. This silicon is then re-allocated to tensor cores for MX4 tensor cores which are far more desirable to AI Lab customers. The below table illustrates how cuBLAS FP64 emulation nonetheless delivers meaningful TFLOP improvements for the B200 over the H200 in FP64.<br>NVIDIA 正在追求不同的战略。在从 Hopper 迁移到 Blackwell 时，Nvidia 将 FP64 张量核心的数量减少了 50%，而是使用 7-8 个 INT8 张量核心操作进行软件仿真，以仍然实现不错的 FP64 性能。这些硅片随后被重新分配给 MX4 张量核心，这对 AI 实验室客户来说更具吸引力。下表说明了 cuBLAS FP64 仿真如何在 FP64 方面为 B200 提供了相对于 H200 的显著 TFLOP 提升。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-45.-cuBLAS.png?fit=1024,578&ssl=1"></p><p>Source: NVIDIA 来源：NVIDIA</p><p>This emulation works decently well that we in fact believe that NVIDIA will remove FP64 tensor cores almost entirely in the future and shift to only using software emulation to provide FP64.<br>这种仿真效果相当不错，我们实际上相信 NVIDIA 将在未来几乎完全移除 FP64 张量核心，并转而仅使用软件仿真来提供 FP64。</p><h2 id="MI400-Series-Flexible-IOMI400-系列灵活-IO"><a href="#MI400-Series-Flexible-IOMI400-系列灵活-IO" class="headerlink" title="MI400 Series Flexible IOMI400 系列灵活 IO"></a>MI400 Series Flexible IOMI400 系列灵活 IO</h2><p>AMD’s learned from their mistake on MI300 with regard to infinity fabric being much worse than NVLink while also recognizing they don’t have the hardware talent to actually execute on an NVSwitch equivalent. Furthermore, they don’t want to encroach on the industry ecosystem and verticalize. As such they have gone with the shotgun approach of supporting everything under the sun.<br>AMD 从 MI300 的错误中吸取了教训，意识到无限织物远不如 NVLink，同时也认识到他们没有硬件人才来实际执行 NVSwitch 的等效方案。此外，他们不想侵犯行业生态系统并进行垂直整合。因此，他们采取了支持所有事物的广泛策略。</p><p>In come flexible IO lanes. Instead of having seperate SerDes and IO paths for each different type of IO such as PCIe and Scale Up, AMD has gone with having 144 lanes of IO that can support many different standards. They can support PCIe 6.0, Infinity Fabric at 64G, UALink at 128G, xGMI 4 at 128G which is somewhat of a superset of UALink, and Infinity Fabric over Ethernet at 212G. This enables AMD silicon team to have maximum flexibility for various different use cases.<br>灵活的 IO 通道来了。AMD 并没有为每种不同类型的 IO（如 PCIe 和 Scale Up）设置单独的 SerDes 和 IO 路径，而是采用了 144 条 IO 通道，可以支持多种不同的标准。它们可以支持 PCIe 6.0、64G 的 Infinity Fabric、128G 的 UALink、128G 的 xGMI 4（这在某种程度上是 UALink 的超集）以及 212G 的以太网 Infinity Fabric。这使得 AMD 的硅团队在各种不同的使用案例中具有最大的灵活性。</p><p>AMD with this can do scale up UALink or Ethernet. They can do SSDs directly attached to the GPU. They can have Infinity fabric directly connected to GPUs. They can have NICs attached via UALink. The possibilities are almost endless. It is an incredibly large array of permutations for systems and a lot of changes can happen.<br>AMD 可以通过此实现 UALink 或以太网的扩展。他们可以将 SSD 直接连接到 GPU。他们可以将 Infinity Fabric 直接连接到 GPU。他们可以通过 UALink 连接 NIC。可能性几乎是无穷无尽的。这是一个极其庞大的系统排列组合，许多变化可以发生。</p><p>As such there are a number of different racks AMD is considering from Torus fabrics with UALink to up to 256 GPU Ethernet based fabrics. We will detail the most likely current racks below, but its over a year from mass production and customers are still discussing their options. This incredible flexibility comes at a cost though.<br>因此，AMD 正在考虑多种不同的机架，从带有 UALink 的 Torus fabrics 到最多 256 个 GPU 的以太网基础的 fabrics。我们将在下面详细介绍最可能的当前机架，但距离大规模生产还有一年多的时间，客户仍在讨论他们的选择。不过，这种令人难以置信的灵活性是有代价的。</p><p>With different forms of IO, the silicon engineering is not easy. AMD has to make SerDes and data paths that work with all of these different permutations. This is an incredibly hard engineering path fraught with engineering risk.<br>随着不同形式的 IO，硅工程并不容易。AMD 必须制造与所有这些不同排列兼容的 SerDes 和数据路径。这是一条充满工程风险的极其困难的工程路径。</p><h2 id="MI430X-UL4-–-SemiAnalysis-EstimatesMI430X-UL4-–-SemiAnalysis-估计"><a href="#MI430X-UL4-–-SemiAnalysis-EstimatesMI430X-UL4-–-SemiAnalysis-估计" class="headerlink" title="MI430X UL4 – SemiAnalysis EstimatesMI430X UL4 – SemiAnalysis 估计"></a>MI430X UL4 – SemiAnalysis EstimatesMI430X UL4 – SemiAnalysis 估计</h2><p>Due to AMD’s accelerated roadmap, we believe that they are skipping MI375X (CDNA4 refresh) and going straight to their CDNA-NEXT generation of chips.<br>由于 AMD 加速的路线图，我们认为他们跳过了 MI375X（CDNA4 刷新），直接进入他们的 CDNA-NEXT 代芯片。</p><p>For AMD’s CDNA-NEXT generation of chips, we believe that AMD will release two SKUs – one targeted at FP64 boomer HPC workloads and another targeted at AI workloads that will not have many FP64 tensor cores. They both consist of 2 IO dies, 2 base active interposers, and 4 compute dies 3D stacked.<br>对于 AMD 的 CDNA-NEXT 代芯片，我们相信 AMD 将发布两个 SKU——一个针对 FP64 高性能计算工作负载，另一个针对 AI 工作负载，后者将不会有太多 FP64 张量核心。它们都由 2 个 IO 芯片、2 个基础活动中介层和 4 个 3D 堆叠的计算芯片组成。</p><p>We believe that MI430X UL4 will be their HPC focused CDNA-NEXT chip. Since this is an HPC focused chip, the MI430X UL4 will have plenty of FP64 tensor cores. Because there will be zero UALink switches available for a while to come, the MI430X will only connect between 4 GPUs in a point-to-point mesh topology hence the “4” in UL4. This is the current limitation of UALink deployments that are available today – in the absence of a UALink switch taped out from Astera Lab, Enfabrica, XConn, or Auradine, UALink can only connect a low number of GPUs together. While they can do torus or mesh topologies, AMD doesn’t have the design or software resources to do both hence prioritizing only ethernet for scale up.<br>我们相信 MI430X UL4 将是他们专注于高性能计算的 CDNA-NEXT 芯片。由于这是一个专注于高性能计算的芯片，MI430X UL4 将拥有大量的 FP64 张量核心。由于在可预见的未来将没有可用的 UALink 交换机，MI430X 将仅在点对点网状拓扑中连接 4 个 GPU，因此 UL4 中的“4”。这是目前可用的 UALink 部署的限制——在没有来自 Astera Lab、Enfabrica、XConn 或 Auradine 的 UALink 交换机的情况下，UALink 只能将少量 GPU 连接在一起。虽然它们可以实现环形或网状拓扑，但 AMD 没有设计或软件资源来同时实现这两者，因此优先考虑以太网进行扩展。</p><p>The concept of UALink sounds amazing in theory, where there is an open standard for scale up. The unfortunate truth is that for some open standards, progress is very slow due to the numerous amount of committee meetings. Furthermore, AMD does not currently have their own internal UALink switching group which means they must rely on Astera Lab or Broadcom. We believe that from Broadcom perspective, UALink switches will be lower volume than Ethernet switches thus they are not investing enough resources to make the switch come to market quicker.<br>UALink 的概念在理论上听起来很棒，因为它有一个开放的扩展标准。不幸的事实是，对于某些开放标准，由于委员会会议的数量众多，进展非常缓慢。此外，AMD 目前没有自己的内部 UALink 交换组，这意味着他们必须依赖 Astera Lab 或 Broadcom。我们认为，从 Broadcom 的角度来看，UALink 交换机的销量将低于以太网交换机，因此他们没有投入足够的资源来使交换机更快上市。</p><p>To be clear, Ultra Ethernet for scale out open standards is going great &amp; will be widely adopted &amp; UEC ready switches are already on market.<br>明确来说，超以太网用于扩展开放标准进展顺利，且将被广泛采用，UEC 兼容的交换机已经上市。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-46.-AMD-UALink-1.png?fit=845,1024&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>There is a possibility from the hardware perspective for MI430X and MI455X to do a larger point to point network with some sort of Torus like Google TPUs, or other topology, but AMD isn’t investing the resources to enable that. Previously Alibaba wanted UALink Scale Up, but with the newest bans, we believe this will no longer be prioritized.<br>从硬件的角度来看，MI430X 和 MI455X 有可能实现更大规模的点对点网络，类似于谷歌 TPU 的某种环形拓扑或其他拓扑，但 AMD 并没有投入资源来实现这一点。之前阿里巴巴希望实现 UALink 扩展，但由于最新的禁令，我们认为这将不再被优先考虑。</p><p>In the next section, we will talk about the CDNA-NEXT generation SKU that is targeted for AI workload – MI450X.<br>在下一部分，我们将讨论针对 AI 工作负载的 CDNA-NEXT 代 SKU - MI450X。</p><h2 id="MI450X-Infinity-Fabric-over-Ethernet-IFoE-IFoE64-MI450X-IFoE128-–-SemiAnalysis-EstimatesMI450X-无限织物以太网-IFoE-IFoE64-和-MI450X-IFoE128-–-SemiAnalysis-估计"><a href="#MI450X-Infinity-Fabric-over-Ethernet-IFoE-IFoE64-MI450X-IFoE128-–-SemiAnalysis-EstimatesMI450X-无限织物以太网-IFoE-IFoE64-和-MI450X-IFoE128-–-SemiAnalysis-估计" class="headerlink" title="MI450X Infinity Fabric over Ethernet (IFoE) IFoE64 &amp; MI450X IFoE128 – SemiAnalysis EstimatesMI450X 无限织物以太网 (IFoE) IFoE64 和 MI450X IFoE128 – SemiAnalysis 估计"></a>MI450X Infinity Fabric over Ethernet (IFoE) IFoE64 &amp; MI450X IFoE128 – SemiAnalysis EstimatesMI450X 无限织物以太网 (IFoE) IFoE64 和 MI450X IFoE128 – SemiAnalysis 估计</h2><p>As we have written about many times, when it comes to AI compute, <a href="https://semianalysis.com/2023/04/12/google-ai-infrastructure-supremacy/">systems matter more than chip level performance</a>.<br>正如我们多次提到的，谈到人工智能计算时，系统比芯片级性能更为重要。</p><p>[<img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/11/b6fb8cda-5f23-477e-b4f3-27245364be1e_1088x828.png?fit=1024,779&ssl=1"></p><p>Apr 12, 2023<br>2023 年 4 月 12 日</p><h5 id="Google-AI-Infrastructure-Supremacy-Systems-Matter-More-Than-Microarchitecture谷歌人工智能基础设施的优势：系统比微架构更重要"><a href="#Google-AI-Infrastructure-Supremacy-Systems-Matter-More-Than-Microarchitecture谷歌人工智能基础设施的优势：系统比微架构更重要" class="headerlink" title="Google AI Infrastructure Supremacy: Systems Matter More Than Microarchitecture谷歌人工智能基础设施的优势：系统比微架构更重要"></a>Google AI Infrastructure Supremacy: Systems Matter More Than Microarchitecture谷歌人工智能基础设施的优势：系统比微架构更重要</h5><p>Dylan Patel, George Cozma, Gerald Wong<br>迪伦·帕特尔，乔治·科兹马，杰拉尔德·黄</p><p>](<a href="https://semianalysis.com/2023/04/12/google-ai-infrastructure-supremacy/">https://semianalysis.com/2023/04/12/google-ai-infrastructure-supremacy/</a>)</p><p>While ASIC competitors, Nvidia, and even <a href="https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/">Huawei</a> have offered their AI hardware in rack level scale-up form factor, AMD customers have thus far been stuck with a smaller and thus inferior scale up domain of only 8 accelerators versus the 72 offered for Nvidia to cite one example.<br>尽管 ASIC 竞争对手 Nvidia 甚至华为已经提供了机架级别的 AI 硬件，但 AMD 的客户迄今为止仍然被困在只有 8 个加速器的较小且劣质的扩展领域，而 Nvidia 则提供了 72 个加速器作为一个例子。</p><p>The pressure has been on for AMD to come up with its own rack-scale solution. However, Taiwanese ODMs, namely Quanta and Foxconn, have been focusing most of their engineering resources on supporting Nvidia, particularly with the well-known ongoing issues with GB200 NVL72 racks. Meanwhile, smaller ODMs are also busy as they are very focused on ASIC customers like Google, AWS, and OpenAI.<br>AMD 面临着推出自己机架级解决方案的压力。然而，台湾的 ODM，特别是广达和富士康，已经将大部分工程资源集中在支持 Nvidia 上，特别是针对 GB200 NVL72 机架的众所周知的持续问题。同时，较小的 ODM 也很忙，因为他们非常专注于像 Google、AWS 和 OpenAI 这样的 ASIC 客户。</p><p>This leaves very little spare capacity or incentive for ODM partners to develop rack scale solution for AMD. <a href="https://www.amd.com/en/newsroom/press-releases/2025-3-31-amd-completes-acquisition-of-zt-systems.html">AMD’s solution to this problem was to acquire ZT Systems, a US based ODM, for $4.9 billion USD in March 2025.</a> After the acquisition the engineering team at <a href="https://x.com/LisaSu/status/1910131191946121533">ZT will focus completely on developing the rack scale solution for AMD, dropping most of their work on Nvidia systems.</a> There are some smaller efforts from Celestica and Lenovo, but primarily it is AMD &#x2F; ZT.<br>这几乎没有给 ODM 合作伙伴留下任何额外的能力或动力来为 AMD 开发机架规模解决方案。AMD 对此问题的解决方案是在 2025 年 3 月以 49 亿美元收购美国 ODM ZT Systems。收购后，ZT 的工程团队将完全专注于为 AMD 开发机架规模解决方案，放弃大部分对 Nvidia 系统的工作。Celestica 和联想有一些小规模的努力，但主要还是 AMD &#x2F; ZT。</p><p>That said, AMD will not have their rack scale solution ready until the release of the MI450X, which is scheduled to launch in 2H26. This launch comes around the same time as Rubin, which will be offered in the Oberon (NVL144) form factor. In the sections below we will detail the concept of the MI450X rack scale solution and explain why it could be a competitive product to Rubin NVL144 when it launches.<br>也就是说，AMD 的机架规模解决方案要等到 MI450X 发布时才会准备好，MI450X 计划在 2026 年下半年发布。这个发布与 Rubin 同时进行，Rubin 将以 Oberon (NVL144) 形态提供。在下面的部分中，我们将详细介绍 MI450X 机架规模解决方案的概念，并解释为什么它在发布时可能成为 Rubin NVL144 的竞争产品。</p><p>First, we show the compute tray concept:<br>首先，我们展示计算托盘的概念：</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-47-1.png?fit=1024,743&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>We believe that four MI450X OAM modules will be socketed onto one compute board for serviceability and repairability. It is a reasonable assumption that there are four MI450X GPUs and one Venice CPUs per compute tray as x86 EPYC CPU is more powerful than Nvidia’s arm-based CPU. Furthermore, x86 CPU enables more seamline software integrations. MI450X’s TDP will be between 1600W and 2000W. Its FLOPS will be competitive to Rubin based going by the expected TDP.<br>我们相信四个 MI450X OAM 模块将被插入到一个计算板上，以便于维护和修理。合理的假设是每个计算托盘有四个 MI450X GPU 和一个 Venice CPU，因为 x86 EPYC CPU 比 Nvidia 的基于 ARM 的 CPU 更强大。此外，x86 CPU 使得更多的 Seamline 软件集成成为可能。MI450X 的 TDP 将在 1600W 到 2000W 之间。根据预期的 TDP，其 FLOPS 将与 Rubin 基于的性能具有竞争力。</p><p>The MI450X GPU will have direct access to three different levels of memory, which indicates that the rack solution will be optimized for inference workloads as the availability of multiple memory tiers enables more efficient KVCaching. It is also interesting to see a direct PCIe channel between GPU and SSD, whereas for the GB200, the GPU still accesses NVMe storage via the Grace CPU. The four tiers of memory:<br>MI450X GPU 将直接访问三种不同级别的内存，这表明机架解决方案将针对推理工作负载进行优化，因为多个内存层的可用性使得 KVCaching 更加高效。值得注意的是，GPU 和 SSD 之间有直接的 PCIe 通道，而对于 GB200，GPU 仍然通过 Grace CPU 访问 NVMe 存储。四个内存层：</p><ol><li>In-package HBM (288GB&#x2F;432GB, 18TB&#x2F;s)<br> 封装内 HBM（288GB&#x2F;432GB，18TB&#x2F;s）</li><li>Direct GPU Attached LPDDR5X through custom HBM (819GB&#x2F;s)<br> 通过定制 HBM 直接连接 GPU 的 LPDDR5X（819GB&#x2F;s）</li><li>Direct attached PCIe linked SSD<br> 直接连接的 PCIe 链接 SSD</li><li>CPU MR DIMM DDR5 over 16 lanes of 64G Infinity Fabric<br> CPU MR DIMM DDR5 超过 16 条 64G Infinity Fabric 通道</li></ol><p>The Direct GPU attached LPDDR5X is similar to the architecture of Rubin Ultra. The Direct attached PCIe linked SSD similar to NVIDIA HGX’s local NVMe GPUDirect Storage.<br>直接连接的 GPU 附加 LPDDR5X 类似于 Rubin Ultra 的架构。直接连接的 PCIe 链接 SSD 类似于 NVIDIA HGX 的本地 NVMe GPUDirect Storage。</p><p>The UAlink scale up switch will not be ready by 2H26, hence the scale up protocol of MI450X will be the ethernet based – Infinity Fabric over Ethernet (IFoE) T <strong>he IFoE scale up bandwidth of MI450X is at least 1.8TB&#x2F;s uni-di &#x3D; 72 lanes of 200Gbit&#x2F;s. This will be competitive with Rubin.</strong><br>UAlink 扩展开关将在 2H26 之前无法准备好，因此 MI450X 的扩展协议将基于以太网 - 以太网上的无限结构（IFoE）。MI450X 的 IFoE 扩展带宽至少为 1.8TB&#x2F;s 单向 &#x3D; 72 条 200Gbit&#x2F;s 的通道。这将与 Rubin 竞争。</p><p>Overall, we believe there will be three different SKUs of the MI450X – each with a different backend network configuration:<br>总体而言，我们认为 MI450X 将有三种不同的 SKU - 每种都有不同的后端网络配置：</p><ol><li>Three AMD 800GbE ethernet NICs per GPU (shown in the chart above)<br> 每个 GPU 配备三张 AMD 800GbE 以太网 NIC（如上图所示）<ol><li>UALink protocol between the GPU and the NIC<br> GPU 与 NIC 之间的 UALink 协议</li><li>2.4Tbit&#x2F;s per GPU 每个 GPU 2.4Tbit&#x2F;s</li><li>The most aggressive SKU in terms of scale out bandwidth.<br> 在扩展带宽方面最具攻击性的 SKU。</li></ol></li><li>Two AMD 800GbE ethernet NICs per GPU<br> 每个 GPU 配备两个 AMD 800GbE Ethernet NIC<ol><li>UALink protocol between the GPU and the NIC<br> GPU 与 NIC 之间的 UALink 协议</li><li>1.6Tbit&#x2F;s per GPU 每个 GPU 1.6Tbit&#x2F;s</li><li>Less aggressive scale out option.<br> 较不激进的扩展选项。</li></ol></li><li>Two custom PCIe 6.0 800GbE ethernet NICs per GPU<br> 每个 GPU 两个定制的 PCIe 6.0 800GbE 网卡<ol><li>PCIe 6.0 protocol between the GPU and the NIC<br> GPU 与网卡之间的 PCIe 6.0 协议</li><li>1.6Tbit&#x2F;s per GPU 每个 GPU 1.6Tbit&#x2F;s</li><li>Allow the customers to choose non-AMD NIC.<br> 允许客户选择非 AMD 网卡。</li></ol></li></ol><p>Customers that choose AMD’s NIC will have 1.5x more scale out bandwidth per GPU compared to a non-AMD NIC.<br>选择 AMD 网络接口卡的客户将比非 AMD 网络接口卡每个 GPU 拥有 1.5 倍的扩展带宽。</p><p>At the compute tray level, we believe that AMD’s solution will be competitive on paper to NVIDIA’s H2 2026 VR200 NVL144 solution. Next, let’s look at the scale-up network and architecture in more detail.<br>在计算托盘级别，我们相信 AMD 的解决方案在纸面上将与 NVIDIA 的 2026 年下半年 VR200 NVL144 解决方案具有竞争力。接下来，让我们更详细地看看扩展网络和架构。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-48.png?fit=708,1024&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>We believe AMD’s MI450X IFoE rack will have switch trays between the compute trays using the ORV3 rack, much like Nvidia’s Oberon platform. There will be 8 compute trays at the top of each rack, 9 switch trays in the middle, and 8 compute trays at the bottom of each rack. The key difference between the AMD IFoE64 rack and the Nvidia GB200 NVL72 is that there are 10 compute trays at the top of Nvidia’s rack while AMD’s will only have 8 compute trays above the switch trays. This brings the total scale up world size for AMD’s MI450x IFoE to 64 GPUs, which is 8 less than Nvidia’s. <a href="https://semianalysis.com/2024/08/04/nvidias-blackwell-reworked-shipment/">We have explained why Nvidia chose to go with 72 scale up world size instead of 64 before.</a> To recap, 64 is the most efficient scale-up domain size to run AI workloads on but having a set of 8 extra GPUs in the scale up domain allows a degree of flexibility should compute nodes fail while carrying out workloads. The scale up size of the MI450X is not yet finalized, but we still believe customers prefer a 72-GPU scale up domain size.<br>我们相信 AMD 的 MI450X IFoE 机架将在计算托盘之间使用 ORV3 机架的交换机托盘，类似于 Nvidia 的 Oberon 平台。每个机架顶部将有 8 个计算托盘，中间有 9 个交换机托盘，底部有 8 个计算托盘。AMD IFoE64 机架与 Nvidia GB200 NVL72 之间的主要区别在于，Nvidia 机架顶部有 10 个计算托盘，而 AMD 的机架在交换机托盘上方只有 8 个计算托盘。这使得 AMD 的 MI450X IFoE 的总规模提升世界大小为 64 个 GPU，比 Nvidia 少 8 个。我们之前解释过 Nvidia 为何选择 72 的规模提升世界大小而不是 64。回顾一下，64 是运行 AI 工作负载的最有效的规模提升域大小，但在规模提升域中拥有 8 个额外的 GPU 可以在计算节点在执行工作负载时出现故障时提供一定的灵活性。MI450X 的规模提升大小尚未最终确定，但我们仍然相信客户更喜欢 72-GPU 的规模提升域大小。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-49-2.png?fit=1024,591&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>The IFoE scale up domain will use an all to all topology. Each switch tray will contain two 51.2T switches, meaning that the aggregate scale up bandwidth of MI450 IFoE64 will be 921.6 Tbit&#x2F;s of uni-directional bandwidth. For scale-up connectivity, each GPU will utilize 72 lanes of 200G ethernet connecting 18 switches. This works out to 4 lanes of 200G ethernet or 800Gbit&#x2F;s of uni-directional (uni-di) bandwidth between each GPU and each Switch ASIC. There will be a total of 9,216 DAC cables in the copper backplane, more than Nvidia’s 5,184 DAC cable in the GB200 copper backplane.<br>IFoE 扩展域将使用全对全拓扑。每个交换机托盘将包含两个 51.2T 交换机，这意味着 MI450 IFoE64 的总扩展带宽将为 921.6 Tbit&#x2F;s 的单向带宽。对于扩展连接，每个 GPU 将利用 72 条 200G 以太网通道连接 18 个交换机。这相当于每个 GPU 与每个交换机 ASIC 之间有 4 条 200G 以太网通道或 800Gbit&#x2F;s 的单向（uni-di）带宽。铜背板中将有总共 9,216 条 DAC 电缆，超过 Nvidia 在 GB200 铜背板中的 5,184 条 DAC 电缆。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-50.png?fit=1024,652&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>Interestingly, we believe that AMD will also launch a more aggressive rack scale architecture of MI450X IFoE128, which basically combines two MI450X IFoE64 racks into one scale up domain. MI450X IFoE128 will have the same 1.8TByte&#x2F;s scaleup bandwidth per GPU as MI450X IFoE64. While Nvidia has taken a similar approach in joining two physical racks into one scale up domain in its NVL36x2 concept, AMD’s concept utilizes a different cable solution. Nvidia’s attempt of NVL36x2 was short lived and never saw the light of day due to issues with the ACC cables that connect the two NVL36 racks via the front of the rack using the NVSwitch trays. Instead of using ACCs, we believe that AMD will attempt to connect the two racks with DAC cables via a massive copper backplane that spans across two racks.<br>有趣的是，我们相信 AMD 还将推出更具攻击性的 MI450X IFoE128 机架规模架构，它基本上将两个 MI450X IFoE64 机架合并为一个扩展域。MI450X IFoE128 将与 MI450X IFoE64 具有相同的每个 GPU 1.8TByte&#x2F;s 扩展带宽。虽然 Nvidia 在其 NVL36x2 概念中采取了类似的方法，将两个物理机架合并为一个扩展域，但 AMD 的概念采用了不同的电缆解决方案。Nvidia 的 NVL36x2 尝试短暂存在，因 ACC 电缆连接两个 NVL36 机架的问题而未能问世，这些电缆通过机架前部使用 NVSwitch 托盘连接。我们相信，AMD 将尝试通过跨越两个机架的大型铜背板，使用 DAC 电缆连接这两个机架，而不是使用 ACCs。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-51.png?fit=1024,923&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>We believe that the DAC backplane approach of AMD provides two advantages over the ACC approach that Nvidia tried with.<br>我们相信，AMD 的 DAC 背板方案相较于 Nvidia 尝试的 ACC 方案提供了两个优势。  </p><p>First, with Nvidia’s approach, each cross-rack link between GPUs requires two hops. AMD’s DAC backplane solves this problem as all the hops are still one hops.<br>首先，使用 Nvidia 的方案，每个 GPU 之间的跨机架链接需要两次跳转。AMD 的 DAC 背板解决了这个问题，因为所有的跳转仍然是一次跳转。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-52.png?fit=880,1024&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>Second, as the interconnect does not go through the front of the rack, the switch ASICs can be place closer to the backplane connector reducing the distance between each link.<br>其次，由于互连不经过机架的前面，交换机 ASIC 可以更靠近背板连接器，从而减少每个链路之间的距离。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-54-scaled.png?fit=1024,487&ssl=1"></p><p>Source: SemiAnalysis Estimates 来源：SemiAnalysis 估计</p><p>With benefits comes challenges. In order to double the scale up domain from 64 to 128 while maintain 1.8TByte&#x2F;s per GPU, the uni-di scale up bandwidth between each GPU and each Switch will be halved to be 2 lanes of 212G ethernet SerDes or 400Gbit&#x2F;s. Cable management would present a significant challenge, as there would even more cables, a total of 18,432 DAC cables with half the cable from each rack going across to the other rack. The longer distance of the cross-rack cables also poses a significant challenge for execution as it becomes harder to maintain signal integrity over longer distances on DAC cables.<br>随着好处而来的还有挑战。为了将 64 的扩展域规模翻倍到 128，同时保持每个 GPU 1.8TByte&#x2F;s 的速度，单向扩展带宽在每个 GPU 和每个交换机之间将减半，变为 2 条 212G 以太网 SerDes 或 400Gbit&#x2F;s。电缆管理将面临重大挑战，因为将会有更多的电缆，总共有 18,432 条 DAC 电缆，每个机架的一半电缆跨越到另一个机架。跨机架电缆的更长距离也对执行构成了重大挑战，因为在 DAC 电缆上保持信号完整性在更长距离上变得更加困难。</p><p>Overall, we believe that AMD will make significant progress with rack scale solution post acquisition of ZT System to remain competitive on the hardware level. With the Kyber architecture of Nvidia scheduled to be launched in 2H27, this presents an opportunity for AMD to catch up with Nvidia on rack scale solution in 2H26 against 2H26 Rubin Oberon form factor.<br>总体而言，我们相信 AMD 在收购 ZT 系统后将在机架规模解决方案上取得重大进展，以保持在硬件层面的竞争力。随着 Nvidia 的 Kyber 架构计划在 2027 年下半年推出，这为 AMD 在 2026 年下半年与 Nvidia 在机架规模解决方案上迎头赶上提供了机会，针对的是 2026 年下半年 Rubin Oberon 形态因素。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-56-scaled.png?fit=1024,424&ssl=1"></p><p>Source: SemiAnalysis Estimates, Nvidia 来源：SemiAnalysis 估计，Nvidia</p><p>We believe the MI450X IFoE64 architecture will be competitive with 2H 2026’s Rubin Vera NVL144 as they have the same scale up bandwidth per GPU @1.8TByte&#x2F;s and competitive FLOPs. There is a chance that AMD even one up Nvidia with the MI450X IFoE128 solution which almost doubles the scale up domain of Rubin Vera NVL144.<br>我们相信 MI450X IFoE64 架构将在 2026 年下半年与 Rubin Vera NVL144 竞争，因为它们每个 GPU 的扩展带宽相同，均为 1.8TByte&#x2F;s，并且 FLOPs 具有竞争力。AMD 甚至有可能通过 MI450X IFoE128 解决方案超越 Nvidia，该解决方案几乎将 Rubin Vera NVL144 的扩展域翻倍。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-57-scaled.png?fit=1024,346&ssl=1"></p><p>Source: SemiAnalysis Estimates, Nvidia 来源：SemiAnalysis 估计，Nvidia</p><p>They have a 256 GPU DAC backplane approach in design too.<br>他们在设计中也采用了 256 GPU DAC 背板方案。</p><h2 id="MI300X-MI325X-MI355X-Pricing-TCOMI300X、MI325X、MI355X-定价与总拥有成本"><a href="#MI300X-MI325X-MI355X-Pricing-TCOMI300X、MI325X、MI355X-定价与总拥有成本" class="headerlink" title="MI300X, MI325X, MI355X Pricing &amp; TCOMI300X、MI325X、MI355X 定价与总拥有成本"></a>MI300X, MI325X, MI355X Pricing &amp; TCOMI300X、MI325X、MI355X 定价与总拥有成本</h2><p>As discussed earlier, AMD’s launch timing means that its current generation products have been facing off against Nvidia’s next generation products. We have already observed price cuts in the MI300X into late 2024 as we have gotten closer to the first shipment dates for Blackwell systems. In 2025 thus far, the MI325X has been competing with Nvidia’s Blackwell for customer orders. With AMD’s GPUs seeing less than ideal traction, we see a round of price cuts coming to the MI325X and MI355X in the second half of the year just to stay competitive with the B200 1000W and B300A NVL16. And this is even before we consider how AMD’s current products stack against the GB200 NVL72 and its intimidating inference performance that is enhanced even further with the release of Dynamo.<br>正如之前讨论的，AMD 的发布时机意味着其当前一代产品正在与 Nvidia 的下一代产品竞争。随着我们接近 Blackwell 系统的首次发货日期，我们已经观察到 MI300X 在 2024 年末的降价情况。因此，到目前为止，MI325X 在 2025 年与 Nvidia 的 Blackwell 争夺客户订单。由于 AMD 的 GPU 表现不尽如人意，我们预计在下半年 MI325X 和 MI355X 将进行一轮降价，以便与 B200 1000W 和 B300A NVL16 保持竞争力。这甚至是在我们考虑 AMD 当前产品与 GB200 NVL72 及其令人畏惧的推理性能（在 Dynamo 发布后进一步增强）相比之前的情况。</p><p>AMD’s MI300X and MI325X server level cost and total cluster upfront costs are 20-30% lower than Nvidia’s H100 due to lower chip pricing as well as the availability of less expensive Ethernet networking switches and the use of transceivers from vendors other than Nvidia.<br>由于芯片价格较低以及更便宜的以太网网络交换机的可用性，以及使用来自非 Nvidia 供应商的收发器，AMD 的 MI300X 和 MI325X 服务器级成本和整个集群的前期总成本比 Nvidia 的 H100 低 20-30%。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-58.png?fit=1024,580&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>Operating costs for AMD’s servers are similar to Nvidia’s servers as they generally have comparable TDPs and most operating costs scale with respect to IT power requirements.<br>AMD 服务器的运营成本与 Nvidia 服务器相似，因为它们通常具有可比的 TDP，并且大多数运营成本与 IT 电力需求成比例。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-59.png?fit=1024,556&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>However, with leading edge GPUs, the Capital Cost generally dominates and so AMD’s total cost of ownership is noticeably below that of Nvidia, with the MI300X coming in at a TCO of $1.37 per hour per GPU vs Nvidia’s H100 at $1.62 per hour per GPU. Note that the below graph is based on theoretical 8 bit FLOP&#x2F;s and not realistic FLOP&#x2F;s. <a href="https://rocm.blogs.amd.com/software-tools-optimization/Understanding_Peak_and_Max-Achievable_FLOPS/README.html">As discussed earlier, AMD as owned to overstate their theoretical FLOP&#x2F;s</a><br>然而，对于领先的 GPU，资本成本通常占主导地位，因此 AMD 的总拥有成本明显低于 Nvidia，MI300X 的 TCO 为每个 GPU 每小时 1.37 美元，而 Nvidia 的 H100 为每个 GPU 每小时 1.62 美元。请注意，下面的图表基于理论的 8 位 FLOP&#x2F;s，而不是现实的 FLOP&#x2F;s。如前所述，AMD 曾被指控夸大其理论 FLOP&#x2F;s。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig.-60.png?fit=1024,375&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p><img src="https://pixel.wp.com/g.gif?v=ext&blog=233841863&post=150426353&tz=0&srv=semianalysis.com&hp=atomic&ac=3&amp=0&j=1:14.6-a.9&host=semianalysis.com&ref=https://t.co/&rand=0.12718469237316932"></p>]]></content>
    
    
    <summary type="html">SemiAnalysis is expanding the AI engineering team! If you have an experience in PyTorch, training, inferencing, system modelling, SLURM/Kubernetes, send us your resume and 5 bullet points demonstrating your engineering excellence to letsgo@semianalysis.com. Ever since SemiAnalysis published an article in December 2024 detailing mediocre AMD software and the lack of usability, AMD has kicked into…</summary>
    
    
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="clippings" scheme="http://example.com/tags/clippings/"/>
    
  </entry>
  
  <entry>
    <title>【译】GPU 云集群 评级系统 ｜ AI新云经济学</title>
    <link href="http://example.com/2025/04/03/%E3%80%90%E8%AF%91%E3%80%91The%20GPU%20Cloud%20ClusterMAX%E2%84%A2%20Rating%20System%20-%20AI%20Neocloud%20Economics/"/>
    <id>http://example.com/2025/04/03/%E3%80%90%E8%AF%91%E3%80%91The%20GPU%20Cloud%20ClusterMAX%E2%84%A2%20Rating%20System%20-%20AI%20Neocloud%20Economics/</id>
    <published>2025-04-03T08:36:39.059Z</published>
    <updated>2025-04-03T08:36:39.060Z</updated>
    
    <content type="html"><![CDATA[<h2 id="GPU-云集群-MAX™评级系统-如何租用-GPU-90-覆盖率通过租用-GPU-价值、GPU-泡沫破裂、CoreWeave-IPO、AI-新云经济学"><a href="#GPU-云集群-MAX™评级系统-如何租用-GPU-90-覆盖率通过租用-GPU-价值、GPU-泡沫破裂、CoreWeave-IPO、AI-新云经济学" class="headerlink" title="GPU 云集群 MAX™评级系统 | 如何租用 GPU 90%+覆盖率通过租用 GPU 价值、GPU 泡沫破裂、CoreWeave IPO、AI 新云经济学"></a>GPU 云集群 MAX™评级系统 | 如何租用 GPU 90%+覆盖率通过租用 GPU 价值、GPU 泡沫破裂、CoreWeave IPO、AI 新云经济学</h2><p>由迪伦·帕特尔、邓洪·陈、丹尼尔·尼什巴尔、伊万·恰姆和雷克·克努赫特森撰写</p><p><img src="https://semianalysis.com/wp-content/uploads/2025/03/file_000000001dcc51f6900bfe241a0a01ea_conversation_id67e3bba7-e4a8-800d-a1c6-8b68859eb1d7message_id70400c72-4c38-472b-8262-06a8cef06f44.png"></p><p><em>The ClusterMAX™ Rating System and content within this article were prepared independently by SemiAnalysis. No part of SemiAnalysis’s compensation by our clients was, is, or will be directly or indirectly related to the specific tiering, ratings or comments expressed.<br>ClusterMAX™ 评级系统及本文内容由 SemiAnalysis 独立准备。SemiAnalysis 从客户处获得的任何补偿与具体的分级、评级或表达的评论无直接或间接关系。</em></p><h2 id="Introduction-介绍"><a href="#Introduction-介绍" class="headerlink" title="Introduction 介绍"></a>Introduction 介绍</h2><p>The exuberance in the GPU rental market has cooled off. We predicted this in our <a href="https://semianalysis.com/2023/12/04/gpu-cloud-economics-explained-the/">December 2023 GPU Cloud Economics Report</a> and re-iterated this view in our <a href="https://semianalysis.com/2024/10/03/ai-neocloud-playbook-and-anatomy/#part-2-the-ai-neocloud-economy">AI Neocloud Anatomy and Playbook Report released in October 2024</a>. Technological improvements mean the cost of computing goes down over time, and we now believe it’s a buyers’ market for GPU rentals, especially for the Hopper class and MI300 class GPU. There is widespread availability from over 100+ AI Neoclouds and Hyperscalers.<br>GPU 租赁市场的热情已经降温。我们在 2023 年 12 月的 GPU 云经济报告中预测了这一点，并在 2024 年 10 月发布的 AI Neocloud 解剖与行动计划报告中重申了这一观点。技术进步意味着计算成本随着时间的推移而降低，我们现在认为 GPU 租赁市场是买方市场，特别是对于 Hopper 系列和 MI300 系列 GPU。来自 100 多个 AI Neocloud 和超大规模云服务商的广泛可用性。</p><p>Part of this is due to new entrants, and with more options to rent. Currently, there is no “how-to guide” to rent a GPU or any independent evaluation of GPU clouds until today.<br>这部分是由于新进入者的影响，以及租赁选项的增加。目前，直到今天还没有关于租用 GPU 的“操作指南”或任何独立的 GPU 云评估。</p><p>For the past 12 months we have spent time creating the GPU Cloud ClusterMAX™ Rating System, or ClusterMAX ™ for short. We have independently tested and&#x2F;or collected customer feedback from as many GPU clouds as possible. We believe that with this first GPU Cloud Rating, we will cover <strong>90% of the GPU rental market by GPU volume</strong>. We hope to include more providers in our next rating exercise so that we can evaluate their quality.<br>在过去的 12 个月里，我们花时间创建了 GPU 云 ClusterMAX™评级系统，简称 ClusterMAX™。我们独立测试和&#x2F;或收集了尽可能多的 GPU 云的客户反馈。我们相信，通过这个首个 GPU 云评级，我们将覆盖 90%的 GPU 租赁市场按 GPU 数量计算。我们希望在下一个评级活动中包括更多的提供商，以便我们能够评估他们的质量。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/Fig-1.-Neocloud-Ranking-GIMP.png?resize=778,868&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>This isn’t an exhaustive list of GPU providers. We have a much more extensive list of players that we are aware of, and the entire market map is shown in the following image. This list appears to be expanding daily, but many of the neoclouds are not yet ready for customers. This is the point of ClusterMAX™, as it’s a simple tool to help you navigate complexity. It is probably worth spending your dollar on a ClusterMAX™ rated provider.<br>这并不是一个详尽的 GPU 供应商列表。我们知道的参与者有一个更广泛的列表，整个市场地图显示在下面的图像中。这个列表似乎每天都在扩展，但许多新云尚未准备好为客户服务。这就是 ClusterMAX™的意义所在，因为它是一个简单的工具，可以帮助您应对复杂性。花钱选择一个 ClusterMAX™评级的供应商可能是值得的。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-2.-Neocloud-providers-by-type-GIMP.png?resize=1024,562&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>Our rating classifications are Platinum, Gold, Silver, Bronze, and UnderPerform. We will explain each rating in more detail later in this report.<br>我们的评级分类为铂金、黄金、白银、铜牌和表现不佳。我们将在本报告的后面详细解释每个评级。</p><p>In addition we will also be discussing the market for H100 rentals, where it’s headed from here, Hyperscaler vs Neocloud pricing, cluster level TCO, cluster returns and scenario analysis, various debates around demand, and applying this framework&#x2F;analysis to Coreweave and their IPO.<br>此外，我们还将讨论 H100 租赁市场的现状及未来发展，Hyperscaler 与 Neocloud 的定价，集群级总拥有成本（TCO），集群回报和情景分析，关于需求的各种辩论，以及将这一框架&#x2F;分析应用于 Coreweave 及其 IPO。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-3.-Trophies-GIMP.png?resize=1024,211&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><h2 id="Executive-Summary-执行摘要"><a href="#Executive-Summary-执行摘要" class="headerlink" title="Executive Summary 执行摘要"></a>Executive Summary 执行摘要</h2><ol><li>SemiAnalysis has developed the world’s first GPU Cloud Rating System – we have named this system ClusterMAX™. We look into rating GPUs from the perspective of an average reasonable customer.<br> SemiAnalysis 开发了全球首个 GPU 云评级系统——我们将该系统命名为 ClusterMAX™。我们从普通合理客户的角度来评估 GPU。</li><li>SemiAnalysis has independently tested dozens of GPUs and ClusterMAX™ currently has approximately 90% coverage of the entire GPU market by GPU volume.<br> SemiAnalysis 独立测试了数十款 GPU，目前 ClusterMAX™ 在 GPU 体量方面覆盖了整个 GPU 市场的约 90%。</li><li>The bar across the GPU cloud industry is currently very low. ClusterMAX™ aims to provide a set of guidelines to help raise the bar across the whole GPU cloud industry. ClusterMAX™ guidelines evaluate features that most GPU renters care about.<br> 目前，GPU 云行业的标准非常低。ClusterMAX™旨在提供一套指导方针，以帮助提高整个 GPU 云行业的标准。ClusterMAX™指导方针评估大多数 GPU 租赁者关心的特性。</li><li>ClusterMAX™ has five different tiers: Platinum, Gold, Sliver, Bronze and UnderPerform.<br> ClusterMAX™有五个不同的等级：铂金、黄金、白银、铜和表现不佳。</li><li>We will be conducting regular ClusterMAX™ rating and evaluation exercises every 3-6 months so that various GPUs can have their improvements reflected, and customer can have the latest information on GPUs.<br> 我们将每 3-6 个月进行定期的 ClusterMAX™评级和评估，以便各种 GPU 的改进能够得到反映，客户可以获得最新的 GPU 信息。</li><li>ClusterMAX™ Platinum represents GPU clouds that are raising the industry bar and there is only one GPU cloud, CoreWeave, that provides services at this tier.<br> ClusterMAX™ Platinum 代表着提升行业标准的 GPU 云，只有一个 GPU 云，CoreWeave，提供此级别的服务。</li><li>CoreWeave is the only non-hyperscaler currently that is experienced at operating large-scale 10k+ H100 clusters reliably.<br> CoreWeave 是目前唯一一家在可靠地运营大规模 10k+ H100 集群方面具有经验的非超大规模云服务商。</li><li>Some of these providers in ClusterMAX™ Bronze category are already making a considerable effort to catch up such as Google Cloud. We believe Google Cloud is on a Rocketship path toward ClusterMAX™ Gold or ClusterMAX™ Platinum by the next time we re-evaluate them.<br> ClusterMAX™ 铜级别中的一些提供商，例如 Google Cloud，已经在努力追赶。我们相信 Google Cloud 正在朝着 ClusterMAX™ 金级或 ClusterMAX™ 白金级的方向快速发展，直到我们下次重新评估他们。</li><li>Enterprises mainly rent GPUs from Hyperscalers + CoreWeave. Enterprises rarely rent from Emerging Neoclouds.<br> 企业主要从超大规模云服务商和 CoreWeave 租用 GPU。企业很少从新兴的 Neoclouds 租用。</li><li>Hyperscalers’ GPU rental prices are higher than that of Neocloud Giants and Emerging Neoclouds as Hyperscalers mainly serve the enterprise market.<br>超大规模云服务商的 GPU 租赁价格高于 Neocloud 巨头和新兴 Neoclouds，因为超大规模云服务商主要服务于企业市场。</li><li>Oracle comes in at one of the lowest GPU Rental price points amongst the Hyperscalers.<br>在超大规模云服务商中，Oracle 的 GPU 租赁价格处于最低水平之一。</li><li>Amongst GPU clouds that are highly competent on the technical front, Nebius offers the lowest absolute price and the best terms for short to medium-term rents. Crusoe also offers reasonable pricing and contract terms in additional to strong technical competency.<br>在技术能力非常强的 GPU 云服务中，Nebius 提供最低的绝对价格和最佳的短期到中期租赁条款。Crusoe 也提供合理的定价和合同条款，并且在技术能力上也很强。</li><li>As we first discussed in our article on <a href="https://semianalysis.com/2023/12/04/gpu-cloud-economics-explained-the/">GPU Cloud Economics published back in December 2023</a>, technological improvements mean the cost of compute goes down over time, and we now believe it’s a buyers’ market for GPU rentals. There are 100 of GPU clouds all competing for mostly the same customers.<br>正如我们在 2023 年 12 月发布的关于 GPU 云经济的文章中首次讨论的那样，技术进步意味着计算成本随着时间的推移而下降，我们现在认为 GPU 租赁市场是买方市场。目前有 100 个 GPU 云服务商都在争夺大致相同的客户。</li><li>The launch of DeepSeek provided a momentary short-term stabilization and even increase in H200 rental prices but in the medium to long term, prices are still declining.<br>DeepSeek 的推出在短期内提供了暂时的稳定，甚至使 H200 租赁价格有所上涨，但从中长期来看，价格仍在下降。</li><li>Jensen Huang, Chief Revenue Destroyer, said last week, “ <em>When Blackwells start shipping in volume, you couldn’t even give Hoppers away”.</em> From perspective of the GPU operator,this should be a call for GPU Rental providers to ensure that they engage in contracts that protect them from rapid compute price declines – i.e. sign long term contracts where possible. From the customer’s perspective, they may prefer flexibility in their commitments and opt for shorter-term contracts.<br>詹森·黄，首席收入破坏者，上周表示：“当黑威尔开始大规模发货时，你甚至无法赠送霍普斯。” 从 GPU 运营商的角度来看，这应该是对 GPU 租赁提供商的呼吁，确保他们签订保护他们免受快速计算价格下跌的合同——即尽可能签订长期合同。从客户的角度来看，他们可能更喜欢在承诺上的灵活性，选择短期合同。</li><li>We will talk more about GPU Rental pricing and the IRR of GPUs as well as recent GPU Rental Market Rates for different contract lengths at the end of the article. Scroll down to the end if you are the reader who cares mainly about the finance side of GPUs or how to think about unit economics for the GPU rental business.<br>我们将在文章末尾讨论更多关于 GPU 租赁定价和 GPU 的内部收益率，以及不同合同期限的最新 GPU 租赁市场利率。如果你是主要关心 GPU 财务方面或如何考虑 GPU 租赁业务的单位经济的读者，请向下滚动到文章末尾。</li></ol><h2 id="The-GPU-Cloud-ClusterMAX™-Rating-SystemGPU-云集群-MAX™评级系统"><a href="#The-GPU-Cloud-ClusterMAX™-Rating-SystemGPU-云集群-MAX™评级系统" class="headerlink" title="The GPU Cloud ClusterMAX™ Rating SystemGPU 云集群 MAX™评级系统"></a>The GPU Cloud ClusterMAX™ Rating SystemGPU 云集群 MAX™评级系统</h2><p>The goal of the ClusterMAX™ rating is to evaluate and benchmark the more than 100 GPU providers. This provides the broader ML community with an understanding of the capabilities, features, advantages, and disadvantages of each GPU provider. This better informs a provider which GPU cloud(s) can best meet their needs. Our second objective is to provide a set of guidelines to help raise the bar across the whole GPU cloud industry. Currently, the bar is lower than you can imagine.<br>ClusterMAX™评级的目标是评估和基准测试超过 100 个 GPU 提供商。这为更广泛的机器学习社区提供了对每个 GPU 提供商的能力、特性、优点和缺点的理解。这更好地告知提供商哪些 GPU 云可以最好地满足他们的需求。我们的第二个目标是提供一套指导方针，以帮助提升整个 GPU 云行业的标准。目前，标准低于你想象的水平。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-4.-Southpark-GIMP.png?resize=1024,560&ssl=1"></p><p>Source: <a href="https://www.youtube.com/watch?v=jUsf_BXUbKY">South Park</a> 来源：南方公园</p><p>ClusterMAX ™ hopefully will raise the bar, and we’ve stepped into the deep end and done extensive benchmarking and profiling over the last year to see the best and worst practices at GPU providers. We have scaled from single nodes to 1024 GPU clusters, and today we share our findings with the broader community.<br>ClusterMAX ™ 希望能够提升标准，我们已经深入研究并进行了广泛的基准测试和性能分析，以了解 GPU 供应商的最佳和最差实践。我们已经从单节点扩展到 1024 GPU 集群，今天我们与更广泛的社区分享我们的发现。</p><p>We evaluate features that GPU renters care about, such as:<br>我们评估 GPU 租赁者关心的特性，例如：</p><ul><li>Security 安全</li><li>LifeCycle and Technical Expertise<br>  生命周期和技术专长</li><li>Slurm and Kubernetes Slurm 和 Kubernetes</li><li>Storage 存储</li><li>NCCL&#x2F;RCCL Networking Performance<br>  NCCL&#x2F;RCCL 网络性能</li><li>Reliability and Service Level Agreements (SLAs)<br>  可靠性和服务水平协议（SLA）</li><li>Automated Active and Passive Health Checks and Monitoring<br>  自动化主动和被动健康检查与监控</li><li>Consumption Models, Price Per Value, and Availability<br>  消费模型、价值价格和可用性</li><li>Technical Partnerships 技术合作伙伴关系</li></ul><p>In today’s report we will dive deeper into specifics on how we evaluate GPUs and our guidelines later in the article.<br>在今天的报告中，我们将深入探讨我们如何评估 GPU 的具体细节以及文章后面的指导方针。</p><p>We will re-evaluate and update our GPU Cloud ClusterMAX™ Tier list every 3-6 months to incorporate new information for the GPU rental market. We want to give GPU providers feedback and the opportunity to improve, and we are always open to and appreciate our dialogues with GPU providers.<br>我们将每 3-6 个月重新评估并更新我们的 GPU 云集群 MAX™等级列表，以纳入 GPU 租赁市场的新信息。我们希望为 GPU 提供商提供反馈和改进的机会，并始终欢迎并感激与 GPU 提供商的对话。</p><p>We break our ClusterMAX™ rating system into five tiers:<br>我们将 ClusterMAX™评级系统分为五个等级：</p><ul><li>ClusterMAX™ Platinum ClusterMAX™铂金</li><li>ClusterMAX™ Gold ClusterMAX™ 金</li><li>ClusterMAX™ Sliver ClusterMAX™ 银</li><li>ClusterMAX™ Bronze ClusterMAX™ 铜</li><li>UnderPerform 表现不佳</li></ul><p>Each tier has attributes that justify the rating it receives. Let’s discuss from best to worst.<br>每个级别都有其属性来证明其获得的评级。让我们从最好到最差进行讨论。</p><p>The <strong>ClusterMAX™ Platinum</strong> tier represents <em>the best</em> GPU cloud providers in the industry. Providers in this category consistently excel across evaluation criteria, including security, price for value, technical expertise, reliability backed by clearly defined SLAs, seamless managed Slurm&#x2F;Kubernetes offering, and best in class NCCL&#x2F;RCCL networking performance. Platinum-tier providers are proactive, innovative, and maintain an active feedback loop with the community to continually raise the bar. That’s why they are the best.<br>ClusterMAX™ 白金级别代表了行业中最优秀的 GPU 云服务提供商。该类别的提供商在评估标准上始终表现出色，包括安全性、性价比、技术专长、可靠性（有明确的服务水平协议支持）、无缝的管理 Slurm&#x2F;Kubernetes 提供以及一流的 NCCL&#x2F;RCCL 网络性能。白金级别的提供商积极主动、创新，并与社区保持活跃的反馈循环，以不断提高标准。这就是他们为何是最优秀的原因。</p><p><strong>ClusterMAX™ Gold</strong> tier providers deliver <em>strong</em> performance across most key evaluation categories, with some opportunities for improvement. They offer solid security, reliable infrastructure, and competitive pricing, and competent technical support. Although Gold-tier GPU clouds may have gaps or inconsistencies in features like active health checks, they generally demonstrate responsiveness to feedback and a commitment to improvement. They are positioned as great choices for GPU renters to maximize throughput.<br>ClusterMAX™ 金级提供商在大多数关键评估类别中表现出色，尽管仍有一些改进的机会。他们提供可靠的安全性、稳定的基础设施和具有竞争力的定价，以及称职的技术支持。尽管金级 GPU 云在主动健康检查等功能上可能存在差距或不一致，但它们通常对反馈表现出响应性，并致力于改进。它们被视为 GPU 租户最大化吞吐量的绝佳选择。</p><p>Providers rated at <strong>ClusterMAX™ Silver</strong> demonstrate <em>adequate</em> GPU cloud offerings with a satisfactory balance of performance, security, and value, but they typically have noticeable gaps compared to Gold or Platinum-tier services. These providers meet basic standards for reliability, security, support, and have adequate networking performance but lack advanced orchestration, or have confusing pricing structures. Silver-tier GPU clouds have room for improvement and typically benefit significantly from adopting industry best practices.<br>ClusterMAX™ 银级提供商展示了足够的 GPU 云服务，性能、安全性和价值之间有令人满意的平衡，但与金级或铂金级服务相比，通常存在明显的差距。这些提供商满足可靠性、安全性、支持的基本标准，并具有足够的网络性能，但缺乏高级编排，或定价结构令人困惑。银级 GPU 云有改进的空间，通常通过采用行业最佳实践而显著受益。</p><p>The <strong>ClusterMAX™ Bronze</strong> tier includes GPU cloud providers that fulfill the <em>minimum</em> criteria but consistently exhibit shortcomings in our evaluation areas. Common issues may include inconsistent support, subpar networking performance, unclear SLAs, limited integration with popular tools like Kubernetes or Slurm, or less competitive pricing. Providers in this category need considerable improvements to enhance reliability and customer experience. Some of these providers in this category are already making considerable effort to catch up – Google Cloud for example – and we are excited to see what their next ClusterMAX™ result will be in 3-6 months.<br>ClusterMAX™ 铜级包括满足最低标准但在我们的评估领域中持续表现不佳的 GPU 云服务提供商。常见问题可能包括支持不一致、网络性能不佳、服务水平协议不明确、与 Kubernetes 或 Slurm 等流行工具的集成有限，或定价竞争力不足。此类别的提供商需要进行大量改进，以增强可靠性和客户体验。该类别中的一些提供商已经在努力追赶，例如 Google Cloud，我们期待看到他们在 3-6 个月内的下一个 ClusterMAX™ 结果。</p><p>GPU providers placed in the <strong>UnderPerform</strong> category fail to meet basic industry and security requirements across multiple evaluation metrics. Providers in this tier generally exhibit substantial such as insecure security practices, poor reliability or uptime, unclear or misleading marketing, limited technical knowledge or customer support, and inadequate orchestration capabilities. Most commonly, providers in the Underperform tier do not have SOC2 compliance or have security risks exposing traffic between your workload and the internet which could be logged by networking equipment. The GPU providers that land themselves in the <strong>UnderPerform</strong> category are often the same companies that spam AI generated advertisements.<br>被归类为表现不佳的 GPU 供应商未能满足多个评估指标下的基本行业和安全要求。此层级的供应商通常表现出显著的问题，例如不安全的安全实践、可靠性或正常运行时间差、模糊或误导性的营销、有限的技术知识或客户支持，以及不足的编排能力。最常见的是，表现不佳层级的供应商没有 SOC2 合规性，或者存在安全风险，暴露了您的工作负载与互联网之间的流量，这可能被网络设备记录。落入表现不佳类别的 GPU 供应商通常是那些发送 AI 生成广告的公司。</p><h2 id="Jensen-Huang-Chief-Revenue-Destroyer詹森·黄，首席收入破坏者"><a href="#Jensen-Huang-Chief-Revenue-Destroyer詹森·黄，首席收入破坏者" class="headerlink" title="Jensen Huang, Chief Revenue Destroyer詹森·黄，首席收入破坏者"></a>Jensen Huang, Chief Revenue Destroyer詹森·黄，首席收入破坏者</h2><p>Jensen Huang, Chief Revenue Destroyer, said last week, “When Blackwells start shipping in volume, you couldn’t even give Hoppers away.”<br>詹森·黄，首席收入破坏者，上周表示：“当黑威尔开始大规模出货时，你甚至无法赠送霍普斯。”</p><p>Back in April 2024, our pricing model in the <a href="https://semianalysis.com/ai-cloud-tco-model/">AI Cloud Total Cost of Ownership Model</a> suggested such an outcome. GPU prices declined throughout 2024 due to a ramp up in H100 production, with this decline continuing into late 2024 as buyers pivoted to focus on their Blackwell strategy. One year later, our forecasts have been dead on, with a 2-3% margin of error for H100 SXM.<br>回到 2024 年 4 月，我们在 AI 云总拥有成本模型中的定价模型建议了这样的结果。由于 H100 生产的增加，2024 年 GPU 价格持续下降，这一下降趋势一直持续到 2024 年末，因为买家开始转向关注他们的黑威尔战略。一年后，我们的预测非常准确，H100 SXM 的误差范围为 2-3%。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-5.-H100-Pricing-GIMP.png?resize=1024,670&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>The forecast model operates with three main inputs:<br>预测模型主要基于三个输入：  </p><p>1. <strong>Install base of AI Accelerators globally</strong>: We utilize our <a href="https://semianalysis.com/accelerator-industry-model/">Accelerator Industry Model</a> to determine the installed base of every GPU SKU shipped thus far and our estimates based on supply chain analysis for future GPU shipments.<br>1. 全球 AI 加速器的安装基础：我们利用加速器行业模型来确定迄今为止每个 GPU SKU 的安装基础，以及基于供应链分析对未来 GPU 出货量的估计。  </p><p>2. <strong>Total Cost of Ownership for AI Clusters:</strong> We calculate the total cost of ownership for an AI Cluster, including capital costs such as the AI server, networking, storage, installation, and service, as well as operating costs such as colocation rental, power costs, remote hands and support engineers and internet connectivity.<br>2. 人工智能集群的总拥有成本：我们计算人工智能集群的总拥有成本，包括资本成本，如人工智能服务器、网络、存储、安装和服务，以及运营成本，如共置租金、电力成本、远程支持和支持工程师以及互联网连接。  </p><p>3. <strong>Compute Throughput for AI Accelerators</strong>: Estimated and measured effective training FLOPS and inference throughput (in tokens&#x2F;second&#x2F;GPU). For some systems, our AI Engineering team has run training and inference profiling and benchmarks, while for others, we have estimated output based on the chip specifications and architecture.<br>3. 人工智能加速器的计算吞吐量：估计和测量的有效训练 FLOPS 和推理吞吐量（以令牌&#x2F;秒&#x2F;GPU 为单位）。对于某些系统，我们的人工智能工程团队进行了训练和推理的性能分析和基准测试，而对于其他系统，我们则根据芯片规格和架构进行了输出估算。</p><p>We use the total cost of compute together with the compute throughput to calculate the cost of compute in terms of $&#x2F;hr per effective PFLOP for training and $&#x2F;M tokens for inference.<br>我们使用计算的总成本以及计算吞吐量来计算训练时的每有效 PFLOP 的$&#x2F;小时计算成本和推理时的$&#x2F;百万个令牌成本。</p><p>The market cost of compute is then determined by a weighted average of the cost of compute for each accelerator, based on its install base. With this market cost of compute, we can then multiply this by the compute capabilities of a given accelerator to calculate the “mark to market” rental cost for that accelerator.<br>然后，计算的市场成本是通过对每个加速器的计算成本进行加权平均来确定的，基于其安装基础。通过这个市场计算成本，我们可以将其乘以给定加速器的计算能力，以计算该加速器的“市场租赁”成本。</p><p>The below table provides a simple example of the workings behind this forecast. Here, we see that the GB200 NVL72 offers a 75% lower inference unit cost in terms of $&#x2F;M tokens vs an H100 and a 56% lower training cost in terms of $&#x2F;hr per effective PFLOP. This means that if the GB200 NVL72 sets the market cost of compute, then the H100 would have to be priced 65% lower per hour than the GB200 NVL72 in order for buyers to be indifferent between renting the two. The H100 would have to be set at a rental of $0.98 per hour per GPU to compete with a GB200 NVL72 priced at $2.20 per hour per GPU.<br>下表提供了该预测背后工作原理的简单示例。在这里，我们看到 GB200 NVL72 在每百万个令牌的推理单元成本方面比 H100 低 75%，在每有效 PFLOP 每小时的训练成本方面低 56%。这意味着如果 GB200 NVL72 设定了计算市场成本，那么 H100 的定价必须比 GB200 NVL72 每小时低 65%，以便买家在租用这两者之间无差异。H100 的租金必须设定为每小时每个 GPU 0.98 美元，以便与每小时每个 GPU 2.20 美元的 GB200 NVL72 竞争。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-5.1-Figures-of-Merit.png?resize=1734,1204&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>The dynamic of increasing availability of systems with far lower cost of compute that drives the overall cost of compute in terms of $&#x2F;M tokens and $ per effective PFLOP lower, in turn dragging rental prices for older cards down as well.<br>系统可用性不断增加的动态，导致计算成本（以每百万个令牌和每有效 PFLOP 的美元计算）降低，从而也拖累了旧卡的租金价格。</p><p>Behind the paywall (at the end) we have a deep dive into future pricing of GB200, the IRR estimates of H100 and GB200 Neoclouds, and the market prices for different contract lengths. We will also discuss how we can apply the above framework towards analyzing CoreWeave’s unit economics and potential return on investment. This was pretty much midroll advertisement for the AI TCO. Now, back to the ClusterMAX™ rating.<br>在付费墙后（最后），我们深入探讨了 GB200 的未来定价、H100 和 GB200 Neoclouds 的内部收益率估算，以及不同合同期限的市场价格。我们还将讨论如何将上述框架应用于分析 CoreWeave 的单位经济学和潜在投资回报。这几乎是 AI TCO 的中插广告。现在，回到 ClusterMAX™评级。</p><h2 id="GPU-Cloud-ClusterMAX™-Rating-System-GuidelinesGPU-云-ClusterMAX™评级系统指南"><a href="#GPU-Cloud-ClusterMAX™-Rating-System-GuidelinesGPU-云-ClusterMAX™评级系统指南" class="headerlink" title="GPU Cloud ClusterMAX™ Rating System GuidelinesGPU 云 ClusterMAX™评级系统指南"></a>GPU Cloud ClusterMAX™ Rating System GuidelinesGPU 云 ClusterMAX™评级系统指南</h2><p>We have designed our GPU Cloud ClusterMAX™ Rating based on what most GPU Renters want from their GPU. Let’s discuss the needs of GPU users before we jump into our guidelines. Our guidelines prioritize the needs of GPU users and their preferred experience.<br>我们设计的 GPU 云集群 MAX™ 评级是基于大多数 GPU 租用者对其 GPU 的需求。让我们在进入我们的指南之前讨论一下 GPU 用户的需求。我们的指南优先考虑 GPU 用户的需求和他们的首选体验。</p><p>Most GPU Renters want GPU nodes with out-of-the-box-managed Slurm&#x2F;Kubernetes, allowing them to focus on coding, training, and deploying their PyTorch&#x2F;JAX models. GPU renters wish to focus on small to large-scale experiments without much consideration of managing and maintaining the infrastructure underneath. Most GPU renters recognize that GPUs can sometimes fail, and when this happens, they want full visibility into the root cause and how the provider is addressing the issue.<br>大多数 GPU 租用者希望 GPU 节点具备开箱即用的管理 Slurm&#x2F;Kubernetes，这样他们可以专注于编码、训练和部署他们的 PyTorch&#x2F;JAX 模型。GPU 租用者希望专注于小规模到大规模的实验，而不必过多考虑管理和维护底层基础设施。大多数 GPU 租用者意识到 GPU 有时会出现故障，当这种情况发生时，他们希望能够全面了解根本原因以及提供商如何解决该问题。</p><p>To develop our ClusterMAX™ Rating, we have talked to many GPU renters to consider their needs and wants from their GPU. Based on our discussions, we have considered the following attributes during our evaluation:<br>为了开发我们的 ClusterMAX™评级，我们与许多 GPU 租赁者进行了交谈，以考虑他们对 GPU 的需求和期望。根据我们的讨论，我们在评估过程中考虑了以下属性：</p><ul><li>Security 安全</li><li>Lifecycle and Technical Expertise<br>  生命周期和技术专长</li><li>Reliability&#x2F;SLA 可靠性&#x2F;SLA</li><li>Slurm and Kubernetes offerings<br>  Slurm 和 Kubernetes 解决方案</li><li>NCCL&#x2F;RCCL Networking Performance<br>  NCCL&#x2F;RCCL 网络性能</li><li>Storage 存储</li><li>Active&#x2F;Passive Health Checks and Monitoring<br>  主动&#x2F;被动健康检查和监控</li><li>Pricing and Consumption Model<br>  定价和消费模型</li><li>Technical Partnerships 技术合作伙伴关系</li></ul><h2 id="Security-安全"><a href="#Security-安全" class="headerlink" title="Security 安全"></a>Security 安全</h2><p>We start with Security, as this is a critical make-or-break factor for many GPU renters. They store their proprietary model weights on GPUs, which cost tens of thousands to tens of millions of dollars to train and are the core intellectual property of most GenAI companies. Furthermore, training and&#x2F;or inferencing these ML models can involve the use of proprietary or personally identifiable information or other user data. Customers of these companies that rent GPUs do not want their data to be leaked due to the use of an insecure GPU cloud. In EU countries, the stakes are even higher, as there are heavy fines for leaking user data under GDPR law.<br>我们从安全开始，因为这是许多 GPU 租赁者的关键因素。他们将其专有模型权重存储在 GPU 上，这些权重的训练成本从数万美元到数百万美元不等，并且是大多数 GenAI 公司的核心知识产权。此外，训练和&#x2F;或推理这些机器学习模型可能涉及使用专有或个人可识别信息或其他用户数据。这些租用 GPU 的公司的客户不希望由于使用不安全的 GPU 云而导致数据泄露。在欧盟国家，风险更高，因为根据 GDPR 法律，泄露用户数据会面临重罚。</p><p>We also notice that across the industry, there is a long tail of Emerging Neoclouds that do not have even basic SOC2 or ISO 27001 security certifications. We even see some clouds on “AMD Alliance Instinct Cloud Partners” list that do not have basic security such as SOC2 or ISO27001. We have spoken with AMD, and they have confirmed that they are investigating the issue and committed to helping raise the industry standard on this topic.<br>我们还注意到，在整个行业中，有一长串新兴的 Neoclouds，甚至没有基本的 SOC2 或 ISO 27001 安全认证。我们甚至看到一些在“AMD Alliance Instinct Cloud Partners”名单上的云没有基本的安全措施，如 SOC2 或 ISO27001。我们与 AMD 进行了交谈，他们确认正在调查此问题，并承诺帮助提高该主题的行业标准。</p><p>Enterprises mostly rent from hyperscalers, as GPU users trust Hyperscalers to properly implement security measures. We are starting to see some enterprises look into renting from Neoclouds, and most are gravitating toward CoreWeave. These enterprises conduct more stringent due diligence and are risk adverse when renting from a non-Hyperscaler.<br>企业主要从超大规模云服务商那里租用，因为 GPU 用户信任超大规模云服务商能够正确实施安全措施。我们开始看到一些企业考虑从 Neoclouds 租用，大多数企业倾向于 CoreWeave。这些企业在从非超大规模云服务商租用时进行更严格的尽职调查，并且对风险持谨慎态度。</p><p>Many enterprises don’t even have a security checklist to validate their cloud as they expect security akin to the Hyperscalers renting CPUs. Neoclouds entering the enterprise market, such as CoreWeave, must demonstrate to their potential enterprise customers they are secure. CoreWeave, for example, has crossed that hurdle, with financial companies such as Jane Street among its customers. High-frequency trading companies, such as Jane Street, have the strictest security requirements, as they deal with proprietary data and algorithms, which are the secret sauce to how they generate profits.<br>许多企业甚至没有安全检查清单来验证他们的云，因为他们期望的安全性类似于租用 CPU 的超大规模云服务商。进入企业市场的 Neoclouds，例如 CoreWeave，必须向潜在的企业客户证明他们是安全的。例如，CoreWeave 已经跨越了这一障碍，金融公司如 Jane Street 是其客户之一。高频交易公司，如 Jane Street，具有最严格的安全要求，因为他们处理的是专有数据和算法，这些是他们盈利的秘密武器。</p><p>Ensuring tenant network isolation is essential to prevent unauthorized access to data. On Ethernet, this is achieved by setting VLANs on the networking switches such that nodes from Tenant A can only communicate with nodes from Tenant A, and nodes from Tenant A cannot communicate with nodes from Tenant B, and vice versa. On Ethernet, tenant isolation can also be carried out using DPUs, such as Bluefield-3, to manage this isolation instead of having the networking switches handle it. We see only the most advanced GPU cloud operators implementing tenant isolation using DPUs – examples include CoreWeave, OCI, AWS, GCP and Azure.<br>确保租户网络隔离对于防止未经授权访问数据至关重要。在以太网中，通过在网络交换机上设置 VLAN 来实现这一点，使得租户 A 的节点只能与租户 A 的节点通信，而租户 A 的节点不能与租户 B 的节点通信，反之亦然。在以太网中，租户隔离还可以通过使用 DPU（如 Bluefield-3）来进行管理，而不是让网络交换机来处理。我们看到只有最先进的 GPU 云服务提供商在使用 DPU 实施租户隔离——例如 CoreWeave、OCI、AWS、GCP 和 Azure。</p><p>Other GPU clouds lack the technical capabilities to fully utilize the DPU feature set and must resort to implementing tenant isolation on the networking switches instead. On InfiniBand, tenant isolation is done through <a href="https://docs.nvidia.com/networking/display/winof2v310lts/infiniband+network">Partition Keys</a> (PKeys). We recommend that GPU renters explicitly request in their Cloud Master Service Agreement (MSA) that there is tenant networking isolation on both the Ethernet and InfiniBand network through VLANs or IB PKeys to ensure protection via proper tenant isolation further.<br>其他 GPU 云缺乏充分利用 DPU 功能集的技术能力，必须转而在网络交换机上实施租户隔离。在 InfiniBand 中，租户隔离是通过分区密钥（PKeys）来完成的。我们建议 GPU 租户在其云主服务协议（MSA）中明确要求在以太网和 InfiniBand 网络上通过 VLAN 或 IB PKeys 实现租户网络隔离，以确保通过适当的租户隔离进一步保护。</p><p>In addition to PKeys, on InfiniBand there are additional keys that a GPU Operator must set to ensure proper security and to ensure that the InfiniBand network cannot be easily hijacked:<br>除了 PKeys，InfiniBand 还有其他密钥，GPU 操作员必须设置这些密钥以确保适当的安全性，并确保 InfiniBand 网络不会被轻易劫持：</p><p>The <a href="https://docs.oracle.com/cd/E76424_01/html/E36266/z4001ba12074893.html">Subnet Manager Key (SM Key)</a> must be set to prevent unauthorized Subnet Managers&#x2F;UFMs from being deployed. The <a href="https://docs.nvidia.com/networking/display/ibdiagnetusermanualv290/infiniband+security#src-80580471_safe-id-SW5maW5pQmFuZFNlY3VyaXR5LU1hbmFnZW1lbnRLZXkoTUtFWSk">Management Key</a> (MKey) must also be set to protect the fabric against unauthorized configuration changes. To prevent congestion control functions of the fabric from being hijacked, <a href="https://docs.nvidia.com/networking/display/ibdiagnetusermanualv290/infiniband+security#src-80580471_safe-id-SW5maW5pQmFuZFNlY3VyaXR5LUNvbmdlc3Rpb25Db250cm9sS2V5KENDS2V5KQ">CongestionControl Key (CC Key)</a> must be set, this needs to be set in nonblocking IB fabrics too. For InfiniBand fabrics that have SHARP in network reduction enabled, <a href="https://docs.nvidia.com/networking/display/ibdiagnetusermanualv290/infiniband+security#src-80580471_safe-id-SW5maW5pQmFuZFNlY3VyaXR5LUFnZ3JlZ2F0aW9uTWFuYWdlbWVudEtleShBTUtleSk">Aggregation Management Key (AM Key)</a> must be enabled to prevent the InfiniBand’s SHARP Aggregation Manager from being hijacked. As one can see, there are numerous InfiniBand security keys that must be set to ensure proper security; however, there is a notable lack of public documentation from Nvidia and a general lack of awareness and education in the industry regarding these critical keys.<br>子网管理器密钥（SM 密钥）必须设置，以防止未经授权的子网管理器&#x2F;UFMs 被部署。管理密钥（MKey）也必须设置，以保护网络免受未经授权的配置更改。为了防止网络的拥塞控制功能被劫持，必须设置拥塞控制密钥（CC 密钥），这也需要在非阻塞的 IB 网络中设置。对于启用了网络缩减的 SHARP 的 InfiniBand 网络，必须启用聚合管理密钥（AM 密钥），以防止 InfiniBand 的 SHARP 聚合管理器被劫持。可以看出，必须设置许多 InfiniBand 安全密钥以确保适当的安全性；然而，Nvidia 的公开文档明显不足，行业内对这些关键密钥的意识和教育普遍缺乏。</p><p>We recommend that Nvidia provide publicly accessible documentation and education on InfiniBand security, helping GPU clouds properly set it up. We’ve pointed many GPU clouds directly to Nvidia for best practices.<br>我们建议 Nvidia 提供公开可访问的文档和关于 InfiniBand 安全的教育，帮助 GPU 云正确设置。我们已经将许多 GPU 云直接指向 Nvidia 以获取最佳实践。</p><p>We recommend as a GPU renter, you explicitly request in your Cloud Master Service Agreement (MSA) that PKeys, AM Keys, SM Keys, M Keys, CC Keys, VS Keys be set to further solidify that the GPU you rent from have enabled these security protections.<br>我们建议作为 GPU 租用者，您在云主服务协议（MSA）中明确要求设置 PKeys、AM Keys、SM Keys、M Keys、CC Keys、VS Keys，以进一步确保您租用的 GPU 启用了这些安全保护。</p><p>CoreWeave used to provide their multi-tenant GPU cluster offering by having a single Kubernetes cluster housing multiple tenants and then using Kubernetes namespace isolation between each tenant on the same Kubernetes cluster, providing each tenant with a <a href="https://www.vcluster.com/docs/vcluster/introduction/what-are-virtual-clusters">vCluster</a>. This offering, now commonly referred to as “CoreWeave Classic,” was not a secure offering, and CoreWeave had already migrated away from the approach of implementing clusters with Kubernetes namespace isolation between tenants a couple of years ago.  <br>CoreWeave 曾通过拥有一个单一的 Kubernetes 集群来提供其多租户 GPU 集群服务，该集群容纳多个租户，并在同一 Kubernetes 集群上使用 Kubernetes 命名空间隔离每个租户，为每个租户提供一个 vCluster。这个服务现在通常被称为“CoreWeave Classic”，并不是一个安全的服务，CoreWeave 在几年前已经迁移 away from 通过 Kubernetes 命名空间隔离租户来实现集群的方式。</p><p>CoreWeave has switched to having only one tenant per Kubernetes cluster, as this is a properly secure offering. In this implementation, a physical cluster will comprise multiple Kubernetes clusters, and each tenant will have its own Kubernetes cluster, rather than each tenant just having a Kubernetes namespace.<br>CoreWeave 已切换为每个 Kubernetes 集群仅有一个租户，因为这是一个适当安全的服务。在这种实现中，一个物理集群将由多个 Kubernetes 集群组成，每个租户将拥有自己的 Kubernetes 集群，而不是每个租户仅拥有一个 Kubernetes 命名空间。</p><p>The reason why Kubernetes namespace isolation between tenants is not secure is that there are many container escape vulnerabilities, mainly in the GPU Driver or in the container toolkit. These container escape vulnerabilities would allow an attacker to escape a container and move laterally to another user on the same host, potentially escalating to another tenant’s host within the Kubernetes cluster.<br>Kubernetes 命名空间在租户之间的隔离不安全的原因是存在许多容器逃逸漏洞，主要在 GPU 驱动程序或容器工具包中。这些容器逃逸漏洞可能允许攻击者逃离容器并横向移动到同一主机上的另一个用户，可能会升级到 Kubernetes 集群中另一个租户的主机。</p><p>Currently, there are monthly newly discovered known container escape vulnerabilities, but there could potentially be dozens of unknown container escape vulnerabilities. In September 2024, Wiz discovered a critical GPU container and Kubernetes vulnerability that affected over 35% of environments. Thus, doing just Kubernetes namespace isolation is not safe. The isolation boundaries should be on VLANs and each tenant getting their own Kubernetes cluster.<br>目前，每月都有新发现的已知容器逃逸漏洞，但可能还有数十个未知的容器逃逸漏洞。在 2024 年 9 月，Wiz 发现了一个影响超过 35% 环境的关键 GPU 容器和 Kubernetes 漏洞。因此，仅仅进行 Kubernetes 命名空间隔离是不安全的。隔离边界应该在 VLAN 上，每个租户获得自己的 Kubernetes 集群。</p><ul><li><a href="https://www.wiz.io/blog/nvidia-ai-vulnerability-deep-dive-cve-2024-0132">https://www.wiz.io/blog/nvidia-ai-vulnerability-deep-dive-cve-2024-0132</a></li><li><a href="https://www.wiz.io/blog/wiz-research-critical-nvidia-ai-vulnerability">https://www.wiz.io/blog/wiz-research-critical-nvidia-ai-vulnerability</a></li><li><a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5614https:/nvd.nist.gov/vuln/detail/CVE-2025-23359">https://nvidia.custhelp.com/app/answers/detail/a_id&#x2F;5614</a></li><li><a href="https://nvd.nist.gov/vuln/detail/CVE-2025-23359">https://nvd.nist.gov/vuln/detail/CVE-2025-23359</a></li><li><a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5599">https://nvidia.custhelp.com/app/answers/detail/a_id&#x2F;5599</a></li><li><a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5585/~/security-bulletin%3A-nvidia-container-toolkit---november-2024">https://nvidia.custhelp.com/app/answers/detail/a_id&#x2F;5585&#x2F;~&#x2F;security-bulletin%3A-nvidia-container-toolkit—november-2024</a></li></ul><p>Many GPU offerings utilize a multi-tenant cluster, but each tenant is on different sets of specific physical servers such that no two tenants share the same physical servers. For some GPU cloud offerings, particularly with on-demand offerings, there may be more than one tenant per physical host. If there is more than one tenant per physical host, there needs to be VM isolation as container isolation between multi tenants on the same host is not stringent enough due to the above Nvidia&#x2F;AMD container security issues that have arisen nearly every month. To reiterate – if only container isolation is used, then hackers can use these known container escapes to escape to the physical host privileges and potentially peek into the other tenant’s containers and examine model weights. They might even be able to access other servers and retrieve the tenant models of other servers. We strongly recommend against multiple tenants per physical host with just container-based isolation.<br>许多 GPU 产品利用多租户集群，但每个租户都在不同的特定物理服务器上，以至于没有两个租户共享相同的物理服务器。对于某些 GPU 云产品，特别是按需产品，可能每个物理主机上会有多个租户。如果每个物理主机上有多个租户，则需要进行虚拟机隔离，因为在同一主机上多租户之间的容器隔离并不够严格，原因是上述 Nvidia&#x2F;AMD 容器安全问题几乎每个月都会出现。重申一下——如果仅使用容器隔离，那么黑客可以利用这些已知的容器逃逸来获取物理主机的权限，并可能窥视其他租户的容器并检查模型权重。他们甚至可能能够访问其他服务器并检索其他服务器的租户模型。我们强烈建议不要在仅使用基于容器的隔离的情况下，在每个物理主机上使用多个租户。</p><h2 id="Lifecycle-and-Technical-Expertise生命周期和技术专长"><a href="#Lifecycle-and-Technical-Expertise生命周期和技术专长" class="headerlink" title="Lifecycle and Technical Expertise生命周期和技术专长"></a>Lifecycle and Technical Expertise生命周期和技术专长</h2><p>Evaluating technical expertise is crucial when selecting a GPU cloud provider, as this directly impacts your team’s overall experience. The impact of technical expertise is felt even before onboarding, particularly in areas such as marketing clarity, sales process, transparent pricing, reasonable drafts of master service agreements (MSAs), pre-onboarding support, and data migration capabilities.<br>评估技术专长在选择 GPU 云服务提供商时至关重要，因为这直接影响到您团队的整体体验。技术专长的影响在入职前就能感受到，特别是在市场营销清晰度、销售流程、透明定价、合理的主服务协议（MSA）草案、入职前支持和数据迁移能力等方面。</p><p>Moreover, assessing the <strong>Sales Phase</strong> —whether the communication was transparent, technical questions were promptly addressed, and commitments were clearly defined—also reflects the provider’s overall customer-centric approach. We see that experienced GPU clouds often have a technical engineer who consults with the customer to help with a smooth sales and onboarding experience.<br>此外，评估销售阶段——沟通是否透明、技术问题是否及时解决、承诺是否明确——也反映了提供商的整体以客户为中心的方式。我们看到，经验丰富的 GPU 云服务通常会有一位技术工程师与客户咨询，以帮助顺利进行销售和入职体验。</p><p>A straightforward question to ask during the sales process is whether they are familiar with Sylvain. All the top GPU clouds that have optimized their NCCL settings and InfiniBand switch configurations and are skilled at debugging NCCL and Networking fabrics have interacted with the renowned Sylvain.<br>在销售过程中可以问的一个简单问题是他们是否熟悉 Sylvain。所有优化了其 NCCL 设置和 InfiniBand 交换机配置并擅长调试 NCCL 和网络架构的顶级 GPU 云都与著名的 Sylvain 有过互动。</p><p>One essential item in your MSA is the exact delivery dates. Late deliveries are widespread in the GPU cloud industry. As a customer, you should ensure that you are happy with the exact delivery date in your MSA and should make sure there is an escape clause if there are any delays.<br>您的 MSA 中一个重要条款是确切的交付日期。延迟交付在 GPU 云行业中很普遍。作为客户，您应该确保对 MSA 中的确切交付日期感到满意，并确保在出现任何延迟时有逃生条款。</p><p>During the <strong>Preparatory Phase,</strong> leading GPUs typically enable users to migrate data into clusters in advance, significantly reducing the “time to value” by ensuring workloads can begin immediately after onboarding. All Hyperscalers such as GCP, Azure, OCI, and AWS all allow for this. Furthermore, most Neoclouds such as CoreWeave and Nebius allow customers the opportunity to upload giant datasets in advance not to waste GPU time. The GPU cloud should gather enough relevant information and ask key questions to ensure that there are no unexpected speed bumps or roadblocks.<br>在准备阶段，领先的 GPU 通常使用户能够提前将数据迁移到集群中，从而显著减少“价值实现时间”，确保工作负载可以在入驻后立即开始。所有超大规模云服务商，如 GCP、Azure、OCI 和 AWS 都允许这样做。此外，大多数 Neoclouds，如 CoreWeave 和 Nebius，允许客户提前上传大型数据集，以避免浪费 GPU 时间。GPU 云应该收集足够的相关信息并提出关键问题，以确保没有意外的障碍或阻碍。</p><p>The <strong>Onboarding Process</strong> itself is pivotal; the cluster should be provided on time, and there should be a high degree of automation for cluster provisioning, ensuring that there are no human errors or mistakes. The cluster that is provided should have undergone burn-in, and the burn-in process and acceptance tests should be publicly available on their website or in YouTube conference talk recordings. Instances should have no issues with reboot. After a reboot, all systems should be working, and there should be no need to manually set anything, for example, re-mounting networked filesystems. When we apply the ClusterMAX™ Rating System during the onboarding phase, we assess the “time to value” or “time to successfully launch useful work.” For example, if managed Slurm or Kubernetes is available out of the box, the end user does not need to spend a few days figuring out how to install it themselves, thus shortening the time to value.<br>入职流程本身至关重要；集群应按时提供，并且集群配置应高度自动化，以确保没有人为错误或失误。提供的集群应经过烧机测试，烧机过程和验收测试应在其网站或 YouTube 会议录音中公开可用。实例在重启时应没有问题。重启后，所有系统应正常工作，无需手动设置任何内容，例如重新挂载网络文件系统。当我们在入职阶段应用 ClusterMAX™评级系统时，我们评估“价值实现时间”或“成功启动有用工作的时间”。例如，如果托管的 Slurm 或 Kubernetes 开箱即用，最终用户就不需要花几天时间自己安装，从而缩短了价值实现时间。</p><p>Ongoing support during the <strong>Main Working Phase</strong> is critical as GPUs do have a higher failure rate than traditional CPU servers. H100s&#x2F;H200s tend to experience soft or hard failures; therefore, it is critical to have excellent support services. <strong>The</strong> <strong>MI300x has more than twice the failure rate as the H100&#x2F;H200 due to higher temperatures and less mature Samsung HBM.</strong> For maintenance incidents and outages, we observe that top GPU clouds communicate what is happening, the debug steps being carried out, and an ETA for implementing the fix. For failures of 1-2 GPU nodes, we observe top GPU clouds quickly spinning up new nodes within 90 seconds to provide to their tenants, ensuring that their customers don’t need to wait for troubleshooting. Top GPU clouds will also compensate customers fairly for outages. The loss of just a single GPU server for training means the unavailability of a significant portion of a cluster, as most training codebases require a specific number of GPUs and cannot operate without even a single GPU.<br>在主要工作阶段期间，持续支持至关重要，因为 GPU 的故障率确实高于传统的 CPU 服务器。H100&#x2F;H200 往往会出现软故障或硬故障；因此，拥有优秀的支持服务至关重要。由于温度较高和三星 HBM 不够成熟，MI300x 的故障率是 H100&#x2F;H200 的两倍以上。对于维护事件和停机，我们观察到顶级 GPU 云会沟通发生了什么，正在进行的调试步骤，以及实施修复的预计时间。对于 1-2 个 GPU 节点的故障，我们观察到顶级 GPU 云会在 90 秒内快速启动新的节点以提供给他们的租户，确保他们的客户不需要等待故障排除。顶级 GPU 云还会公平地补偿客户因停机而造成的损失。仅仅失去一台用于训练的 GPU 服务器就意味着集群中很大一部分不可用，因为大多数训练代码库需要特定数量的 GPU，并且没有任何一个 GPU 就无法运行。</p><p>Lastly, for the <strong>Offboarding Phase</strong>, we evaluate if there is vendor lock-in. Hyperscalers famously have high egress fees to prevent their customers from switching to another cloud. Most Neocloud Giants and Emerging Neoclouds do not charge egress fees for moving data to another GPU cloud.<br>最后，在离职阶段，我们评估是否存在供应商锁定。超大规模云服务商以高额的出口费用而闻名，以防止客户切换到其他云服务。大多数新云巨头和新兴云服务商在将数据迁移到其他 GPU 云时不收取出口费用。</p><h2 id="Slurm-and-Kubernetes-Slurm-和-Kubernetes"><a href="#Slurm-and-Kubernetes-Slurm-和-Kubernetes" class="headerlink" title="Slurm and Kubernetes Slurm 和 Kubernetes"></a>Slurm and Kubernetes Slurm 和 Kubernetes</h2><p>90% of customers prefer Kubernetes for inference workloads, and about 50% of the customers use Slurm for training. Top-ranking GPU providers are increasingly differentiating themselves by offering fully tested, out-of-the-box managed Kubernetes and Slurm environments, in addition to providing raw GPU node offerings without pre-configured schedulers. Customers universally prefer these managed scheduling solutions, as self-setup Slurm and self-setup Kubernetes consume valuable GPU resources and may take days to set up, directly increasing costs and delaying productive work.<br>90%的客户更喜欢使用 Kubernetes 进行推理工作负载，约 50%的客户使用 Slurm 进行训练。顶级 GPU 供应商通过提供经过全面测试的即插即用的托管 Kubernetes 和 Slurm 环境，越来越多地实现差异化，此外还提供没有预配置调度程序的原始 GPU 节点产品。客户普遍更喜欢这些托管调度解决方案，因为自我设置的 Slurm 和自我设置的 Kubernetes 会消耗宝贵的 GPU 资源，并且可能需要几天时间来设置，直接增加了成本并延迟了生产工作。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-6.-SLURM-GIMP.png?resize=1024,765&ssl=1"></p><p>Source: Futurama 来源：未来狂想曲</p><p>We expect that Slurm will remain popular well into the distant future. A common misconception is that the choice of scheduler is independent of whether virtualization is used. You can have bare metal Slurm, or you can have Slurm with VMs. Slurm and bare metal are not mutually exclusive. The same applies to Kubernetes; you can have bare-metal Kubernetes, as is the case at CoreWeave, or have Kubernetes with virtual machines, such as in GKE or EKS.<br>我们预计 Slurm 将在遥远的未来继续受欢迎。一个常见的误解是调度程序的选择与是否使用虚拟化是独立的。您可以使用裸金属 Slurm，或者可以使用带有虚拟机的 Slurm。Slurm 和裸金属并不相互排斥。Kubernetes 也是如此；您可以使用裸金属 Kubernetes，正如 CoreWeave 的情况，或者使用带有虚拟机的 Kubernetes，例如 GKE 或 EKS。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-7.-Kubernetes-and-Bare-metal-GIMP.png?resize=522,238&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>When providers offer managed Kubernetes and Slurm platforms, this enables customers to maximize their GPU utilization, significantly reducing their time-to-useful work. Notably, even technically sophisticated organizations such as <strong>Meta</strong> and <strong>Jane Street</strong> choose to use CoreWeave’s managed Slurm and Kubernetes offerings due to their effectiveness and reliability. CoreWeave’s managed Slurm and Kubernetes go a long way to help increase goodput and increase time to value. A notable exception is OpenAI, which opts out of managed schedulers due to heightened security and operational paranoia surrounding artificial general intelligence (AGI).<br>当提供商提供托管的 Kubernetes 和 Slurm 平台时，这使客户能够最大化他们的 GPU 利用率，显著减少他们的有效工作时间。值得注意的是，即使是技术上复杂的组织，如 Meta 和 Jane Street，也选择使用 CoreWeave 的托管 Slurm 和 Kubernetes 产品，因为它们的有效性和可靠性。CoreWeave 的托管 Slurm 和 Kubernetes 在提高有效吞吐量和增加价值时间方面发挥了重要作用。一个显著的例外是 OpenAI，由于对人工通用智能（AGI）周围的安全性和操作偏执的担忧，选择不使用托管调度程序。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-8.-SemiAnalysis-Slurm-estimates-GIMP.png?resize=1024,523&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>We also see that a lot of providers do not have <a href="https://slurm.schedmd.com/topology.html">topology.conf set up for their out of the box Slurm solution.</a> Not having topology.conf set up leads to workload slowdowns and decreases NCCL performance. Some providers’ Slurm solutions are also not properly set up and do not have the <a href="https://github.com/NVIDIA/pyxis">pyxis plugin which allows reproducibility environments with containers as Slurm and is widely used by customers</a> across CoreWeave, GCP, AWS, OCI, and other major providers. Other providers do not adequately set up NVIDIA HPC-X modules or Slurm MPI integrations.<br>我们还看到许多提供商没有为他们的开箱即用的 Slurm 解决方案设置 topology.conf。没有设置 topology.conf 会导致工作负载减慢并降低 NCCL 性能。一些提供商的 Slurm 解决方案也没有正确设置，并且没有 pyxis 插件，该插件允许使用容器作为 Slurm 的可重现环境，并被 CoreWeave、GCP、AWS、OCI 和其他主要提供商的客户广泛使用。其他提供商没有充分设置 NVIDIA HPC-X 模块或 Slurm MPI 集成。</p><h2 id="Storage-存储"><a href="#Storage-存储" class="headerlink" title="Storage 存储"></a>Storage 存储</h2><p>Efficient and performant storage solutions are essential for machine learning workloads, both for training and inference. We see most customers want managed high-performance parallel filesystems such as Weka, Lustre, Vast Data, DDN, and&#x2F;or want a managed s3-compatible object storage.<br>高效且性能优越的存储解决方案对于机器学习工作负载至关重要，无论是训练还是推理。我们看到大多数客户希望使用托管的高性能并行文件系统，如 Weka、Lustre、Vast Data、DDN，和&#x2F;或希望使用托管的兼容 S3 的对象存储。</p><p>During training, vast quantities of data must be quickly and reliably accessed to feed GPUs without bottlenecks, which means that high-performance storage is needed for model checkpoint loads and saves such that GPUs can maximize MFU and thereby significantly accelerate training time.<br>在训练过程中，必须快速且可靠地访问大量数据，以便为 GPU 提供数据而不造成瓶颈，这意味着需要高性能存储来加载和保存模型检查点，以便 GPU 能够最大化 MFU，从而显著加快训练时间。</p><p>Managed object storage options are equally crucial for flexible, cost-effective, and scalable data storage, enabling teams to efficiently store, version, and retrieve training datasets, checkpoints, and model artifacts.<br>管理对象存储选项对于灵活、经济高效和可扩展的数据存储同样至关重要，使团队能够高效地存储、版本控制和检索训练数据集、检查点和模型工件。</p><p>For ML inference workloads, performance-oriented storage ensures that models are loaded rapidly from storage production scenarios. Slow or inefficient storage can cause noticeable delays, degrading the end-user experience or reducing real-time responsiveness of AI-driven applications. It is, therefore, vital to assess whether GPU cloud providers offer robust managed parallel filesystem and object storage solutions, ensuring that these options are optimized and validated for excellent performance across varied workloads.<br>对于机器学习推理工作负载，面向性能的存储确保模型能够快速从存储生产场景中加载。缓慢或低效的存储可能会导致明显的延迟，降低最终用户体验或减少基于人工智能的应用程序的实时响应能力。因此，评估 GPU 云服务提供商是否提供强大的托管并行文件系统和对象存储解决方案至关重要，确保这些选项经过优化和验证，以在各种工作负载中实现卓越性能。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-9.-Nvidia-NIXL-GIMP.png?resize=1024,643&ssl=1"></p><p>Source: Nvidia 来源：Nvidia</p><p>With GPUs, the main two sources of user frustration with storage are when file volumes randomly unmount and when users encounter the Lots of Small File (LOSF) problem. The solution to the random unmounting issue is to use a program called “autofs” that will automatically keep your shared filesystem mounted.<br>使用 GPU 时，用户在存储方面主要有两个来源的挫败感：文件卷随机卸载和用户遇到大量小文件（LOSF）问题。解决随机卸载问题的方法是使用一个名为“autofs”的程序，它会自动保持您的共享文件系统挂载。</p><p>Next, the LOSF problem can easily be avoided as it is only an issue if you decide to roll out your own storage solution like an NFS-server instead of paying for a storage software vendor like Weka or Vast. An end user will very quickly notice an LOSF problem on the cluster as the time even to import PyTorch into Python will lead to a complete lag out if an LOSF problem exists on the cluster.<br>接下来，LOSF 问题可以很容易地避免，因为它仅在您决定推出自己的存储解决方案（如 NFS 服务器）而不是支付像 Weka 或 Vast 这样的存储软件供应商时才会成为问题。最终用户会很快注意到集群中的 LOSF 问题，因为即使是将 PyTorch 导入 Python 的时间，如果集群中存在 LOSF 问题，也会导致完全的延迟。</p><p>The diagram below, produced during our testing on Crusoe’s cluster, demonstrates how a cluster storage solution optimized and free of the LOSF problem should behave. As you can see, the time to complete importing PyTorch into the Python process stays relatively flat even when scaling up GPU count.<br>下面的图表是在我们对 Crusoe 集群进行测试时生成的，展示了一个经过优化且没有 LOSF 问题的集群存储解决方案应如何表现。正如您所看到的，即使在增加 GPU 数量时，导入 PyTorch 到 Python 进程所需的时间仍然相对平稳。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-10.-Time-to-Pytorch-1-GIMP.png?resize=1024,677&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>This is a world of difference to a cluster that is running on unoptimized shared storage, where the time required to import PyTorch in a Python multi-node training run explodes, often causing the cluster to be completely unusable. Notice the difference between high-performing storage and how another cluster with LOSF issues would behave.<br>这与在未优化的共享存储上运行的集群有天壤之别，在这种情况下，导入 PyTorch 到 Python 多节点训练运行所需的时间会激增，常常导致集群完全无法使用。请注意高性能存储与另一个存在 LOSF 问题的集群之间的差异。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-11.-Time-to-Pytorch-2-GIMP.png?resize=1024,678&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><h2 id="NCCL-RCCL-Networking-PerformanceNCCL-RCCL-网络性能"><a href="#NCCL-RCCL-Networking-PerformanceNCCL-RCCL-网络性能" class="headerlink" title="NCCL&#x2F;RCCL Networking PerformanceNCCL&#x2F;RCCL 网络性能"></a>NCCL&#x2F;RCCL Networking PerformanceNCCL&#x2F;RCCL 网络性能</h2><p>When selecting GPU cloud services, thorough validation of NCCL&#x2F;RCCL networking performance is vital for maximizing training and inference performance. Providers should offer validated, out-of-the-box NCCL&#x2F;RCCL-tests scripts enabling customers to independently confirm network performance, particularly within the critical real-world message-size range of 16MiB to 512MiB.<br>在选择 GPU 云服务时，彻底验证 NCCL&#x2F;RCCL 网络性能对于最大化训练和推理性能至关重要。服务提供商应提供经过验证的即插即用 NCCL&#x2F;RCCL 测试脚本，使客户能够独立确认网络性能，特别是在关键的实际消息大小范围内，即 16MiB 到 512MiB。</p><p>A network that is half as slow on all reduce leads to a 10% drop in performance on MFU on an O(70B) training and a 15-20% drop in MFU for an O(8x7B) mixture of expert models. It is a common misconception that inference does not need high-speed networking. In reality, inference providers apply network-bandwidth-intensive techniques like <a href="https://arxiv.org/abs/2401.09670">disaggregated serving</a> to achieve cost-effective, high-performance inferencing. Disaggregated serving has been an industry standard for years, and last week NVIDIA open-sourced <a href="https://ynamo/">Dynamo</a>, a distributed inference framework, further democratized disaggregated serving and many other inference optimization techniques.<br>在所有 reduce 操作中，网络速度减半会导致 O(70B)训练的 MFU 性能下降 10%，而 O(8x7B)专家模型混合的 MFU 性能下降 15-20%。人们普遍误解推理不需要高速网络。实际上，推理提供商应用了网络带宽密集型技术，如分离服务，以实现具有成本效益的高性能推理。分离服务多年来一直是行业标准，上周 NVIDIA 开源了 Dynamo，一个分布式推理框架，进一步使分离服务和许多其他推理优化技术民主化。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-12.-Blackwell-25x-hopper-GIMP.png?resize=1024,538&ssl=1"></p><p>Source: Nvidia 来源：Nvidia</p><p>It’s essential to recognize that not all on paper 400G networks offer identical performance – actual realized network performance depends heavily on whether the network is non-blocking and rail optimized, whether it uses InfiniBand or Ethernet, and which NICs and switches are used, and finally if the GPU operator has properly configured and validated their network fabric with nccl&#x2F;rccl-tests.<br>重要的是要认识到，并非所有纸面上的 400G 网络都提供相同的性能——实际实现的网络性能在很大程度上取决于网络是否是无阻塞的和铁路优化的，是否使用 InfiniBand 或以太网，以及使用了哪些 NIC 和交换机，最后，如果 GPU 操作员已正确配置并验证了他们的网络结构与 nccl&#x2F;rccl 测试。</p><p>We observe that network fabrics utilizing ConnectX-7 NICs perform the best. We see that well-tuned network with premium switches, as is the case at OCI, can be very competitive with Spectrum-X Ethernet networking. InfiniBand still tends to perform the best, especially when enabling SHARP in-network reductions. We have run networking NCCL&#x2F;RCCL benchmarks from 128 GPUs to 1024 GPUs and will release all the performance data in our upcoming NCCL&#x2F;RCCL networking Deep Dive article within the next couple of months. We have tested and are releasing our analysis of the following networks in the upcoming article:<br>我们观察到，使用 ConnectX-7 NIC 的网络结构表现最佳。我们看到，像 OCI 这样的优质交换机与良好调优的网络可以与 Spectrum-X 以太网网络非常具有竞争力。InfiniBand 仍然往往表现最佳，特别是在启用 SHARP 网络内缩减时。我们已经从 128 个 GPU 到 1024 个 GPU 运行了网络 NCCL&#x2F;RCCL 基准测试，并将在接下来的几个月内发布我们即将推出的 NCCL&#x2F;RCCL 网络深度分析文章中的所有性能数据。我们已经测试并将在即将发布的文章中发布以下网络的分析：</p><ul><li>8x400GbE Spectrum-X RoCEv2 Ethernet H100<br>  8x400GbE Spectrum-X RoCEv2 以太网 H100</li><li>8x400G InfiniBand NDR H100</li><li>8x400G InfiniBand NDR with SHARP H100<br>  8x400G InfiniBand NDR 与 SHARP H100</li><li>8x400GbE Oracle Cloud RoCev2 Ethernet H100<br>  8x400GbE Oracle Cloud RoCev2 以太网 H100</li><li>8x200GbE Google Cloud Fastrak Ethernet a3-mega H100<br>  8x200GbE 谷歌云 Fastrak 以太网 a3-mega H100</li><li>8x400GbE Google Cloud RoCEv2 Ethernet a3-ultra H200<br>  8x400GbE 谷歌云 RoCEv2 以太网 a3-ultra H200</li><li>16x200GbE AWS EFAv3 Ethernet p5en H200<br>  16x200GbE AWS EFAv3 以太网 p5en H200</li><li>8x400GbE RoCEv2 Ethernet MI300X<br>  8x400GbE RoCEv2 以太网 MI300X</li></ul><p>Even if a network is non-blocking, we see that networks with larger 1-hop rail optimized pods can achieve higher NCCL performance because less traffic will need to transit between rail pods, thus leading to less congestion. On GCP’s 8x400GbE a3-ultra, their 1-hop rail pod consists of just 4 nodes, whereas for OCI Ethernet and the InfiniBand reference architecture, the 1-hop rail pod is 32 servers in size. This is one of the reasons why OCI 4x400GbE offering has better NCCL performance than GCP’s latest 8x400GbE a3-ultra offering.<br>即使网络是非阻塞的，我们看到具有更大 1 跳轨道优化节点的网络可以实现更高的 NCCL 性能，因为需要在轨道节点之间传输的流量更少，从而导致拥堵更少。在 GCP 的 8x400GbE a3-ultra 上，他们的 1 跳轨道节点仅由 4 个节点组成，而对于 OCI 以太网和 InfiniBand 参考架构，1 跳轨道节点的规模为 32 台服务器。这是 OCI 4x400GbE 产品的 NCCL 性能优于 GCP 最新的 8x400GbE a3-ultra 产品的原因之一。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-13.-H100-Topology-GIMP.png?resize=1024,549&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>GPU clouds implementing topology-aware tenant allocation means better performance as tenant nodes can minimize the number of hops when communicating with each other by binpacking tenant nodes into the minimum amount of rail pods needed. Topology-aware tenant allocation is required even for nonblocking networks.<br>实施拓扑感知租户分配的 GPU 云意味着更好的性能，因为租户节点可以通过将租户节点打包到所需的最少轨道节点中来最小化相互通信时的跳数。即使对于非阻塞网络，拓扑感知租户分配也是必需的。</p><p>Within a tenant’s environment, out-of-box topology-aware scheduling configurations such as Kubernetes topology or Slurm’s topology.conf are crucial, even in optimized and non-blocking network setups. We have seen that such configurations can yield performance gains of 20-30% for certain message sizes, regardless of cluster size. We observe that this has a significant impact on performance for workloads that utilize only a portion of a cluster, as well as for workloads that occupy the entire cluster. We have seen on a 1024-GPU non-blocking InfiniBand Reference architecture deployment that nccl-tests are 20-30% slower for certain message sizes. We will expand on this topic in our NCCL&#x2F;RCCL deep dive article.<br>在租户环境中，开箱即用的拓扑感知调度配置，如 Kubernetes 拓扑或 Slurm 的 topology.conf，是至关重要的，即使在优化和非阻塞的网络设置中。我们已经看到，这些配置可以为某些消息大小带来 20-30%的性能提升，无论集群大小如何。我们观察到，这对仅利用集群一部分的工作负载以及占用整个集群的工作负载的性能有显著影响。我们在 1024-GPU 非阻塞 InfiniBand 参考架构部署中看到，对于某些消息大小，nccl-tests 的速度慢了 20-30%。我们将在我们的 NCCL&#x2F;RCCL 深度探讨文章中进一步扩展这一主题。</p><p>For Ethernet-based setups, understanding whether providers employ high-quality Arista switches with a high-quality, battle-tested NOS, such as Arista EOS, that has been properly tuned for optimal performance, or whether they are using less expensive alternatives, can significantly impact NCCL performance. Note that there are considerable performance differences between a great Tomahawk5 switch from a top switch company and a mediocre Tomahawk5 switch from a middle-of-the-pack switch company. This is especially true when it comes to WhiteBox Tomahawk5 switches that are not well-tuned and poorly configured by the GPU cloud. It is possible to tune them really well, it is just the case that many clouds have not because it is challenging to do so.<br>对于基于以太网的设置，了解提供商是否使用高质量的 Arista 交换机以及经过充分调优以实现最佳性能的高质量、经过实战检验的 NOS（如 Arista EOS），或者他们是否使用更便宜的替代品，可能会显著影响 NCCL 性能。请注意，顶级交换公司出色的 Tomahawk5 交换机与中等水平交换公司普通的 Tomahawk5 交换机之间存在显著的性能差异。尤其是在 GPU 云未经过良好调优和配置的 WhiteBox Tomahawk5 交换机方面，这一点尤为明显。虽然可以对它们进行良好的调优，但许多云服务提供商并没有这样做，因为这是一项具有挑战性的工作。</p><p>Another consideration is if SHARP in-network reduction is enabled on the InfiniBand fabric. There are only three GPU providers in the world that have correctly set up SHARP – namely CoreWeave, Azure, and Firmus&#x2F;Sustainable Metal Cloud. InfiniBand SHARP provides a boost to network performance by doing reductions inside the InfiniBand switch instead of in the SMs of the GPUs. Even when SHARP is enabled by the GPU provider, it is very difficult for the end user to correctly tune and make sure their training and inference codebase have the proper versions of PyTorch, Nvidia drivers, and the proper NCCL library version to gain a performance speedup.<br>另一个考虑因素是是否在 InfiniBand 网络上启用了 SHARP 内网缩减。全球只有三家 GPU 供应商正确设置了 SHARP，即 CoreWeave、Azure 和 Firmus&#x2F;Sustainable Metal Cloud。InfiniBand SHARP 通过在 InfiniBand 交换机内部进行缩减，而不是在 GPU 的 SM 中，从而提升网络性能。即使 GPU 供应商启用了 SHARP，最终用户也很难正确调整并确保他们的训练和推理代码库具有适当版本的 PyTorch、Nvidia 驱动程序和适当的 NCCL 库版本，以获得性能提升。</p><p>Due to this difficulty, there are only five customers in the world across the GPU cloud industry that are using SHARP for production training and inference workloads, and out of these five customers, one is Nvidia itself. As such, we recommend that Nvidia make SHARP easier to set up for the GPU provider and enable SHARP by default, thereby improving the overall user experience when deploying and managing SHARP.<br>由于这个困难，全球只有五个客户在 GPU 云行业中使用 SHARP 进行生产训练和推理工作，其中一个客户就是 Nvidia。因此，我们建议 Nvidia 使 SHARP 更容易为 GPU 提供商设置，并默认启用 SHARP，从而改善在部署和管理 SHARP 时的整体用户体验。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-14.-128-GPU-with-400G-non-blocking-GIMP.png?resize=1024,799&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>Lastly, we see that the top 10% of GPU clouds are focused on developing <a href="https://github.com/NVIDIA/nccl/tree/master/ext-profiler">NCCL profiler plugins,</a> enabling them to gain deep performance observability and let their customers gain more insights into the NCCL performance for debugging and optimization. GPU clouds that choose to deploy NCCL monitoring plugins into production environments will enable their customers to achieve faster performance and higher goodput, leading to a better overall customer experience and greater performance per dollar spent.<br>最后，我们看到前 10%的 GPU 云专注于开发 NCCL 分析器插件，使他们能够获得深度性能可观察性，并让他们的客户获得更多关于 NCCL 性能的洞察，以便进行调试和优化。选择将 NCCL 监控插件部署到生产环境中的 GPU 云将使他们的客户实现更快的性能和更高的有效吞吐量，从而带来更好的整体客户体验和更高的每花费一美元的性能。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-15.-Nccl-plugin-GIMP.png?resize=1024,541&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><h2 id="Reliability-and-Service-Level-Agreements-SLAs-可靠性和服务水平协议（SLA）"><a href="#Reliability-and-Service-Level-Agreements-SLAs-可靠性和服务水平协议（SLA）" class="headerlink" title="Reliability and Service Level Agreements (SLAs)可靠性和服务水平协议（SLA）"></a>Reliability and Service Level Agreements (SLAs)可靠性和服务水平协议（SLA）</h2><p>Reliability and clear Service Level Agreements (SLAs) are the foundational agreement of what you expect from your GPU provider’s uptime. Precision about how providers define SLA events – whether they’re related to node failures, network disruptions like link flapping, or hardware-software-level issues such as NCCL timeouts – is essential. For example, a GPU cloud operating under a vaguely defined SLA could try to claim they are meeting their 99% SLA requirement even if a cluster is unusable if there is a NIC that flaps for one microsecond every minute.<br>可靠性和明确的服务水平协议（SLA）是您对 GPU 提供商正常运行时间的期望的基础协议。关于提供商如何定义 SLA 事件的精确性至关重要——无论是与节点故障、网络中断（如链路抖动）还是与硬件-软件级别问题（如 NCCL 超时）相关。例如，运行在模糊定义的 SLA 下的 GPU 云可能会试图声称他们满足 99%的 SLA 要求，即使在集群不可用的情况下，如果每分钟有一个 NIC 抖动一微秒。</p><p>When NICs flap even for one microsecond, it causes NCCL to stall, and the entire training workload hangs. In such a case, it may take anywhere from a couple of minutes to 30 minutes to restart the workload. Top GPU clouds typically have a low rate of NCCL timeouts and NIC flaps, as they have already undergone the process of burning in their network and transceivers, and have taken essential steps such as cleaning dust off fiber cables, which is one of the leading causes of degraded goodput.<br>当 NIC 抖动即使只有一微秒时，会导致 NCCL 停滞，整个训练工作负载挂起。在这种情况下，重新启动工作负载可能需要几分钟到 30 分钟不等。顶级 GPU 云通常具有较低的 NCCL 超时和 NIC 抖动率，因为它们已经经历了网络和收发器的烧录过程，并采取了清洁光纤电缆等必要步骤，而光纤电缆上的灰尘是导致良好吞吐量下降的主要原因之一。</p><p>Top GPU Providers typically outline conditions under which service credits are issued, including the mechanisms for credit reimbursement, transparency around these processes, and the turnaround time for resolving incidents. Equally important is whether the provider maintains readily available “hot spares,” which enable immediate failover in the event of hardware failure, thereby significantly reducing downtime.<br>顶级 GPU 供应商通常会概述发放服务信用的条件，包括信用报销的机制、这些过程的透明度以及解决事件的周转时间。同样重要的是，供应商是否保持随时可用的“热备件”，以便在硬件故障发生时能够立即切换，从而显著减少停机时间。</p><p>We see top GPU clouds leverage specialized deployment teams for cluster burn-in and deployment. These teams will integrate and test at the individual server level and at the cluster-wide level, during which networking testing will be carried out at OEMs’ integration factory. We recommend that the cluster-wide high-temperature burn-in should last at least 3-4 weeks so as to catch all the infant mortality-related failures among the node’s components.<br>我们看到顶级 GPU 云利用专业的部署团队进行集群的烧机和部署。这些团队将在单个服务器级别和集群级别进行集成和测试，在此期间将在 OEM 的集成工厂进行网络测试。我们建议集群范围内的高温烧机应持续至少 3-4 周，以捕捉节点组件中的所有早期故障相关问题。</p><p>It is common for integration teams to pitch using LINPACK as their burn-in and acceptance process. Still, we don’t believe this is a very good test, as LINPACK does not utilize the network extensively, nor does it heavily utilize the GPU’s HBM memory; instead, it only utilizes and tests the GPU’s FP64 cores. ML Training, by contrast, is very network, HBM, and BF16&#x2F;FP16&#x2F;FP8 tensor core intensive, and as such, we believe that it is a burn-in and acceptance test that carries out a proper burn-in of critical components.<br>集成团队通常会使用 LINPACK 作为其烧机和验收过程。然而，我们认为这并不是一个很好的测试，因为 LINPACK 并没有广泛利用网络，也没有大量使用 GPU 的 HBM 内存；相反，它只利用并测试 GPU 的 FP64 核心。相比之下，机器学习训练非常依赖网络、HBM 和 BF16&#x2F;FP16&#x2F;FP8 张量核心，因此我们认为这是一个能够对关键组件进行适当烧机的烧机和验收测试。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-16.-Failure-rate-GIMP.png?resize=1024,691&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>Testing and burn-in procedures are essential practices designed to identify potential hardware or networking issues before deployment. Providers that openly share burn-in reports and document their testing processes publicly demonstrate higher accountability and confidence in their infrastructure’s stability. This level of transparency is required to be a ClusterMAX™ Platinum GPU cloud. CoreWeave has demonstrated its in-depth acceptance and burn-in procedures through its KubeCon talk and its many public blog posts.<br>测试和烧机程序是旨在在部署之前识别潜在硬件或网络问题的基本实践。那些公开分享烧机报告并公开记录其测试过程的提供商展示了更高的责任感和对其基础设施稳定性的信心。要成为 ClusterMAX™铂金 GPU 云，这种透明度是必需的。CoreWeave 通过其 KubeCon 演讲和众多公开博客文章展示了其深入的接受和烧机程序。</p><p>For failures of 1-2 GPU nodes, we see top GPU clouds typically can quickly spin up new nodes within 90 seconds to give to their tenants such that their customers don’t need to wait for troubleshooting. Top GPU clouds also fairly compensate for inaccessibility. The unavailability of just a single GPU server means the unavailability of a large chunk of a cluster, since most training codebases require a set number of GPUs and cannot afford to be missing a single GPU.<br>对于 1-2 个 GPU 节点的故障，我们看到顶级 GPU 云通常可以在 90 秒内快速启动新节点，以便提供给其租户，这样他们的客户就不需要等待故障排除。顶级 GPU 云也会对不可用性进行合理补偿。仅仅一个 GPU 服务器的不可用意味着集群中大部分的不可用，因为大多数训练代码库需要一定数量的 GPU，无法承受缺少一个 GPU。</p><h2 id="Automated-Active-and-Passive-Health-Checks-and-Monitoring自动化主动和被动健康检查与监控"><a href="#Automated-Active-and-Passive-Health-Checks-and-Monitoring自动化主动和被动健康检查与监控" class="headerlink" title="Automated Active and Passive Health Checks and Monitoring自动化主动和被动健康检查与监控"></a>Automated Active and Passive Health Checks and Monitoring自动化主动和被动健康检查与监控</h2><p>Top GPU providers offer customers the option to implement robust, out-of-the-box passive health checks designed to identify GPU anomalies, such as critical XID errors promptly. These passive checks monitor GPUs for fallback events, mainly when GPUs degrade and revert to slower PCI bus communication modes. Providers ensure continuous awareness of GPU health, detecting conditions that might otherwise lead to reduced performance or failures. These passive health checks verify node integrity by confirming that storage mounts remain stable and accessible, ensuring data availability and system reliability.<br>顶级 GPU 供应商为客户提供实施强大、开箱即用的被动健康检查的选项，旨在及时识别 GPU 异常，例如关键的 XID 错误。这些被动检查监控 GPU 的回退事件，主要是在 GPU 降级并恢复到较慢的 PCI 总线通信模式时。供应商确保持续了解 GPU 健康状况，检测可能导致性能下降或故障的情况。这些被动健康检查通过确认存储挂载保持稳定和可访问来验证节点完整性，确保数据可用性和系统可靠性。</p><p>Beyond passive monitoring, leading GPU providers equip their solutions with automated weekly scheduled and preemptible <strong>active health checks</strong> that proactively validate hardware and software performance on a weekly basis. These active diagnostics commonly include NCCL-tests, which evaluate GPU-to-GPU communication integrity and performance, ensuring clusters can efficiently perform collective operations and match the expected reference numbers. Providers also integrate the NVIDIA Data Center GPU Manager (DCGM) diagnostic suite (sudo dcgmi diag -r 4), which deeply evaluates GPU hardware health, identifying issues before they escalate.<br>除了被动监控，领先的 GPU 供应商为其解决方案配备了自动化的每周定期和可抢占的主动健康检查，这些检查每周主动验证硬件和软件性能。这些主动诊断通常包括 NCCL 测试，评估 GPU 之间的通信完整性和性能，确保集群能够高效地执行集体操作并匹配预期的参考数字。供应商还集成了 NVIDIA 数据中心 GPU 管理器（DCGM）诊断套件（sudo dcgmi diag -r 4），深入评估 GPU 硬件健康，识别问题以防止其升级。</p><p>To further strengthen detection capabilities, top GPU providers incorporate sophisticated automatically weekly scheduled <strong>Silent Data Corruption (SDC) detection checks</strong>, such as TinyMeg2, enabling early detection of subtle corruption issues within the tensor core or SIMT unit. They enhance these diagnostics with rapid ML training Megatron convergence tests—typically two-minute benchmarks—that quickly reveal deviations in computational correctness, thereby minimizing downtime and preserving data accuracy.<br>为进一步增强检测能力，顶级 GPU 供应商结合了复杂的自动每周定期静默数据损坏（SDC）检测检查，如 TinyMeg2，使得能够及早发现张量核心或 SIMT 单元中的微妙损坏问题。他们通过快速的机器学习训练 Megatron 收敛测试增强这些诊断——通常是两分钟的基准测试——迅速揭示计算正确性的偏差，从而最小化停机时间并保持数据准确性。</p><p>Finally, premium GPU providers offer extensive <strong>out-of-the-box Grafana monitoring capabilities</strong>, providing operators with deep visibility into their clusters’ operational states. This monitoring typically includes live tracking of TFLOP&#x2F;s estimates, enabling an accurate and real-time assessment of GPU performance. Providers deliver comprehensive monitoring for critical interconnect links, promptly identifying link flaps or intermittent connectivity issues. Additionally, integrations with visualization platforms such as Grafana enable intuitive monitoring dashboards that include features like real-time Slurm job queue integration, performance trend visualization, and clear indicators showing when active health checks were last executed, empowering administrators with actionable insights to maintain optimal GPU performance and reliability.<br>最终，优质 GPU 供应商提供广泛的开箱即用 Grafana 监控功能，为操作员提供对其集群操作状态的深度可见性。此监控通常包括对 TFLOP&#x2F;s 估算的实时跟踪，使 GPU 性能的准确和实时评估成为可能。供应商提供对关键互连链路的全面监控，及时识别链路波动或间歇性连接问题。此外，与 Grafana 等可视化平台的集成使得直观的监控仪表板成为可能，包含实时 Slurm 作业队列集成、性能趋势可视化以及清晰的指示器，显示上次执行活动健康检查的时间，赋予管理员可操作的洞察，以维持最佳的 GPU 性能和可靠性。</p><h2 id="Consumption-Models-Price-Per-Value-and-Availability消费模型、价值价格和可用性"><a href="#Consumption-Models-Price-Per-Value-and-Availability消费模型、价值价格和可用性" class="headerlink" title="Consumption Models, Price Per Value, and Availability消费模型、价值价格和可用性"></a>Consumption Models, Price Per Value, and Availability消费模型、价值价格和可用性</h2><p>Pricing, consumption models, and immediate availability are one of the most important factors when selecting a GPU provider. Customers want the most comprehensive feature sets at the lowest price with the best terms.<br>定价、消费模型和即时可用性是选择 GPU 供应商时最重要的因素之一。客户希望以最低的价格和最佳的条款获得最全面的功能集。</p><p>GPU Compute prices are expressed in USD per hour per GPU – based on a typical on-demand price of USD 2.99&#x2F;hr&#x2F;GPU for an H100 SXM with 8x400G InfiniBand, renting one server of 8 GPUs would cost USD 23.92 per hour and $574.08 per day.<br>GPU 计算价格以每小时每个 GPU 的美元计价——基于 H100 SXM 的典型按需价格为每小时每个 GPU 2.99 美元，租用一台 8 个 GPU 的服务器每小时将花费 23.92 美元，每天 574.08 美元。</p><p>For most Neoclouds, this price is all-inclusive and bundles in the on-board CPU, networking, power usage, local NVMe storage, as well as having Slurm and drivers properly set up. Customers typically take up dedicated network storage used for training, checkpointing, or managing training and inference data, as well as object storage. Storage generally is charged separately, with prices of around 6-9c per GB&#x2F;month for high-performance network storage and 2-3c per GB&#x2F;month for Object storage. Internet connectivity and data ingress and egress are typically not chargeable.<br>对于大多数 Neocloud，价格是全包的，包括板载 CPU、网络、功耗、本地 NVMe 存储，以及正确设置的 Slurm 和驱动程序。客户通常会使用专用网络存储来进行训练、检查点或管理训练和推理数据，以及对象存储。存储通常是单独收费的，高性能网络存储的价格约为每 GB 每月 6-9 美分，对象存储的价格约为每 GB 每月 2-3 美分。互联网连接和数据进出通常不收费。</p><p>There are a few different options when subscribing for GPU Compute:<br>订阅 GPU 计算时有几种不同的选项：</p><ul><li><strong>On-demand</strong>: The GPU compute buyer pays based on the actual time the GPU instance&#x2F;server is used, with the price of compute subject to adjustment. This affords the most flexibility and is most often used for development, burst inferencing, or hobbyist work. However, it typically has the highest price out of the three main options. The current best on-demand pricing is $2.99 USD per hour per GPU.<br>  按需：GPU 计算买家根据实际使用 GPU 实例&#x2F;服务器的时间支付费用，计算价格可进行调整。这提供了最大的灵活性，通常用于开发、突发推理或爱好者工作。然而，它通常是三种主要选项中价格最高的。目前最佳的按需定价为每小时每个 GPU 2.99 美元。</li><li><strong>Spot</strong>: Also known as pre-emptive, like on-demand, usage is charged by actual time on the instance&#x2F;server, but usage can be interrupted at any time to make way for other workloads or users. This is best suited for jobs that do not require real-time processing, although the smooth resumption of jobs remains a developing capability. Spot instances are most suitable for inference workloads or batch jobs that can be interrupted with one minute’s notice. Nobody uses spot instances for training, as being randomly kicked off a multi-node instance is highly disruptive. Spot pricing gives Neoclouds flexibility to quickly free up capacity for more important customers or more lucrative workloads. Spot pricing can be lower than on-demand – we have seen quotes in the $2.00 and even $1.00 to $2.00 range.<br>  现货：也称为预 emptive，像按需一样，使用费用按实例&#x2F;服务器的实际时间收费，但使用可以在任何时候被中断，以便为其他工作负载或用户腾出空间。这最适合不需要实时处理的工作，尽管工作的平稳恢复仍然是一个正在发展的能力。现货实例最适合可以在一分钟通知内中断的推理工作负载或批处理作业。没有人会使用现货实例进行训练，因为被随机踢出多节点实例会造成很大的干扰。现货定价使 Neoclouds 能够灵活地迅速释放容量，以便为更重要的客户或更有利可图的工作负载腾出空间。现货定价可能低于按需定价——我们看到的报价在 2.00 美元甚至 1.00 美元到 2.00 美元的范围内。</li><li><strong>Contract&#x2F;Reserved</strong>: The compute price is locked in for a given time, and usage cannot be interrupted. Ordinary contract tenors include one month, 6 months, one year, 18 months, 2 years, 3 years. Due to the wide availability of H100&#x2F;H200 across 100 of GPU clouds, most customers do not sign 1-3 years deals anymore<br>  合同&#x2F;保留：计算价格在给定时间内锁定，使用不能中断。普通合同期限包括一个月、六个月、一年、十八个月、两年、三年。由于 H100&#x2F;H200 在 100 个 GPU 云中的广泛可用性，大多数客户不再签订 1-3 年的合同。</li></ul><p>Pre-emptible or on-demand cluster options offer more flexibility, making them ideal for intermittent or elastic workloads. A noteworthy strategy emerging among some Neocloud providers involves selling idle compute capacity at discounted rates but with specific clauses permitting providers to reclaim resources with short notice (typically seven days) should higher-paying customers emerge.<br>可抢占或按需集群选项提供了更多灵活性，使其非常适合间歇性或弹性工作负载。一些 Neocloud 提供商中出现的一种值得注意的策略是以折扣价出售闲置计算能力，但附带特定条款，允许提供商在高价客户出现时以短期通知（通常为七天）收回资源。</p><p>Providers like Google Cloud Platform (GCP) and Amazon Web Services (AWS) offer scheduled capacity through flex modes or capacity blocks, allowing customers predictable access at defined intervals.<br>像谷歌云平台（GCP）和亚马逊网络服务（AWS）这样的提供商通过灵活模式或容量块提供定期容量，使客户能够在定义的时间间隔内获得可预测的访问。</p><p>From a <strong>GPU Provider’s perspective</strong>, it’s typically advantageous to secure long-term contracts, mainly because, as highlighted by NVIDIA’s “Chief Revenue Destroyer” Jensen Huang, GPU performance per dollar improves extremely rapidly each year, making early commitments favorable for the GPU cloud to lock in margins early on with long term contracts.<br>从 GPU 提供商的角度来看，确保长期合同通常是有利的，主要是因为正如 NVIDIA 的“首席收入破坏者”黄仁勋所强调的，GPU 每美元的性能每年都在极快地提高，这使得 GPU 云在早期承诺时能够通过长期合同锁定利润。</p><p>From the <strong>Customer’s perspective</strong>, it is typically advantageous to secure short-term contracts due to the reason that Mr. “Chief Revenue Destroyer” mentioned at his GTC Keynote – namely, the exponentially increasing GPU performance per dollar delivered each year that he launches a new GPU generation.<br>从客户的角度来看，通常获得短期合同是有利的，原因是“首席收入破坏者”在他的 GTC 主题演讲中提到的——即每年推出新一代 GPU 时，每美元所提供的 GPU 性能呈指数级增长。</p><p>Most customers are engaged in an AI arms race with their competitors, meaning that availability remains a critical differentiator. Customers frequently require immediate resource provisioning and want their clusters available not just today, but they practically want them available yesterday. Both Nebius and Crusoe excel in this domain, with extensive availability and the capability to provision substantial GPU clusters (such as 128 GPUs) from initial contact to contract signing and provisioning within remarkably short timelines—often less than two days.<br>大多数客户正与竞争对手进行 AI 军备竞赛，这意味着可用性仍然是一个关键的差异化因素。客户经常需要立即提供资源，并希望他们的集群不仅今天可用，实际上他们希望它们昨天就可用。Nebius 和 Crusoe 在这一领域表现出色，具备广泛的可用性和在初次接触到合同签署及资源提供之间在极短时间内（通常少于两天）提供大量 GPU 集群（如 128 个 GPU）的能力。</p><p>Nebius currently stands out by offering the best absolute pricing while still maintaining robust technical capabilities. Their approach involves aggressive cost optimization strategies, including the adoption of Original Design Manufacturer (ODM) hardware for GPU servers, significantly lowering their total cost of ownership. For example, by bypassing traditional OEM providers like Dell or Supermicro, which can price servers with as high as 10-15% gross margins, Nebius achieves cost reductions through a custom-designed ODM chassis, minimizing gross margins from a typical 10-15% down to around 2%.<br>Nebius 目前通过提供最佳的绝对定价，同时保持强大的技术能力而脱颖而出。他们的做法涉及激进的成本优化策略，包括采用原始设计制造商（ODM）硬件用于 GPU 服务器，显著降低了其总拥有成本。例如，通过绕过传统的 OEM 供应商如戴尔或超微，这些供应商的服务器毛利率高达 10-15%，Nebius 通过定制设计的 ODM 机箱实现了成本降低，将毛利率从典型的 10-15% 降至约 2%。</p><p>This strategy not only reduces initial hardware expenditures but also lowers <a href="https://www.youtube.com/watch?v=jPLbKjYAado">ongoing power consumption</a>, allowing Nebius to pass substantial savings onto customers. All Hyperscalers use this ODM strategy, too, but Nebius stands out as the only non-Hyperscaler that deploys an ODM-built chassis.<br>这一策略不仅减少了初始硬件支出，还降低了持续的电力消耗，使 Nebius 能够将可观的节省转嫁给客户。所有超大规模云服务商也使用这种 ODM 策略，但 Nebius 是唯一一个部署 ODM 机箱的非超大规模云服务商。</p><p>Oracle presents a distinct advantage in pricing for customers, prioritizing Hyperscaler capabilities and stringent security measures. Their pricing model reflects enterprise-grade security and comprehensive integration with other cloud services, aligning well with organizations that require deep ecosystem integration or compliance-focused workloads. Consequently, while not necessarily the cheapest absolute option, Oracle delivers exceptional price-to-value for enterprise customers.<br>甲骨文在定价方面为客户提供了独特的优势，优先考虑超大规模云服务能力和严格的安全措施。他们的定价模型反映了企业级安全性和与其他云服务的全面集成，适合需要深度生态系统集成或合规性工作负载的组织。因此，尽管不一定是绝对最便宜的选择，甲骨文为企业客户提供了卓越的性价比。</p><p>Nvidia has a program called “Nvidia Cloud Partner (NCP),” where GPU clouds that can meet certain requirements can achieve NCP status, and Nvidia helps them get sales and ensure they have lines of communication with technical folks to help advance their GPU cloud offering. We tend to see that GPU clouds with NCP status have better performance than those without it.<br>Nvidia 有一个名为“Nvidia Cloud Partner (NCP)”的项目，符合某些要求的 GPU 云可以获得 NCP 状态，Nvidia 会帮助他们获得销售并确保他们与技术人员保持沟通，以帮助推进他们的 GPU 云产品。我们发现，获得 NCP 状态的 GPU 云通常比没有 NCP 状态的表现更好。</p><p>Jensen has invested in the following GPU clouds:<br>Jensen 已投资于以下 GPU 云：</p><ul><li>Together AI 一起 AI</li><li>CoreWeave 核心织造</li><li>Nebius 内比乌斯</li><li>Crusoe 克鲁索</li><li>Lambda labs</li></ul><p>These five GPU clouds tend to have a good user experience overall and performance based on our testing. 4 out of the five have offerings that are at the ClusterMAX™ Platinum standard or ClusterMAX™ Gold standard.<br>根据我们的测试，这五个 GPU 云整体上用户体验良好，性能表现出色。五个中有四个的服务达到了 ClusterMAX™白金标准或 ClusterMAX™黄金标准。</p><p>In contrast, we tend to see that GPU clouds that AMD has invested in do not do well on user experience, and none of the clouds that AMD invested in even rank as ClusterMAX™ Platinum or ClusterMAX™ Gold or ClusterMAX™ Sliver.<br>相比之下，我们倾向于看到 AMD 投资的 GPU 云在用户体验方面表现不佳，且 AMD 投资的云中没有一个甚至排名为 ClusterMAX™白金或 ClusterMAX™黄金或 ClusterMAX™白银。</p><p>Some of the “AMD Alliance Instinct Cloud Partners” don’t even have basic security, such as SOC2. As such, we view being on the “AMD Alliance Instinct Cloud Partners” list as not a good predictor of tiering well in ClusterMAX™.<br>一些“AMD 联盟 Instinct 云合作伙伴”甚至没有基本的安全性，例如 SOC2。因此，我们认为在“AMD 联盟 Instinct 云合作伙伴”名单上的表现并不能很好地预测在 ClusterMAX™中的等级。</p><p>We have spoken with AMD, and they have confirmed that they are investigating the issue and committed to helping raise the industry standard on this topic.<br>我们已与 AMD 进行了沟通，他们确认正在调查此问题，并承诺帮助提高该领域的行业标准。</p><p>GPU clouds that pay for support from <a href="https://www.schedmd.com/">SchedMD</a> (the makers of Slurm) can significantly enhance the customer experience and services by providing robust and efficient management of GPU resources. By leveraging SchedMD’s expertise, GPU cloud providers can offer users a seamless and optimized experience, ensuring that computational tasks are handled efficiently and effectively.<br>支付 SchedMD（Slurm 的开发者）支持费用的 GPU 云可以通过提供强大而高效的 GPU 资源管理显著提升客户体验和服务。通过利用 SchedMD 的专业知识，GPU 云服务提供商可以为用户提供无缝且优化的体验，确保计算任务高效且有效地处理。</p><p>We recommend to AMD that AMD ensures that all their “Alliance Instinct Cloud Partners” achieve SOC2 and ensure that any new “Cloud Partner” have SOC2 security before joining. We recommend to Nvidia and AMD that they need to set the industry-wide bar and help even their non-partner cloud get SOC2 security. Getting SOC2 Security should not be optional for GPU cloud, but instead, it is a must-have for GPU clouds. It is just like FAA certifications on airlines. You might want to fly on an airline that doesn’t have FAA certification, but most won’t. As most model weights and codebases are proprietary intellectual property (IP) and not open source and are worth tens of thousands to millions of dollars, most customers want basic security, such as SOC2.<br>我们建议 AMD 确保所有“联盟本能云合作伙伴”都获得 SOC2 认证，并确保任何新的“云合作伙伴”在加入之前具备 SOC2 安全性。我们建议 Nvidia 和 AMD 需要设定行业标准，并帮助他们的非合作伙伴云获得 SOC2 安全性。获得 SOC2 安全性对于 GPU 云来说不应是可选的，而应是必备的。这就像航空公司需要获得 FAA 认证一样。您可能想乘坐没有 FAA 认证的航空公司，但大多数人不会。由于大多数模型权重和代码库是专有知识产权（IP），而不是开源的，价值从数万美元到数百万美元不等，因此大多数客户希望获得基本的安全性，例如 SOC2。</p><p>Furthermore, we recommend that AMD provide <a href="https://github.com/NVIDIA/pyxis">pyxis container Slurm support</a> such that running containers on Slurm has a good UX. Currently, it is challenging to run containers on Slurm, and even AMD’s own internal scripts are a mess due to this missing Pyxis capability. Verus on a correctly setup Nvidia GPU cloud, running containers with Pyxis on Slurm is an effortless experience.<br>此外，我们建议 AMD 提供 Pyxis 容器的 Slurm 支持，以便在 Slurm 上运行容器时有良好的用户体验。目前，在 Slurm 上运行容器具有挑战性，甚至 AMD 自己内部的脚本由于缺少 Pyxis 功能而变得混乱。在正确设置的 Nvidia GPU 云上，使用 Pyxis 在 Slurm 上运行容器是一种轻松的体验。</p><p>For NVIDIA, we recommend that they should provide good publicly accessible documentation and education about the different keys (SMKeys, MKeys, PKeys, VSKeys, CCKeys, AMKeys, etc) needed to secure an InfiniBand network properly. We recommend that Nvidia help their GPU clouds properly secure their InfiniBand network and complete an audit of all GPU clouds that use InfiniBand.<br>对于 NVIDIA，我们建议他们提供良好的公开可访问文档和教育，关于安全地正确配置 InfiniBand 网络所需的不同密钥（SMKeys、MKeys、PKeys、VSKeys、CCKeys、AMKeys 等）。我们建议 Nvidia 帮助他们的 GPU 云正确保护其 InfiniBand 网络，并对所有使用 InfiniBand 的 GPU 云进行审计。</p><p>Furthermore, we recommend that Nvidia should fix ease of use for SHARP for the GPU provider and recommend that Nvidia should enable it by default instead of making it challenging for the GPU provider to set it up and for the end user to see the real-world benefits of SHARP on their training and inference workloads.<br>此外，我们建议 Nvidia 应改善 SHARP 的易用性，以便于 GPU 提供商，并建议 Nvidia 应默认启用该功能，而不是让 GPU 提供商设置起来困难，以及让最终用户在其训练和推理工作负载中看到 SHARP 的实际好处。</p><p>We recommend that Nvidia runs regression tests on GCP networking, AWS networking and Oracle networking to prevent regressions on their Hyperscalers partners’ NCCL performance every time they release a new NCCL version. For example: on Oracle, since NCCL 2.21.5, there have been performance regressions on certain clouds when they attempted to deploy any of the subsequent NCCL versions up to 2.26.<br>我们建议 Nvidia 在 GCP 网络、AWS 网络和 Oracle 网络上进行回归测试，以防止每次发布新版本的 NCCL 时对其超大规模合作伙伴的 NCCL 性能造成回归。例如：在 Oracle 上，自 NCCL 2.21.5 以来，当他们尝试部署任何后续 NCCL 版本（直到 2.26）时，某些云的性能出现了回归。</p><h2 id="ClusterMAX™-Platinum-GPU-ProvidersClusterMAX™-Platinum-GPU-提供商"><a href="#ClusterMAX™-Platinum-GPU-ProvidersClusterMAX™-Platinum-GPU-提供商" class="headerlink" title="ClusterMAX™ Platinum GPU ProvidersClusterMAX™ Platinum GPU 提供商"></a>ClusterMAX™ Platinum GPU ProvidersClusterMAX™ Platinum GPU 提供商</h2><p>The <strong>ClusterMAX™ Platinum</strong> tier represents the highest standard of GPU cloud services available in the industry. Providers in this category consistently excel across all critical evaluation criteria, including adopting robust security measures, competitive pricing for the value they offer, extensive technical expertise, outstanding reliability with clearly defined SLAs, seamless managed Slurm&#x2F;Kubernetes offering, and superior NCCL&#x2F;RCCL networking performance. Platinum-tier providers are proactive, innovative, and maintain an active feedback loop with the community to continually raise the industry bar, setting benchmarks for excellence. Currently, there is only one GPU cloud that is raising the industry bar and qualifies for ClusterMAX™ Platinum which is CoreWeave**.**<br>ClusterMAX™ Platinum 级别代表了行业内可用的最高标准的 GPU 云服务。该类别的提供商在所有关键评估标准上始终表现出色，包括采用强大的安全措施、提供具有竞争力的定价、广泛的技术专长、出色的可靠性（具有明确的服务水平协议）、无缝的管理 Slurm&#x2F;Kubernetes 提供以及卓越的 NCCL&#x2F;RCCL 网络性能。Platinum 级别的提供商积极主动、创新，并与社区保持活跃的反馈循环，以不断提高行业标准，设定卓越的基准。目前，只有一个 GPU 云正在提升行业标准并符合 ClusterMAX™ Platinum 资格，那就是 CoreWeave。</p><h2 id="CoreWeave-核心织造"><a href="#CoreWeave-核心织造" class="headerlink" title="CoreWeave 核心织造"></a>CoreWeave 核心织造</h2><p>CoreWeave is clearly leading in providing the best GPU cloud experience and has very high goodput and are entrusted to manage the large-scale GPU infrastructure for AGI labs like OpenAI and MetaAI, high frequency trading firms like Jane Street, and even NVIDIA’s internal clusters. CoreWeave is the experts at running large scale GPU clusters reliably.<br>CoreWeave 显然在提供最佳 GPU 云体验方面处于领先地位，具有非常高的良品率，并被信任管理大型 GPU 基础设施，为像 OpenAI 和 MetaAI 这样的 AGI 实验室、高频交易公司如 Jane Street，甚至 NVIDIA 的内部集群提供服务。CoreWeave 是可靠运行大规模 GPU 集群的专家。</p><p>CoreWeave provides 4 offerings:<br>CoreWeave 提供 4 种服务：</p><ul><li>CoreWeave Bare Metal without any managed scheduler<br>  CoreWeave 裸金属，无需任何管理调度程序</li><li>CoreWeave Managed SUNK (Slurm in Kubernetes)<br>  CoreWeave 管理的 SUNK（Kubernetes 中的 Slurm）</li><li>CoreWeave Managed Slurm CoreWeave 管理的 Slurm</li><li>CoreWeave Managed Kubernetes<br>  CoreWeave 管理的 Kubernetes</li></ul><p>For their bare metal offering, which doesn’t include any CoreWeave-managed software or scheduler, there are essentially only two customers: OpenAI&#x2F;Azure and Nvidia EOS. OpenAI&#x2F;Azure wants bare metal due to their security and operational paranoia surrounding artificial general intelligence (AGI), as well as the ability to control the cluster more tightly for reliability and performance reasons. For the CoreWeave 11,000 H100 EOS cluster, which Nvidia rents and uses for internal development, it is also bare metal as it is useful for Nvidia to manage their own Slurm and Kubernetes while they develop their Slurm and Kubernetes operators and plugins.<br>对于他们的裸金属产品，不包括任何 CoreWeave 管理的软件或调度程序，实际上只有两个客户：OpenAI&#x2F;Azure 和 Nvidia EOS。OpenAI&#x2F;Azure 希望使用裸金属是因为他们对人工通用智能（AGI）周围的安全性和操作偏执，以及为了可靠性和性能原因更紧密地控制集群的能力。对于 Nvidia 租用并用于内部开发的 CoreWeave 11,000 H100 EOS 集群，它也是裸金属，因为 Nvidia 在开发他们的 Slurm 和 Kubernetes 操作员及插件时，管理自己的 Slurm 和 Kubernetes 是有用的。</p><p>Although the rest of their customers have the option not to use the CoreWeave managed offering, all their customers decide to opt for it due to CoreWeave’s amazingly managed offering.<br>尽管其余客户可以选择不使用 CoreWeave 管理的产品，但所有客户都决定选择它，因为 CoreWeave 的管理产品非常出色。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-17.-Coreweave-customers-GIMP.png?resize=1024,563&ssl=1"></p><p>Source: CoreWeave 来源：CoreWeave</p><p>Firstly, we will talk about CoreWeave’s automated node lifecycle controller that ensure that during cluster bring up nodes receive a full burn-in test and full cluster InfiniBand network high-temperature burn in with NCCL-tests and ib_write_bw. Not only does this bring up burn-in test for hard failures, but they also compare it to reference numbers and see when nodes do not meet the performance expectations or are running into silence data corruption (SDC) issues. Nodes that do not meet this comprehensive test will be automatically drained for investigation and will not proceed to the customer cluster till it is fully resolved.<br>首先，我们将讨论 CoreWeave 的自动节点生命周期控制器，该控制器确保在集群启动期间，节点接收全面的烧机测试和完整的集群 InfiniBand 网络高温烧机测试，使用 NCCL 测试和 ib_write_bw。这不仅为硬件故障提供了烧机测试，还将其与参考数字进行比较，以查看节点何时未达到性能预期或遇到静默数据损坏（SDC）问题。未通过此综合测试的节点将被自动排空以进行调查，并且在完全解决之前不会进入客户集群。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-18.-Coreweave-lifecycle-GIMP.png?resize=1024,454&ssl=1"></p><p>Source: CoreWeave 来源：CoreWeave</p><p>Once deployed in a customer environment, they will continuously do passive health checks every couple of seconds to ensure the GPUs are functioning properly. By doing this, it leads to high goodput and automatically drains and fixes unhealthy nodes. For passive health checks, they monitor for:<br>一旦在客户环境中部署，它们将每隔几秒钟持续进行被动健康检查，以确保 GPU 正常运行。通过这样做，可以实现高吞吐量，并自动排除和修复不健康的节点。对于被动健康检查，它们监控：</p><ul><li>GPUs falling off the bus<br>  GPU 掉线</li><li>PCIe Errors PCIe 错误</li><li>Ethernet and InfiniBand events such as Link Flaps<br>  以太网和 InfiniBand 事件，例如链路波动</li><li>Thermals such as GPU temperate<br>  热量，例如 GPU 温度</li><li>GPU and CPU Memory stats such as ECC error rate<br>  GPU 和 CPU 内存统计信息，例如 ECC 错误率</li><li>Nvidia XID and Nvidia SXID error codes<br>  Nvidia XID 和 Nvidia SXID 错误代码</li><li>Etc.等等。</li></ul><p>In addition to passive health checks, they automatically schedule on a weekly basis active health check to run on idle GPUs to do a full set of active testing to verify nodes are healthy. These tests include:<br>除了被动健康检查外，它们还会每周自动安排在空闲的 GPU 上运行主动健康检查，以进行全面的主动测试以验证节点是否健康。这些测试包括：</p><ul><li>NVIDIA DCGM diag level 3 with Extensive Testing (EUD)<br>  NVIDIA DCGM 诊断级别 3 与广泛测试 (EUD)<br>  DtoH and HtoD Bandwdith for validating PCIe performance from CPU to GPU<br>  从 CPU 到 GPU 验证 PCIe 性能的 DtoH 和 HtoD 带宽</li><li>Local NCCL all reduce tests for validating NVLink&#x2F;NVSwitch&#x2F;NVLS performance<br>  本地 NCCL 全部归约测试用于验证 NVLink&#x2F;NVSwitch&#x2F;NVLS 性能</li><li>Local InfiniBand all reduce test for validating InfiniBand performance and links (by force disabling NVLink&#x2F;p2p&#x2F;SHM)<br>  本地 InfiniBand 全规约测试，用于验证 InfiniBand 性能和链接（通过强制禁用 NVLink&#x2F;p2p&#x2F;SHM）</li><li>Pairwise GPU ib_write_bw and ib_write_latency bidirectional tests to verify that the network is within specs with reference numbers.<br>  成对 GPU ib_write_bw 和 ib_write_latency 双向测试，以验证网络是否符合规格及参考编号。</li><li>Pairwise CPU ib_write_bw and ib_write_latency bidirectional tests to verify that the network is within specs with reference numbers.<br>  成对的 CPU ib_write_bw 和 ib_write_latency 双向测试，以验证网络是否符合规格参考编号。</li><li>GPUBurn for validating GPU won’t fail under load<br>  GPUBurn 用于验证 GPU 在负载下不会失败。</li><li>Nvidia TinyMeg2 for validating hardware correctness and that GPU are free from SDC<br>  Nvidia TinyMeg2 用于验证硬件正确性以及确保 GPU 不受 SDC 影响</li><li>Megatron Tests to test if TFLOP&#x2F;s&#x2F;GPU performance match reference numbers and that the loss convergence matches reference loss curve<br>  Megatron 测试用于测试 TFLOP&#x2F;s&#x2F;GPU 性能是否与参考数字匹配，以及损失收敛是否与参考损失曲线匹配</li></ul><p>By automatically scheduling these tests during cluster bring up and continuously during the whole lifecycle of the customer cluster, they are able to ensure that the customer has high goodput by proactively removing unhealthy nodes to prevent customers from submitting jobs from unhealth nodes. Customers can see in the dashboard when was the last time their nodes have had an active health check ran called “verification”.<br>通过在集群启动期间以及在客户集群的整个生命周期中持续自动调度这些测试，他们能够确保客户拥有高吞吐量，通过主动移除不健康的节点来防止客户从不健康的节点提交作业。客户可以在仪表板上看到他们的节点上一次进行主动健康检查的时间，称为“验证”。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-19.-Node-Health-GIMP.png?resize=1024,392&ssl=1"></p><p>Source: SemiAnalysis 来源：SemiAnalysis</p><p>On their main dashboard, they also show events such as the classic common error of “GPUFallingoffthebus” and “LinkFlaps”. At the infra layer without any changes to application&#x2F;end user code, they use “DCGM_FI_PROF_PIPE_TENSOR_ACTIVE * 1979” to track the current fp8 TFLOP&#x2F;s (times 989 to bf16 TFLOP&#x2F;s estimates) rough estimate and has a system to correlate which alerts causes a drop in cluster or job wide TFLOP&#x2F;s. for example, you can clearly see that the drop in jobwide TFLOP&#x2F;s is caused by PCIeFault and IBLink flapping Fault. While using DCGM_FI_PROF_PIPE_TENSOR_ACTIVE isn’t the most accurate estimate of MFU, it does allow the customer and CoreWeave to view which events are relevant to the drop in MFU. In addition, the CoreWeave MFU infra layer estimates, customers can also calculate their MFU and TFLOP&#x2F;s&#x2F;GPU at their application layer for a more accurate absolute TFLOP&#x2F;s&#x2F;GPU.<br>在他们的主仪表板上，他们还显示了事件，例如经典的常见错误“GPUFallingoffthebus”和“LinkFlaps”。在基础设施层中，在不对应用程序&#x2F;最终用户代码进行任何更改的情况下，他们使用“DCGM_FI_PROF_PIPE_TENSOR_ACTIVE * 1979”来跟踪当前的 fp8 TFLOP&#x2F;s（乘以 989 得到 bf16 TFLOP&#x2F;s 估算值）的粗略估算，并且有一个系统来关联哪些警报导致集群或作业范围内的 TFLOP&#x2F;s 下降。例如，您可以清楚地看到作业范围内的 TFLOP&#x2F;s 下降是由 PCIeFault 和 IBLink 颤动故障引起的。虽然使用 DCGM_FI_PROF_PIPE_TENSOR_ACTIVE 不是 MFU 的最准确估算，但它确实允许客户和 CoreWeave 查看哪些事件与 MFU 下降相关。此外，除了 CoreWeave MFU 基础设施层的估算外，客户还可以在其应用层计算 MFU 和 TFLOP&#x2F;s&#x2F;GPU，以获得更准确的绝对 TFLOP&#x2F;s&#x2F;GPU。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-20.-Observability-Single-pane-of-glass-GIMP.png?resize=1024,559&ssl=1"></p><p>Source: CoreWeave 来源：CoreWeave</p><p>CoreWeave has amazing out of the box dashboards to track InfiniBand and NVLink bandwidth and a whole host of other stats such as temperature and makes them all visible to the end user to help debug. As some are aware, the cold aisle temps change during the day and night and these temp changes may cause 2-3% difference in performance, CoreWeave provides the end user full visibility into the temp sensors of each node they are on.<br>CoreWeave 提供了出色的开箱即用仪表板，以跟踪 InfiniBand 和 NVLink 带宽以及其他一系列统计数据，如温度，并使所有这些信息对最终用户可见，以帮助调试。正如一些人所知，冷通道的温度在白天和夜间会发生变化，这些温度变化可能导致 2-3% 的性能差异，CoreWeave 为最终用户提供了对他们所处每个节点的温度传感器的全面可见性。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-21.-Infiniband-GIMP.png?resize=1024,265&ssl=1"></p><p>Source: CoreWeave, SemiAnalysis<br>来源：CoreWeave，SemiAnalysis</p><p>All of these active and passive metrics are collected to detect outliers, something similar to drawing a “curve of best fit” and finding the nodes and points that don’t fit into a certain range of the median.<br>所有这些主动和被动指标都是为了检测异常值而收集的，类似于绘制“最佳拟合曲线”，并找到不符合某个中位数范围的节点和点。</p><p>Furthermore, CoreWeave has improved their customer’s visibility into reliability and the state of each node by having a node controller overview that shows the state of each node from a healthy state to being triaged to being debugged or RMA’ed back to the OEM.<br>此外，CoreWeave 通过提供节点控制器概览，改善了客户对可靠性和每个节点状态的可见性，该概览显示了每个节点的状态，从健康状态到被分类处理，再到被调试或退回给 OEM。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-22.-Node-controller-GIMP.png?resize=1024,635&ssl=1"></p><p>Source: CoreWeave 来源：CoreWeave</p><p>What we find amazing about CoreWeave is that they provide an automated, managed solution that abstracts away all the tasks that an ML engineer or scientist ideally doesn’t want to do.<br>我们对 CoreWeave 感到惊讶的是，他们提供了一种自动化的、托管的解决方案，抽象掉了机器学习工程师或科学家理想上不想做的所有任务。</p><p>The monitoring, passive and automated schedule active health checks and out-of-the-box managed scheduler, all of this loop back into what an ML engineer&#x2F;scientist wants, which is to focus on non-infra tasks and have a healthy set of verified nodes that is constantly scanned by passive health checks and automatically weekly scanned by active health checks. But an ML engineer&#x2F;scientist also recognizes that sometimes things may break and broken nodes are not caught by the health checks and in those cases, they want FULL visibility into everything that is going on.<br>监控、被动和自动调度的主动健康检查以及开箱即用的托管调度器，这一切都回归到机器学习工程师&#x2F;科学家所希望的，即专注于非基础设施任务，并拥有一组健康的经过验证的节点，这些节点会不断通过被动健康检查进行扫描，并每周自动进行主动健康检查。但机器学习工程师&#x2F;科学家也意识到，有时事情可能会出现故障，而故障节点不会被健康检查捕捉到，在这种情况下，他们希望对正在发生的一切有完全的可见性。</p><p>Their out-of-the-box Slurm solution comes with a <a href="https://github.com/NVIDIA/pyxis">pyxis plugin</a> for running reproducible containers as a first-class inside of Slurm and comes with <a href="https://slurm.schedmd.com/topology.conf.html">automatically generated Slurm topology</a> to ensure optimized NCCL collectives. CoreWeave also has <a href="https://github.com/coreweave/nccl-tests/tree/master">open sourced their nccl-tests scripts</a> for reproducibility. Besides Azure, CoreWeave is one of the only providers of InfiniBand SHARP in network reduction-enabled solutions. It has also worked with Nvidia and some of its customers to enable SHARP, optimizing their customers’ workloads and NCCL performance.<br>他们的开箱即用的 Slurm 解决方案配备了一个 pyxis 插件，用于在 Slurm 内部作为一流运行可重现的容器，并附带自动生成的 Slurm 拓扑，以确保优化的 NCCL 集合。CoreWeave 还开源了他们的 nccl-tests 脚本以实现可重现性。除了 Azure，CoreWeave 是网络减缩启用解决方案中为数不多的 InfiniBand SHARP 提供商之一。它还与 Nvidia 及其一些客户合作，以启用 SHARP，优化客户的工作负载和 NCCL 性能。</p><p>These are all reasons why companies like Meta or Jane Street choose to use CoreWeave-managed Slurm&#x2F;Kubernetes.<br>这些都是像 Meta 或 Jane Street 这样的公司选择使用 CoreWeave 管理的 Slurm&#x2F;Kubernetes 的原因。</p><p>CoreWeave also offers a Slurm in Kubernetes (SUNK) solution where they run Slurm inside of Kubernetes. This allows customers to schedule their Slurm training with their Kubernetes inference services dynamically. This solution has essentially zero downsides, except for one.<br>CoreWeave 还提供了一个在 Kubernetes 中运行 Slurm 的解决方案（SUNK），使他们能够在 Kubernetes 内部运行 Slurm。这使得客户能够动态地安排他们的 Slurm 训练与 Kubernetes 推理服务。这种解决方案几乎没有任何缺点，除了一个。</p><p>The one downside is that changing GPU vboost settings requires the Slurm container setting to be privileged but this is very simple fix to enable in the yaml. Some people may think that SUNK means there is vendor lockup, but this is not true. If customers want to move away from CoreWeave, they can take their batch scripts and Kubernetes yaml files to other providers since Kubernetes yaml and batch scripts are open standards and work on any Slurm solution are properly set it. The reason customers continue to stick with and renew their contracts is CoreWeave’s technical expertise and their amazing node lifecycle controller and health checks.<br>唯一的缺点是更改 GPU vboost 设置需要将 Slurm 容器设置为特权，但在 yaml 中启用这一点非常简单。有些人可能认为 SUNK 意味着存在供应商锁定，但这并不真实。如果客户想要离开 CoreWeave，他们可以将自己的批处理脚本和 Kubernetes yaml 文件带到其他提供商，因为 Kubernetes yaml 和批处理脚本是开放标准，并且在任何适当设置的 Slurm 解决方案上都能工作。客户继续坚持并续签合同的原因是 CoreWeave 的技术专长以及他们出色的节点生命周期控制器和健康检查。</p><p>From our independent testing of CoreWeave H100 clusters, we noticed that the team of engineers and solution architects are the subject matter experts in GPU infrastructure and NCCL. Their onboarding experience was smooth, and they provided an onboarding doc that shows all the IP addresses and certain common FAQ. It becomes very clear to us that CoreWeave is the technical subject matter expert when we ask in-depth questions about certain PCIe AER health checks and about specific InfiniBand security keys, such as the difference between SMKeys and MKeys.<br>根据我们对 CoreWeave H100 集群的独立测试，我们注意到工程师和解决方案架构师团队在 GPU 基础设施和 NCCL 方面是主题专家。他们的入职体验非常顺利，并提供了一份入职文档，显示了所有的 IP 地址和一些常见的 FAQ。当我们询问有关某些 PCIe AER 健康检查和特定 InfiniBand 安全密钥（例如 SMKeys 和 MKeys 之间的区别）时，CoreWeave 显然是技术主题专家。</p><p>One thing that is missing from self-serve deployment, and CoreWeave’s deployment, is a bunch of complex tops, yaml files, and Kubernetes, which is typically not in the vocabulary of an ML Scientist who want to train in Slurm, but luckily, CoreWeave tasks an engineer to any customer that wants CoreWeave to do the deployment themselves. We recommend that CoreWeave work on a UI console flow for deploying their managed Slurm solution, ideally with less than four button clicks.<br>自助部署和 CoreWeave 的部署中缺少的一件事是大量复杂的顶部、yaml 文件和 Kubernetes，这通常不在希望在 Slurm 中进行训练的 ML 科学家的词汇中，但幸运的是，CoreWeave 为任何希望 CoreWeave 自行进行部署的客户指派了一名工程师。我们建议 CoreWeave 开发一个 UI 控制台流程，以便部署他们的托管 Slurm 解决方案，理想情况下不超过四次按钮点击。</p><p>CoreWeave clusters commonly submit <a href="https://www.coreweave.com/blog/mlperf-coreweave-nvidia-record-breaking-cloud-native-ai-supercomputer">very competitive MLPerf training results.</a> Furthermore, all the MLPerf Training results that <a href="https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/">NVIDIA submits</a> are obtained on the CoreWeave 11,000 H100 EOS cluster, which NVIDIA rents from CoreWeave. CoreWeave runs many clusters, many of which have over 10,000 GPUs.  Due to their close partnership with NVIDIA, they get access to early tranches of the next GPU allocation, and as we mentioned in the Neocloud Anatomy, the players that deploy first for each GPU cycle can lock in long-term low-risk contracts from favorable customers.<br>CoreWeave 集群通常提交非常有竞争力的 MLPerf 训练结果。此外，NVIDIA 提交的所有 MLPerf 训练结果都是在 CoreWeave 11,000 H100 EOS 集群上获得的，该集群是 NVIDIA 从 CoreWeave 租用的。CoreWeave 运行许多集群，其中许多集群拥有超过 10,000 个 GPU。由于与 NVIDIA 的紧密合作关系，他们可以提前获得下一轮 GPU 配额的早期份额，正如我们在 Neocloud Anatomy 中提到的，首先部署每个 GPU 周期的参与者可以从有利的客户那里锁定长期低风险合同。</p><p>CoreWeave is the only Neocloud capable of operating clusters with 10,000+ GPUs consistently and reliably. Besides CoreWeave, the only other GPU clouds that are able to operate them reliability are the four Hyperscalers: Azure, OCI, AWS and GCP.<br>CoreWeave 是唯一能够持续可靠地操作 10,000+ GPU 集群的 Neocloud。除了 CoreWeave，唯一能够可靠地操作这些集群的其他 GPU 云是四大超大规模云服务商：Azure、OCI、AWS 和 GCP。</p><p>From the customer perspective, a downside of CoreWeave is that they rarely accept short-term rentals, and most of their rentals are giant clusters for long-term tenants. This is different from Nebius and Crusoe, which offer competitive terms for short-term rentals of GPUs.<br>从客户的角度来看，CoreWeave 的一个缺点是他们很少接受短期租赁，而且他们的大多数租赁都是针对长期租户的巨型集群。这与 Nebius 和 Crusoe 不同，后者为 GPU 的短期租赁提供了有竞争力的条款。</p><h2 id="ClusterMAX™-Gold-Tier-GPU-ProvidersClusterMAX™金级-GPU-供应商"><a href="#ClusterMAX™-Gold-Tier-GPU-ProvidersClusterMAX™金级-GPU-供应商" class="headerlink" title="ClusterMAX™ Gold Tier GPU ProvidersClusterMAX™金级 GPU 供应商"></a>ClusterMAX™ Gold Tier GPU ProvidersClusterMAX™金级 GPU 供应商</h2><p><strong>ClusterMAX™ Gold</strong> tier providers deliver strong performance across most key evaluation categories, with some opportunities for improvement. They offer solid security practices, reliable infrastructure, competitive pricing models, and competent technical support. Although Gold-tier GPU clouds may have occasional gaps or inconsistencies in specific features like advanced active health checks, they generally demonstrate responsiveness to feedback and a clear commitment to continuous improvement, positioning themselves as excellent choices for GPU renters for maximizing goodput. To move from Gold to Platinum, they must have a demonstrated history of raising the industry bar such as CoreWeave.<br>ClusterMAX™ 金牌级提供商在大多数关键评估类别中表现出色，但仍有一些改进的机会。他们提供稳健的安全实践、可靠的基础设施、具有竞争力的定价模型和称职的技术支持。尽管金牌级 GPU 云在某些特性（如高级主动健康检查）上可能存在偶尔的差距或不一致，但他们通常对反馈表现出响应性，并明确致力于持续改进，使他们成为 GPU 租赁者最大化良好产出的优秀选择。要从金牌级提升到铂金级，他们必须有提升行业标准的历史，例如 CoreWeave。</p><h2 id="Crusoe-克鲁索"><a href="#Crusoe-克鲁索" class="headerlink" title="Crusoe 克鲁索"></a>Crusoe 克鲁索</h2><p>We’ve been using Crusoe Cloud for the past seven months and have been consistently impressed by their offering. Their console UI is straightforward to navigate and user-friendly, which has significantly simplified resource management and deployment. The intuitive experience they offer in the dashboard sets a high standard in the GPU cloud market, particularly in terms of ease of use and accessibility.<br>我们已经使用 Crusoe Cloud 七个月了，对他们的服务一直印象深刻。他们的控制台用户界面简单易用，用户友好，这大大简化了资源管理和部署。他们在仪表板上提供的直观体验在 GPU 云市场中树立了高标准，特别是在易用性和可访问性方面。</p><p>We had a very positive experience dealing with GPU bus errors on Crusoe servers. When we discovered the GPU bus error, Crusoe sent us an email to resolve the issue. In the email, Crusoe explained that it had automatically detected a GPU-fell-off-the-bus error, reserved a health spare node, and requested that we restart the node in the console to complete the migration. Crusoe automatically identified issues, proactively fixed them, and guided users on migration. This robust fault management improves user experience.<br>我们在处理 Crusoe 服务器上的 GPU 总线错误时有非常积极的体验。当我们发现 GPU 总线错误时，Crusoe 给我们发了一封电子邮件以解决该问题。在邮件中，Crusoe 解释说它自动检测到了 GPU 掉线错误，保留了一个健康的备用节点，并请求我们在控制台中重启该节点以完成迁移。Crusoe 自动识别问题，主动修复它们，并指导用户进行迁移。这种强大的故障管理改善了用户体验。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-22.1-Crusoe-1-GIMP.png?resize=1024,444&ssl=1"></p><p>Source: Crusoe, SemiAnalysis<br>来源：Crusoe，SemiAnalysis</p><p>Initially, Crusoe lacked a fully managed Slurm solution, requiring customers to set up Slurm clusters through Terraform scripts manually. However, they offset this complexity with an exceptional white-glove service experience, where Crusoe engineers personally handled Slurm setup for most customers, ensuring smooth deployment and minimal friction. Last week at GTC, Crusoe recently addressed this gap by announcing their fully managed Slurm offering called “ <a href="https://static.rainfocus.com/nvidia/gtcs25/sess/1736564473769001z9Hl/FinalPresPDF/S74475_1743005927914001c0TT.pdf">Auto Clusters</a>,” unveiled at GTC last week. This new service promises to simplify customer workflows further and remove previous manual deployment complexities. Their new “Auto Clusters” product will also come with <a href="https://slurm.schedmd.com/topology.conf.html">automatically generated Slurm topology</a> to ensure optimized NCCL collectives and auto node replacement when detecting unhealthy nodes.<br>最初，Crusoe 缺乏一个完全托管的 Slurm 解决方案，要求客户通过 Terraform 脚本手动设置 Slurm 集群。然而，他们通过卓越的白手套服务体验来抵消这种复杂性，Crusoe 工程师亲自为大多数客户处理 Slurm 设置，确保顺利部署和最小摩擦。上周在 GTC 上，Crusoe 最近通过宣布他们的完全托管 Slurm 产品“Auto Clusters”来解决这一差距，该产品在上周的 GTC 上首次亮相。这个新服务承诺进一步简化客户工作流程，并消除之前手动部署的复杂性。他们的新“Auto Clusters”产品还将配备自动生成的 Slurm 拓扑，以确保优化的 NCCL 集合，并在检测到不健康节点时自动替换节点。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/crusoe_autocluster.png?resize=1024,578&ssl=1"></p><p>Source: Crusoe 来源：Crusoe</p><p>Crusoe already provides a robust, fully managed Kubernetes offering, making it straightforward for users to deploy and scale containerized workloads. In terms of monitoring and reliability, Crusoe currently implements basic passive health checks; however, these are less detailed and comprehensive compared to those of industry leaders such as CoreWeave. They have not yet implemented automated active weekly scheduled health checks, such as dcgm diag, nccl-tests, Nvidia TinyMeg2, etc. Still, they’ve indicated that this critical feature is actively in development and will soon be integrated into both their managed Slurm and managed Kubernetes offerings and will try to advance their health checks to the level of CoreWeave.<br>Crusoe 已经提供了一个强大、完全托管的 Kubernetes 解决方案，使用户能够轻松部署和扩展容器化工作负载。在监控和可靠性方面，Crusoe 目前实施了基本的被动健康检查；然而，与行业领导者如 CoreWeave 相比，这些检查的细节和全面性较低。他们尚未实施自动化的每周定期主动健康检查，例如 dcgm diag、nccl-tests、Nvidia TinyMeg2 等。尽管如此，他们表示这一关键功能正在积极开发中，并将很快集成到他们的托管 Slurm 和托管 Kubernetes 解决方案中，并将努力将他们的健康检查提升到 CoreWeave 的水平。</p><p>Although they don’t do weekly scheduled active health checks, during cluster bring-up, they do burn-in and active health checks at the initial launch of the cluster; they do some level of testing and qualification. We recommend they investigate how CoreWeave does their cluster burn-in and advance their cluster burn-in to the same level as CoreWeave. CoreWeave has raised the industry bar for the most advanced cluster wide burn in.<br>尽管他们不进行每周定期的主动健康检查，但在集群启动期间，他们会在集群初始启动时进行烧机测试和主动健康检查；他们会进行一定程度的测试和验证。我们建议他们研究 CoreWeave 如何进行集群烧机测试，并将他们的集群烧机测试提升到与 CoreWeave 相同的水平。CoreWeave 已经为最先进的集群全面烧机测试设定了行业标准。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-22.3-Crusoe-3-GIMP.png?resize=1024,538&ssl=1"></p><p>Source: Crusoe 来源：Crusoe</p><p>In terms of pricing and contract terms, Crusoe offers competitive short- to medium-term contracts that are attractive to startups and some enterprises. Their prices and terms are not as competitive as Nebius, but for fast-moving startups that want a simplified UI and user experience, Crusoe is competitive.<br>在定价和合同条款方面，Crusoe 提供具有竞争力的短期到中期合同，这对初创公司和一些企业具有吸引力。他们的价格和条款不如 Nebius 具有竞争力，但对于希望拥有简化用户界面和用户体验的快速发展的初创公司来说，Crusoe 是具有竞争力的。</p><p>Similarly, Crusoe presently does not offer managed Grafana dashboards for GPU monitoring, which is another area they have identified for improvement. They have communicated clear plans to introduce advanced, managed Grafana monitoring dashboards in their upcoming managed Slurm and Kubernetes solutions, further enhancing observability and usability. Overall, Crusoe demonstrates significant responsiveness to user feedback and shows a strong commitment to rapidly evolving their product to meet customer needs and compete effectively in the GPU cloud marketplace.<br>同样，Crusoe 目前并未提供用于 GPU 监控的托管 Grafana 仪表板，这是他们识别出的另一个改进领域。他们已明确传达计划，在即将推出的托管 Slurm 和 Kubernetes 解决方案中引入先进的托管 Grafana 监控仪表板，进一步增强可观察性和可用性。总体而言，Crusoe 对用户反馈表现出显著的响应能力，并展现出强烈的承诺，迅速发展其产品以满足客户需求，并在 GPU 云市场中有效竞争。</p><h2 id="Nebius-内比乌斯"><a href="#Nebius-内比乌斯" class="headerlink" title="Nebius 内比乌斯"></a>Nebius 内比乌斯</h2><p>Nebius is notable for providing the lowest pricing in the GPU cloud market, enabled by their financial position. With billions of dollars on their balance sheet and no existing debt, they benefit from abundant financial resources and significant maneuvering room. Their financial strength directly translates more risk taking and much stronger investment into business development. Examples of this include innovative offerings like bridging H100 contracts into B200 deployments as well as ubiquitous billboards in Santa Clara designed to capture mindshare. The result is unparalleled cost savings for their customers, as Nebius offers market-leading terms and highly competitive rates.<br>Nebius 因其在 GPU 云市场提供最低价格而备受瞩目，这得益于其财务状况。凭借数十亿美元的资产负债表和没有现有债务，他们享有丰富的财务资源和显著的操作空间。他们的财务实力直接转化为更大的风险承担和更强的业务发展投资。这方面的例子包括将 H100 合同转化为 B200 部署的创新产品，以及在圣克拉拉设计的无处不在的广告牌，旨在吸引关注。结果是为客户带来了无与伦比的成本节省，因为 Nebius 提供市场领先的条款和高度竞争的价格。</p><p>One of Nebius’s key strategies for maintaining such low prices is its commitment to using custom Original Design Manufacturer (ODM) chassis. By designing hardware internally and partnering directly with Original Design Manufacturers (ODMs), Nebius bypasses traditional OEM providers like Dell and Supermicro, which typically apply gross margins of around 10-15%. Nebius’s ODM strategy significantly reduces gross margins to about 2%, dramatically lowering both initial hardware investments and ongoing operating expenses, such as power consumption. This cost efficiency places Nebius uniquely among non-Hyperscalers providers, as they adopt an optimization typically only seen within hyperscale cloud providers.<br>Nebius 保持如此低价格的关键策略之一是其致力于使用定制的原始设计制造商（ODM）机箱。通过内部设计硬件并直接与原始设计制造商（ODM）合作，Nebius 绕过了传统的 OEM 供应商，如戴尔和超微，这些供应商通常施加约 10-15%的毛利率。Nebius 的 ODM 策略将毛利率显著降低到约 2%，大幅降低了初始硬件投资和持续的运营费用，如电力消耗。这种成本效率使 Nebius 在非超大规模提供商中独树一帜，因为他们采用的优化通常只在超大规模云提供商中看到。</p><p>Due to its roots as an ex-Russian cloud provider, it boasts an exceptionally talented team of cracked ex-Russian engineers. Nebius still lags behind competitors regarding user experience, though. Despite offering on-demand NVIDIA H100 GPUs at roughly $1.50 per hour (at least for the first thousand hours per month) —half the cost charged by competitors like Lambda Labs—Nebius struggles with customer adoption. Many users still prefer Lambda Labs primarily because Nebius’s UI and UX remain overly complex and unintuitive, creating friction that deters less technically inclined customers. Nebius is committed to fixing its UI&#x2F;UX issues.<br>由于其作为前俄罗斯云服务提供商的根基，它拥有一支极具才华的前俄罗斯工程师团队。然而，Nebius 在用户体验方面仍落后于竞争对手。尽管提供按需的 NVIDIA H100 GPU，价格约为每小时 1.50 美元（至少在每月的前一千小时内）——这仅为 Lambda Labs 等竞争对手收费的一半——Nebius 在客户采纳方面仍面临困难。许多用户仍然更喜欢 Lambda Labs，主要是因为 Nebius 的 UI 和 UX 仍然过于复杂且不直观，造成了阻碍，令技术能力较弱的客户感到困扰。Nebius 致力于解决其 UI&#x2F;UX 问题。</p><p>Finally, Nebius currently offers a fully managed Kubernetes solution but does not yet provide fully automated managed Slurm clusters, a significant gap in their product portfolio. They are actively developing their “Soperator” Slurm solution, which includes foundational passive and active health checks. However, these checks still fall short of industry-leading standards set by providers like CoreWeave. To match competitors’ reliability and observability, Nebius will need to invest more heavily in comprehensive, weekly scheduled active health checks and implement advanced out-of-the-box Grafana dashboards. Strengthening these operational aspects would further enhance their already compelling value proposition by increasing reliably to the level of CoreWeave and having automated node lifecycles.<br>最后，Nebius 目前提供完全托管的 Kubernetes 解决方案，但尚未提供完全自动化的托管 Slurm 集群，这在他们的产品组合中是一个显著的缺口。他们正在积极开发“ Soperator” Slurm 解决方案，其中包括基础的被动和主动健康检查。然而，这些检查仍然未达到 CoreWeave 等提供商设定的行业领先标准。为了匹配竞争对手的可靠性和可观察性，Nebius 需要在全面的每周计划主动健康检查上进行更多投资，并实施先进的开箱即用 Grafana 仪表板。加强这些运营方面将进一步提升他们已经引人注目的价值主张，通过将可靠性提高到 CoreWeave 的水平并实现自动化节点生命周期。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-22.4-Nebius-1-GIMP.png?resize=1024,793&ssl=1"></p><p>Source: Nebius 来源：Nebius</p><h2 id="Oracle-Cloud-Infrastructure甲骨文云基础设施"><a href="#Oracle-Cloud-Infrastructure甲骨文云基础设施" class="headerlink" title="Oracle Cloud Infrastructure甲骨文云基础设施"></a>Oracle Cloud Infrastructure甲骨文云基础设施</h2><p>From our testing, OCI’s GPU experience is strong and consistently recognized as the most cost-effective among the four major Hyperscalers. Their GPU offerings come with a one-click UI deployment from the OCI marketplace called “ <a href="https://docs.oracle.com/en/solutions/deploy-nvidia-ai-on-oci-gvt-region/configure-hpc-cluster-stack-oracle-cloud-marketplace.html#GUID-DD03DBFF-0258-4669-9753-72930294287C">OCI HPC stack</a> ” for both Slurm and monitoring. However, despite this impressive setup, OCI’s Slurm solution isn’t fully managed—it currently operates as a co-managed offering supported by one or two of OCI’s solution architects. To remain competitive, especially against AWS’s and CoreWeave’s comprehensive managed Slurm solutions (the latter having an exceptional node lifecycle controller and automated active health checks), we strongly recommend OCI invest in a fully managed “Oracle Managed Slurm (OMS)” offering, which would benefit the gamut of Oracle customers (sans OpenAI, due to their AGI safety policies).<br>根据我们的测试，OCI 的 GPU 体验强劲，并且在四大超大规模云服务商中被一致认为是最具成本效益的。它们的 GPU 产品提供了一个来自 OCI 市场的“一键式 UI 部署”，称为“OCI HPC stack”，适用于 Slurm 和监控。然而，尽管这个设置令人印象深刻，OCI 的 Slurm 解决方案并不是完全托管的——它目前作为一个由 OCI 的一两个解决方案架构师支持的共同管理产品运行。为了保持竞争力，特别是针对 AWS 和 CoreWeave 的全面托管 Slurm 解决方案（后者拥有卓越的节点生命周期控制器和自动化的主动健康检查），我们强烈建议 OCI 投资于一个完全托管的“Oracle Managed Slurm (OMS)”产品，这将使 Oracle 的广大客户受益（不包括 OpenAI，因为他们的 AGI 安全政策）。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-22.5-Oracle-GIMP.png?resize=1024,919&ssl=1"></p><p>Source: Oracle 来源：甲骨文</p><p>In terms of monitoring, reliability, and passive health checks, OCI offers a decent solution through their Slurm HPC stack marketplace, complete with DCGM, Grafana monitoring, and passive health checks. Nevertheless, OCI currently lacks the option for advanced active health checks and automated node lifecycle management found in CoreWeave’s offerings, such as automated weekly scheduled active health checks (e.g., NCCL-tests, ib_write_bw, dcgm diag) and automatically marking the ones that are unhealthy. Oracle has confirmed that this functionality is already on their roadmap and is scheduled for completion in Q2. Another thing that is missing from OCI HPC stack offering for Slurm is the integrated with high speed parallel filesystems such as OCI’s Managed Lustre offering.<br>在监控、可靠性和被动健康检查方面，OCI 通过其 Slurm HPC 堆栈市场提供了一个不错的解决方案，配备了 DCGM、Grafana 监控和被动健康检查。然而，OCI 目前缺乏 CoreWeave 提供的高级主动健康检查和自动节点生命周期管理的选项，例如自动每周调度的主动健康检查（例如，NCCL-tests、ib_write_bw、dcgm diag）以及自动标记不健康的节点。Oracle 已确认此功能已在其路线图上，并计划在第二季度完成。OCI HPC 堆栈在 Slurm 中的另一个缺失之处是与高速并行文件系统的集成，例如 OCI 的托管 Lustre 解决方案。</p><p>OCI also provides automated <code>topology.conf</code> configurations out of the box, enabling topology-aware scheduling that enhances network performance—an important feature still overlooked by many emerging GPU cloud providers.<br>OCI 还提供开箱即用的自动化 <code>topology.conf</code> 配置，支持拓扑感知调度，提升网络性能——这是许多新兴 GPU 云服务提供商仍然忽视的重要特性。</p><p>Unlike GCP, OCI has been operating RoCE networks for quite a while now, even before the age of GenAI GPUs. They have a stacked bench of high-performance networking experts such as <a href="https://blogs.oracle.com/cloud-infrastructure/post/first-principles-zettascale-oci-superclusters">Jag</a> and his team. Jag and his team can reason about networking from first principles, such as industry-wide link flapping being caused by DSP recalibration, while drawing on their decades of experience. From our testing, we noticed that OCI’s RoCEv2 networking is very competitive with Spectrum-X Ethernet, assuming you are on the right nccl version and have a custom OCI tuner plugin. Even though OCI uses Nvidia CX-7 NICs as part of their networking stack, we have noticed several small nccl tuner regressions on OCI clusters above nccl 2.21.5 even when on the same number of communication SMs. We will show the comprehensive benchmark results in up to 512 GPUs on OCI we ran on real-world message sizes in our upcoming nccl&#x2F;rccl networking deep dive article.<br>与 GCP 不同，OCI 已经运营 RoCE 网络相当长一段时间，甚至在 GenAI GPU 时代之前。他们拥有一支高性能网络专家团队，如 Jag 和他的团队。Jag 和他的团队能够从基本原理出发推理网络问题，例如行业范围内的链路抖动是由 DSP 重新校准引起的，同时借鉴他们数十年的经验。根据我们的测试，我们注意到 OCI 的 RoCEv2 网络在与 Spectrum-X 以太网的竞争中非常有优势，前提是您使用正确的 nccl 版本并且有自定义的 OCI 调谐插件。尽管 OCI 在其网络堆栈中使用 Nvidia CX-7 NIC，但我们注意到在 nccl 2.21.5 以上的 OCI 集群中，即使在相同数量的通信 SM 上，也出现了几个小的 nccl 调谐回归。我们将在即将发布的 nccl&#x2F;rccl 网络深度分析文章中展示我们在 OCI 上运行的高达 512 个 GPU 的全面基准测试结果，测试真实世界消息大小。</p><p>We recommend to OCI that they work with the Nvidia NCCL team to ensure that there is proper regression testing on OCI clusters before the Nvidia NCCL team puts out a new nccl version release such that the out-of-the-box NCCL tuner will be able to have optimized performance on OCI.<br>我们建议 OCI 与 Nvidia NCCL 团队合作，以确保在 Nvidia NCCL 团队发布新的 nccl 版本之前，对 OCI 集群进行适当的回归测试，以便开箱即用的 NCCL 调优器能够在 OCI 上实现优化性能。</p><p>Ultimately, OCI’s support and service teams stand out due to their technical expertise and customer-centric approach. Throughout our interactions, the OCI team consistently demonstrated deep technical expertise and a genuine commitment to customer success. Beyond GPUs, OCI is a Hyperscalers, which means it offers services such as databases, object storage, and CPU-based VMs for tasks like data processing or web scraping.<br>最终，OCI 的支持和服务团队因其技术专长和以客户为中心的方式而脱颖而出。在我们的互动中，OCI 团队始终展现出深厚的技术专长和对客户成功的真诚承诺。除了 GPU，OCI 还是一个超大规模云服务提供商，这意味着它提供数据库、对象存储和基于 CPU 的虚拟机等服务，用于数据处理或网络爬虫等任务。</p><p>This comprehensive infrastructure eliminates the need for customers to transfer or stream data from other hyperscale providers to specialized GPU Neoclouds, resulting in significant operational efficiency and reduced complexity. Another notable advantage of going with a Hyperscaler is their approach to long-term customer engagements. Renting long-term compute resources often comes bundled with partnership opportunities for go-to-market (GTM) collaboration. These GTM partnerships have the potential to benefit customers by expanding their market reach within OCI’s extensive customer base. However, the actual effectiveness of these GTM partnerships can vary significantly depending on individual circumstances.<br>这种综合基础设施消除了客户从其他超大规模提供商转移或流式传输数据到专用 GPU Neoclouds 的需求，从而显著提高了运营效率并减少了复杂性。选择超大规模提供商的另一个显著优势是他们对长期客户关系的处理。租用长期计算资源通常与市场推广（GTM）合作的伙伴关系机会捆绑在一起。这些 GTM 合作伙伴关系有潜力通过扩大客户在 OCI 广泛客户基础中的市场覆盖面来惠及客户。然而，这些 GTM 合作伙伴关系的实际效果可能会因个别情况而有显著差异。</p><p>Regarding security, OCI excels by providing top-notch, enterprise-grade standards, including robust tenant networking isolation, VLAN isolation on RoCEv2 fabrics, and PKEY isolation for InfiniBand fabrics. In contrast, many GPU-focused cloud providers lack even basic certifications like SOC2 or ISO27001 compliance or necessary network isolation protocols, making OCI a preferred choice for enterprises with stringent security requirements.<br>在安全性方面，OCI 通过提供一流的企业级标准而表现出色，包括强大的租户网络隔离、RoCEv2 结构上的 VLAN 隔离以及 InfiniBand 结构上的 PKEY 隔离。相比之下，许多专注于 GPU 的云服务提供商甚至缺乏基本的认证，如 SOC2 或 ISO27001 合规性或必要的网络隔离协议，这使得 OCI 成为对安全要求严格的企业的首选。</p><h2 id="Azure"><a href="#Azure" class="headerlink" title="Azure"></a>Azure</h2><p>Azure offers a robust GPU cloud infrastructure, recognized for its exceptional networking performance. From our internal testing on clusters with up to 128 H100s, Azure demonstrated impressive capabilities, notably utilizing InfiniBand SHARP for efficient in-network reductions. This advanced networking setup makes Azure a top-tier option for high-performance, large-scale AI workloads, particularly suited to intensive multi-node training scenarios. In our upcoming NCCL&#x2F;RCCL deep dive article, we will show real-world NCCL benchmarks with and without SHARP on Azure InfiniBand networking.<br>Azure 提供了强大的 GPU 云基础设施，以其卓越的网络性能而闻名。在我们对最多 128 个 H100 的集群进行的内部测试中，Azure 展现了令人印象深刻的能力，特别是利用 InfiniBand SHARP 进行高效的网络内缩减。这种先进的网络设置使 Azure 成为高性能、大规模 AI 工作负载的顶级选择，特别适合于密集的多节点训练场景。在我们即将发布的 NCCL&#x2F;RCCL 深度探讨文章中，我们将展示 Azure InfiniBand 网络上有无 SHARP 的实际 NCCL 基准测试。</p><p>Security is another standout strength for Azure. It holds an exceptional reputation for robust security and compliance practices, which has made it a trusted partner for government agencies, defense contractors, and leading AGI research labs such as OpenAI. Azure’s proven reliability at scale is evident in its successful management of massive GPU clusters, including support for OpenAI’s well-known deployments involving clusters of over 100,000 NVIDIA H100 GPUs, which highlights Azure’s ability to manage demanding and sensitive workloads securely. Note that OpenAI does complain a lot about Azure’s reliability for giant clusters, but OpenAI has extremely high standards for reliability since they have such giant clusters.<br>安全是 Azure 的另一个突出优势。它在强大的安全性和合规性实践方面享有卓越的声誉，这使其成为政府机构、国防承包商和领先的 AGI 研究实验室（如 OpenAI）的可信合作伙伴。Azure 在大规模下的可靠性在其成功管理大规模 GPU 集群中得到了体现，包括支持 OpenAI 著名的部署，涉及超过 100,000 个 NVIDIA H100 GPU 的集群，这突显了 Azure 安全管理高要求和敏感工作负载的能力。请注意，OpenAI 确实对 Azure 在大型集群的可靠性有很多抱怨，但 OpenAI 对可靠性的标准极高，因为他们拥有如此庞大的集群。</p><p>For workload management, Azure offers CycleCloud, a user-friendly, web-based UI for deploying and managing Slurm clusters. CycleCloud includes basic health checks to enhance reliability and operational awareness. We look forward to doing a complete analysis of CycleCloud. However, compared to more advanced offerings like CoreWeave’s fully automated active and passive health-checking systems, Azure’s solution has room for improvement. We specifically recommend that Azure consider adopting practices similar to CoreWeave’s comprehensive approach, such as regular automated checks (including NCCL tests, ib_write_bw, and DCGM diagnostics), as well as automated node draining and replacement to improve overall reliability.<br>对于工作负载管理，Azure 提供了 CycleCloud，这是一个用户友好的基于网络的 UI，用于部署和管理 Slurm 集群。CycleCloud 包括基本的健康检查，以增强可靠性和操作意识。我们期待对 CycleCloud 进行全面分析。然而，与 CoreWeave 的完全自动化主动和被动健康检查系统等更高级的产品相比，Azure 的解决方案还有改进的空间。我们特别建议 Azure 考虑采用类似于 CoreWeave 综合方法的做法，例如定期自动检查（包括 NCCL 测试、ib_write_bw 和 DCGM 诊断），以及自动节点排空和更换，以提高整体可靠性。</p><p>Additionally, Azure offers a managed Lustre parallel file system, which provides high-performance storage tailored specifically for large-scale HPC and AI workloads. This integrated, optimized storage solution ensures that data-intensive workloads can scale efficiently and reliably. To further enhance their offerings, Azure would benefit from adopting more extensive passive and active monitoring solutions like those implemented by industry-leading GPU cloud providers like CoreWeave, ensuring even higher reliability and improved performance monitoring for their users.<br>此外，Azure 提供了一个托管的 Lustre 并行文件系统，专为大规模 HPC 和 AI 工作负载量身定制，提供高性能存储。这个集成的、优化的存储解决方案确保数据密集型工作负载能够高效且可靠地扩展。为了进一步增强其产品，Azure 可以借鉴行业领先的 GPU 云服务提供商如 CoreWeave 实施的更广泛的被动和主动监控解决方案，从而确保为用户提供更高的可靠性和改进的性能监控。</p><p>Azure’s Hyperscaler status ensures a unity of ecosystem – one doesn’t need to stream their data from elsewhere. Instead, it can be stored in Azure’s native Data Lake and Data Warehousing options. Further, renting long-term compute from Azure (or other Hyperscalers, for that matter) often comes with added ‘partnership’ benefits, with said Hyperscaler helping you sell your product to other Azure customers.<br>Azure 的超大规模云服务状态确保了生态系统的统一——用户无需从其他地方流式传输数据。相反，数据可以存储在 Azure 的本地数据湖和数据仓库选项中。此外，从 Azure（或其他超大规模云服务提供商）租用长期计算资源通常会带来额外的“合作伙伴”福利，该超大规模云服务提供商会帮助您将产品销售给其他 Azure 客户。</p><h2 id="Together-AI-一起人工智能"><a href="#Together-AI-一起人工智能" class="headerlink" title="Together AI 一起人工智能"></a>Together AI 一起人工智能</h2><p>From our testing, Together AI stands out prominently in the GPU cloud provider market. While their cluster offering alone would typically qualify them as a ClusterMax™ Silver-level provider, what truly elevates them to ClusterMax™ Gold is their exceptional support and technical expertise. Together AI’s team, led by Tri Dao—the inventor of Flash Attention—and their Together Kernel Collection (TKC), significantly boost customer performance. We estimate that roughly 30-40% of their GPU cloud customers leverage TKC. We don’t believe the value created by Together can be replicated elsewhere without cloning Tri Dao. From our testing, we have verified that TKC boosts real-world performance for training and inference and that the Tri Dao kernels are genuinely a performance boost.<br>根据我们的测试，Together AI 在 GPU 云服务提供商市场中脱颖而出。虽然他们的集群产品单独就足以使他们成为 ClusterMax™ 银级提供商，但真正将他们提升至 ClusterMax™ 金级的是他们卓越的支持和技术专长。Together AI 的团队由 Flash Attention 的发明者 Tri Dao 领导，他们的 Together Kernel Collection (TKC) 显著提升了客户的性能。我们估计大约 30-40% 的 GPU 云客户利用 TKC。我们认为，Together 创造的价值在没有克隆 Tri Dao 的情况下是无法复制的。根据我们的测试，我们已验证 TKC 提升了训练和推理的实际性能，并且 Tri Dao 内核确实是性能的提升。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-23.-Training-Gated-MLP-Perf-GIMP.png?resize=1024,548&ssl=1"></p><p>Source: Together AI 来源：Together AI</p><p>Even those not utilizing TKC greatly benefit from Tri Dao’s team’s consulting expertise in debugging, optimizing, and troubleshooting training workloads. This combination creates a genuinely full-service, supportive experience, going far beyond merely renting Kubernetes or Slurm-managed clusters. Their competitive pricing further enhances their appeal.<br>即使是那些不使用 TKC 的人，也能从 Tri Dao 团队在调试、优化和故障排除训练工作负载方面的咨询专业知识中受益匪浅。这种组合创造了一个真正的全方位支持体验，远远超出了仅仅租用 Kubernetes 或 Slurm 管理的集群。他们具有竞争力的定价进一步增强了他们的吸引力。</p><p>Additionally, Together AI provides intuitive, user-friendly, managed Slurm and Kubernetes solutions directly accessible via their dashboard, <a href="https://www.youtube.com/watch?v=J8vTTRi2GN4">enabling deployment with just a few clicks</a>. As an NVIDIA portfolio company, Together AI also benefits from early access to new NVIDIA hardware, such as Blackwell GPUs, and collaborates closely with NVIDIA to develop optimized kernels tailored for next-generation GPUs.<br>此外，Together AI 提供直观、用户友好的托管 Slurm 和 Kubernetes 解决方案，用户可以通过其仪表板直接访问，轻松实现几次点击即可部署。作为 NVIDIA 投资组合公司，Together AI 还受益于对新 NVIDIA 硬件（如 Blackwell GPU）的提前访问，并与 NVIDIA 紧密合作，开发针对下一代 GPU 优化的内核。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-24.-Review-Cluster-Details-GIMP.png?resize=1024,621&ssl=1"></p><p>Source: Together AI 来源：Together AI</p><p>However, there are areas for improvement. Together AI currently lacks the Pyxis plugin for container management in Slurm environments and does not offer robust passive health checks or weekly scheduled active health checks, leaving room for concerns regarding reliability. Their default Grafana dashboards are also basic compared to competitors.<br>然而，仍有改进的空间。Together AI 目前缺乏在 Slurm 环境中用于容器管理的 Pyxis 插件，并且没有提供强大的被动健康检查或每周定期的主动健康检查，这使得可靠性方面存在一些担忧。他们的默认 Grafana 仪表板与竞争对手相比也显得基础。</p><p>We recommend that Together AI implement comprehensive health-check systems, similar to those offered by CoreWeave, along with richer, more detailed Grafana dashboards. Furthermore, since Together AI currently relies on infrastructure provided by other GPU cloud providers, such as Applied Digital or Crusoe, support resolution can be delayed due playing a game of broken telephone. Fortunately, this is set to improve significantly, as Together AI plans to deploy their hardware infrastructure within the year, eliminating the current reliance on external providers and streamlining issue resolution.<br>我们建议 Together AI 实施全面的健康检查系统，类似于 CoreWeave 提供的系统，并提供更丰富、更详细的 Grafana 仪表板。此外，由于 Together AI 目前依赖于其他 GPU 云服务提供商（如 Applied Digital 或 Crusoe）提供的基础设施，支持问题的解决可能会因“打电话游戏”而延迟。幸运的是，这一情况预计将显著改善，因为 Together AI 计划在一年内部署其硬件基础设施，从而消除对外部提供商的当前依赖，并简化问题解决流程。</p><h2 id="LeptonAI"><a href="#LeptonAI" class="headerlink" title="LeptonAI"></a>LeptonAI</h2><p>LeptonAI is the GPU cloud founded by the co-creators of PyTorch. LeptonAI does not own any GPUs but instead provides the ML Platform software layer for managing GPUs and health checks. They will claim to be your supercomputing team. You can either rent GPUs through them, where they rent GPUs from other providers, and LeptonAI adds their software + a couple of cents per GPU hour. Or you can rent your GPUs from Nebius, which offers great pricing, and then buy LeptonAI and support for a couple of cents per GPU hour to get the whole LeptonAI platform. LeptonAI bring big tech (Google, Meta, etc) ML platform experience to the broader world, making it accessible to everyday users. The engineers at LeptonAI clearly good at what they are doing and have a strong product sense for what their customers want.<br>LeptonAI 是由 PyTorch 的共同创始人创立的 GPU 云。LeptonAI 不拥有任何 GPU，而是提供用于管理 GPU 和健康检查的 ML 平台软件层。他们声称是您的超级计算团队。您可以通过他们租用 GPU，他们从其他供应商那里租用 GPU，然后 LeptonAI 在每个 GPU 小时上加收几分钱的软件费用。或者您可以从 Nebius 租用 GPU，Nebius 提供很好的定价，然后以每个 GPU 小时几分钱的费用购买 LeptonAI 和支持，以获得整个 LeptonAI 平台。LeptonAI 将大型科技公司（如 Google、Meta 等）的 ML 平台经验带给更广泛的世界，使其对普通用户可访问。LeptonAI 的工程师显然擅长他们所做的事情，并对客户的需求有很强的产品敏感度。</p><p>For training, they offer a Slurm similar method of submitting jobs. For our testing, it took a couple minutes to patch our sbatch scripts to work on the LeptonAI platform. It was decently intuitive to switch to the LeptonAI ML Platform for training. LeptonAI should launch a fully sbatch superset API instead of just being similar to Slurm sbatch.<br>在训练方面，他们提供类似 Slurm 的作业提交方法。在我们的测试中，修补我们的 sbatch 脚本以在 LeptonAI 平台上工作花了几分钟。切换到 LeptonAI ML 平台进行训练相当直观。LeptonAI 应该推出一个完全的 sbatch 超集 API，而不仅仅是类似于 Slurm sbatch。</p><p>In the LeptonAI platform, you can view the node lifecycle in their console dashboard and see what jobs and state each node is in. They have superior node lifecycle visualization, and the only company that has a better node lifecycle dashboard is CoreWeave.<br>在 LeptonAI 平台上，您可以在其控制台仪表板中查看节点生命周期，并查看每个节点的作业和状态。他们具有卓越的节点生命周期可视化，唯一拥有更好节点生命周期仪表板的公司是 CoreWeave。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-25.-Lepton-AI-1-GIMP.png?resize=1024,423&ssl=1"></p><p>Source: LeptonAI 来源：LeptonAI</p><p>For passive health checks, LeptonAI runs gpud which is <a href="https://github.com/leptonai/gpud/tree/main/pkg">their open sourced solution</a> for passive GPU health checks. It provides a comprehensive passive health check coverage for most of the passive health checks. This passive GPU check is still improving but it is a strong solution.<br>对于被动健康检查，LeptonAI 运行 gpud，这是他们开源的被动 GPU 健康检查解决方案。它为大多数被动健康检查提供了全面的被动健康检查覆盖。这种被动 GPU 检查仍在改进中，但它是一个强大的解决方案。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-26.-Lepton-AI-2-GIMP.png?resize=1024,469&ssl=1"></p><p>Source: LeptonAI 来源：LeptonAI</p><p>LeptonAI also has manual active health checks such as DCGM diag and nccl-tests, but this is run manually through the UI dashboard, and it is not done automatically on a weekly scheduled basis like CoreWeave and LeptonAI do not provide reference numbers for what NCCL tests should be. We recommend that they implement an option for customers to opt-in to having automatically actively scheduled health checks. LeptonAI also does not have Megatron Loss convergence active health checks or have Nvidia TinyMeg2 SDC detector active health checks.<br>LeptonAI 还具有手动主动健康检查，例如 DCGM diag 和 nccl-tests，但这是通过 UI 仪表板手动运行的，并不像 CoreWeave 那样每周定期自动进行，LeptonAI 也没有提供 NCCL 测试应该是什么的参考编号。我们建议他们实施一个选项，让客户选择自动主动安排健康检查。LeptonAI 也没有 Megatron Loss 收敛的主动健康检查或 Nvidia TinyMeg2 SDC 检测器的主动健康检查。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-27.-Lepton-AI-3-GIMP.png?resize=1024,503&ssl=1"></p><p>Source: LeptonAI 来源：LeptonAI</p><p>LeptonAI also has some beta features such as a box zero-impact NCCL profiler, which a customer can click a checkbox, and they can gain the full advantage of their custom in-house NCCL profiler to visualize collective bottlenecks and help their customers optimize network bottlenecks.<br>LeptonAI 还具有一些测试功能，例如一个零影响的 NCCL 分析器，客户可以勾选一个复选框，从而充分利用他们自定义的内部 NCCL 分析器来可视化集体瓶颈，并帮助他们的客户优化网络瓶颈。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-28.-Lepton-AI-4-GIMP.png?resize=1024,488&ssl=1"></p><p>Source: LeptonAI 来源：LeptonAI</p><h2 id="ClusterMAX™-Sliver-Tier-GPU-Providers"><a href="#ClusterMAX™-Sliver-Tier-GPU-Providers" class="headerlink" title="ClusterMAX™ Sliver Tier GPU Providers"></a>ClusterMAX™ Sliver Tier GPU Providers</h2><p>Providers rated at <strong>ClusterMAX™ Silver</strong> demonstrate adequate GPU cloud offerings with a satisfactory balance of performance, security, and value; however, they typically have more noticeable gaps compared to Gold- or Platinum-tier services. While these providers meet basic industry standards for reliability, security, and support, they may lack advanced orchestration integration, exhibit moderate networking performance, or have higher total cost of ownership (TCO) leading to higher prices for their customers. <strong>ClusterMAX™ Silver</strong> is receptive to customer and SemiAnalysis feedback and is actively seeking to improve their offerings to be competitive with ClusterMAX™ Platinum in the future.<br>在 ClusterMAX™ Silver 级别的服务提供商展示了足够的 GPU 云服务，具有令人满意的性能、安全性和价值平衡；然而，与 Gold 或 Platinum 级别的服务相比，它们通常存在更明显的差距。虽然这些提供商满足可靠性、安全性和支持的基本行业标准，但它们可能缺乏高级编排集成，网络性能中等，或拥有更高的总拥有成本（TCO），导致客户价格更高。ClusterMAX™ Silver 对客户和 SemiAnalysis 的反馈持开放态度，并积极寻求改进其服务，以便在未来与 ClusterMAX™ Platinum 竞争。</p><h2 id="AWS"><a href="#AWS" class="headerlink" title="AWS"></a>AWS</h2><p>The chief complaint that we hear about AWS is that their networking is worse than InfiniBand and Spectrum-X Ethernet. This is true, and AWS has been working on it with their new p5en EFAv3 16x200GbE H200 instance, which they released in December 2024.<br>我们听到的关于 AWS 的主要投诉是，他们的网络比 InfiniBand 和 Spectrum-X 以太网差。这是事实，AWS 一直在努力改善这一点，他们在 2024 年 12 月发布了新的 p5en EFAv3 16x200GbE H200 实例。</p><p>Their EFAv3 instances are much closer to InfiniBand&#x2F;Spectrum-X performance on nccl-tests than were EFAv2 instances.<br>他们的 EFAv3 实例在 nccl-tests 上的性能比 EFAv2 实例更接近 InfiniBand&#x2F;Spectrum-X 的表现。</p><p>AWS’s P5 EFAv2 instance is 32x100GbE, which is worse than InfiniBand&#x2F;Spectrum-X&#x2F;RoCEv2 but better than the GCP a3-mega instance (8x200GbE) released in April 2024 from our nccl-tests testing. Our NCCL tests also show that their H100 p5 EFAv2 offering has better networking than GCP a3-mega, and their new H200 p5en EFAv3 (16x200GbE) offering has better networking than GCP a3-mega. GCP’s new h200 a3-ultra offering which was released to public in January 2025 which has 8x400GbE RoCEv2 ethernet has better networking performance than AWS’s new p5en EFAv3 offering. We will show the results and benchmarks we ran on real-world message sizes in our upcoming nccl&#x2F;rccl networking deep dive article.<br>AWS 的 P5 EFAv2 实例是 32x100GbE，这比 InfiniBand&#x2F;Spectrum-X&#x2F;RoCEv2 差，但比 2024 年 4 月发布的 GCP a3-mega 实例（8x200GbE）要好。我们的 NCCL 测试还显示，他们的 H100 p5 EFAv2 产品在网络性能上优于 GCP a3-mega，而他们的新 H200 p5en EFAv3（16x200GbE）产品在网络性能上也优于 GCP a3-mega。GCP 的新 h200 a3-ultra 产品于 2025 年 1 月向公众发布，具有 8x400GbE RoCEv2 以太网，其网络性能优于 AWS 的新 p5en EFAv3 产品。我们将在即将发布的 nccl&#x2F;rccl 网络深度分析文章中展示我们在实际消息大小上运行的结果和基准测试。</p><p>AWS is not just a pure GPU cloud but also has all the other services of a cloud, such as Bigtable, databases, object storage, and parallel filesystem offering, which is needed for data proc and web scraping. By being a complete cloud, they mean you don’t need to copy (or stream) data from a “main Hyperscaler cloud,” where data processing is done, to a new cloud cluster; all your data is already there. Renting long-term computing from a Hyperscaler often comes with the added benefit of a “partnership” and partnering on Go to market, where AWS helps you sell to enterprises and other AWS customers. Whether the GTM partnership is effective really depends.<br>AWS 不仅仅是一个纯 GPU 云，还拥有云的所有其他服务，如 Bigtable、数据库、对象存储和并行文件系统，这些都是数据处理和网络爬虫所需的。作为一个完整的云，他们的意思是您不需要将数据从“主 Hyperscaler 云”复制（或流式传输）到新的云集群；您的所有数据已经在那里。从 Hyperscaler 租用长期计算通常还带来了“合作伙伴关系”的额外好处，并在市场推广方面进行合作，AWS 帮助您向企业和其他 AWS 客户销售。GTM 合作伙伴关系的有效性真的取决于具体情况。</p><p>AWS also offers a managed Lustre parallel filesystem called FSX for posix cluster-wide networked storage. For object storage, they have their famous S3 object-managed storage services too.<br>AWS 还提供了一种名为 FSX 的托管 Lustre 并行文件系统，用于 POSIX 集群范围的网络存储。对于对象存储，他们也有著名的 S3 对象管理存储服务。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-29.-AWS-Cluster-GIMP.png?resize=890,608&ssl=1"></p><p>Source: AWS 来源：AWS</p><p>AWS provides a managed Slurm and Kubernetes offering named Hyperpod, which significantly simplifies cluster setup. They offer an UI dashboard for setup and easy-to-follow instructions for setting up their managed offering.<br>AWS 提供了一种名为 Hyperpod 的托管 Slurm 和 Kubernetes 解决方案，显著简化了集群设置。他们提供了一个用于设置的 UI 仪表板和易于遵循的说明，以便设置他们的托管服务。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-30.-AWS-SageMaker-GIMP.png?resize=1024,349&ssl=1"></p><p>Source: AWS 来源：AWS</p><p>Hyperpod includes basic passive and basic active health checks, and it also integrates straightforward Grafana dashboards for monitoring system health. Unfortunately, out of the box they are missing automated active health checks such as nccl-tests and Nvidia’s tinymeg2 SDC detector and running Megatron convergence tests weekly.<br>Hyperpod 包含基本的被动和主动健康检查，并且它还集成了简单的 Grafana 仪表板用于监控系统健康。不幸的是，开箱即用时缺少自动化的主动健康检查，例如 nccl-tests 和 Nvidia 的 tinymeg2 SDC 检测器，以及每周运行 Megatron 收敛测试。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-31.-AWS-healthcheck-GIMP.png?resize=1024,355&ssl=1"></p><p>Source: AWS 来源：AWS</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-32.-AWS-SageMaker2-GIMP.png?resize=1024,543&ssl=1"></p><p>Source: AWS 来源：AWS</p><p>To enhance their service further, AWS should continue investing in networking improvements and consider having as a default for out-of-the-box or simple checkbox for end customers to opt into advanced passive and active automated weekly scheduled health check strategies akin to those utilized by CoreWeave.<br>为了进一步提升服务，AWS 应继续投资于网络改进，并考虑将高级被动和主动自动每周定期健康检查策略作为默认选项，或为最终客户提供简单的复选框，以便选择类似于 CoreWeave 所采用的策略。</p><h2 id="Lambda-Labs"><a href="#Lambda-Labs" class="headerlink" title="Lambda Labs"></a>Lambda Labs</h2><p>Lambda Labs is highly regarded as a go-to provider for on-demand GPU instances, primarily due to their exceptional user interface and intuitive console experience, especially their seamless JupyterLab integration. Despite the availability of other providers, such as Nebius offering H100 SXM GPUs at half the price, Lambda remains popular for on-demand GPU instances due to other offerings having poor UX or security. Lambda Labs offers H100 SXM at $2.99&#x2F;hr&#x2F;GPU, and they typically set the market rate for on-demand offerings due to their high on-demand volume. When Lambda Labs lowers or raises its on-demand offering price, the rest typically do the same.<br>Lambda Labs 被广泛认为是按需 GPU 实例的首选供应商，主要由于其卓越的用户界面和直观的控制台体验，尤其是其无缝的 JupyterLab 集成。尽管还有其他供应商，例如 Nebius 提供的 H100 SXM GPU 价格仅为一半，但由于其他供应商的用户体验或安全性较差，Lambda 仍然因按需 GPU 实例而受到欢迎。Lambda Labs 提供 H100 SXM 的价格为每小时每个 GPU 2.99 美元，通常由于其高按需量而设定按需产品的市场价格。当 Lambda Labs 降低或提高其按需产品价格时，其他供应商通常也会这样做。</p><p>Users have also expressed interest in broader base image choices beyond the standard Lambda stack base image for their on demand offering. Lots of users and SemiAnalysis’s own testing have shown that the on-demand instance boot times are excessively long, typically around 30 minutes. For comparison, Crusoe’s H100 SXM instance offers a boot up in less than 90 seconds. This should be the bar that Lambda Labs aims for. Additionally, Lambda’s default on-demand instances incorrectly set the CUDA toolkit and CLI tool paths to &#x2F;usr&#x2F;bin&#x2F;nvcc instead of the industry-standard &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin&#x2F;nvcc, causing compatibility issues with many open-source repositories. We have spoken with the team at Lambda Labs, and they are committed to reducing the boot time of their on-demand instances.<br>用户们还表示希望在其按需服务中有更广泛的基础镜像选择，而不仅限于标准的 Lambda 堆栈基础镜像。许多用户和 SemiAnalysis 自己的测试表明，按需实例的启动时间过长，通常约为 30 分钟。相比之下，Crusoe 的 H100 SXM 实例的启动时间不到 90 秒。这应该是 Lambda Labs 的目标。此外，Lambda 的默认按需实例错误地将 CUDA 工具包和 CLI 工具路径设置为&#x2F;usr&#x2F;bin&#x2F;nvcc，而不是行业标准的&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin&#x2F;nvcc，导致与许多开源库的兼容性问题。我们与 Lambda Labs 的团队进行了沟通，他们致力于减少按需实例的启动时间。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-33.-Lambda-Labs-1-GIMP.png?resize=1024,462&ssl=1"></p><p>Source: Lambda Labs, SemiAnalysis<br>来源：Lambda Labs，SemiAnalysis</p><p>Lambda Labs also provides managed Kubernetes services, greatly simplifying container orchestration for users. Their managed Kubernetes offering features an out-of-the-box console UI and Grafana monitoring dashboards for viewing node and GPU metrics. Furthermore, they provide out-of-the-box scripts for nccl-tests for end customers to verify their networking performance. They also offer Vast Data based high-speed parallel filesystem for networked storage.<br>Lambda Labs 还提供托管的 Kubernetes 服务，极大简化了用户的容器编排。他们的托管 Kubernetes 解决方案具有开箱即用的控制台 UI 和 Grafana 监控仪表板，用于查看节点和 GPU 指标。此外，他们还提供开箱即用的 nccl-tests 脚本，供最终客户验证其网络性能。他们还提供基于 Vast Data 的高速并行文件系统，用于网络存储。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-33.-Lambda-Labs-2-GIMP.png?resize=1024,549&ssl=1"></p><p>Source: Lambda Labs 来源：Lambda Labs</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-33.-Lambda-Labs-3-GIMP.png?resize=1024,560&ssl=1"></p><p>Source: Lambda Labs 来源：Lambda Labs</p><p>However, their current Slurm offering is unmanaged and, from our testing, is not good. They lack the Pyxis Slurm plugin and a lot of other Slurm features. Fortunately, Lambda is actively developing a managed Slurm solution, which is expected to improve the user experience significantly.<br>然而，他们目前的 Slurm 解决方案是未管理的，并且根据我们的测试，表现不佳。他们缺少 Pyxis Slurm 插件和许多其他 Slurm 功能。幸运的是，Lambda 正在积极开发一个托管的 Slurm 解决方案，预计将显著改善用户体验。</p><p>However, Lambda still lacks several essential features, such as automated passive and active health checks, and its metrics dashboard currently omits crucial GPU metrics that are found in competitor solutions, like CoreWeave. We recommend to Lambda Labs that they look into what CoreWeave has for passive and active health checks and implement a metrics dashboard comparable to CoreWeave’s fantastic out of the box Grafana dashboard.<br>然而，Lambda 仍然缺乏几个重要功能，例如自动的被动和主动健康检查，并且其指标仪表板目前省略了在竞争对手解决方案（如 CoreWeave）中发现的关键 GPU 指标。我们建议 Lambda Labs 研究 CoreWeave 在被动和主动健康检查方面的做法，并实施一个与 CoreWeave 出色的开箱即用 Grafana 仪表板相当的指标仪表板。</p><h2 id="Firmus-Sustainable-Metal-CloudFirmus-可持续金属云"><a href="#Firmus-Sustainable-Metal-CloudFirmus-可持续金属云" class="headerlink" title="Firmus&#x2F;Sustainable Metal CloudFirmus&#x2F;可持续金属云"></a>Firmus&#x2F;Sustainable Metal CloudFirmus&#x2F;可持续金属云</h2><p>SMC is the AI cloud and GPU service provider of Australian-Singaporean sustainable AI Factory builder, Firmus Technologies. They offer Slurm and Kubernetes scheduling solutions, including Pyxis for containerized workloads, and utilize WEKA’s high-performance storage platform to support large-scale AI applications. From our testing, it’s a fairly decent offering.<br>SMC 是澳大利亚-新加坡可持续 AI 工厂建设者 Firmus Technologies 的 AI 云和 GPU 服务提供商。他们提供 Slurm 和 Kubernetes 调度解决方案，包括用于容器化工作负载的 Pyxis，并利用 WEKA 的高性能存储平台来支持大规模 AI 应用。从我们的测试来看，这是一项相当不错的服务。</p><p>In the MLPerf Training v4.0 benchmarks, SMC demonstrated impressive performance by training the GPT-3 175B model. Additionally, SMC has submitted verified MLPerf power consumption results, confirming that H100 immersion cooling consumes less power than comparable air-cooled GPU solutions. They claim that this savings of power translates to lower TCO, and they claim that due to their lower TCO, it translates to lower prices for their customers.<br>在 MLPerf Training v4.0 基准测试中，SMC 通过训练 GPT-3 175B 模型展示了令人印象深刻的性能。此外，SMC 还提交了经过验证的 MLPerf 功耗结果，确认 H100 浸没冷却的功耗低于可比的空气冷却 GPU 解决方案。他们声称，这种节能转化为更低的总拥有成本（TCO），并且由于他们的 TCO 较低，转化为客户更低的价格。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/04/SMC-new-image.jpg?resize=1600,1066&ssl=1"></p><p>Source: SMC 来源：SMC</p><p>They are one of the few clouds besides CoreWeave and Azure to have InfiniBand SHARP in network reductions enabled and have shown us through nccl-tests that their networking performance is superior. But none of their customers are technical experts, so their customers haven’t used it as SHARP requires tuning on the customer’s application level, too. They have shared nccl-tests without SHARP enabled and have shown competitive results for that too.<br>他们是除了 CoreWeave 和 Azure 之外为数不多的启用 InfiniBand SHARP 网络缩减的云服务提供商，并通过 nccl-tests 向我们展示了他们的网络性能优越。但他们的客户都不是技术专家，因此他们的客户没有使用它，因为 SHARP 还需要在客户的应用层进行调优。他们分享了未启用 SHARP 的 nccl-tests，并且也展示了竞争力的结果。</p><p>A concern noted is that SMC’s GPUs operate approximately 10 degrees warmer under load than comparable properly deployed air-cooled offerings due to the use of air-cooled heatsinks in their immersion environment, leading to a 1-2% performance reduction. Despite this, SMC asserts that their pricing more than compensates for this minor performance loss, offering better performance per TCO. They are exploring the adoption of immersion-specific heatsinks to address this issue.<br>一个注意到的问题是，SMC 的 GPU 在负载下的工作温度比可比的适当部署的空气冷却产品高出大约 10 度，这主要是由于在其浸没环境中使用了空气冷却散热器，导致性能降低 1-2%。尽管如此，SMC 仍然声称其定价足以弥补这一小幅性能损失，提供更好的每单位总拥有成本的性能。他们正在探索采用专门针对浸没环境的散热器来解决这个问题。</p><p>Currently, SMC lacks self-service deployment options for Slurm and Kubernetes, relying on SMC own engineers to assist with setup. It is recommended that they develop a user interface or command-line interface for streamlined deployment. Additionally, implementing automated passive health checks and automated weekly scheduled active health checks, similar to those used by CoreWeave, would enhance system reliability. The absence of basic Grafana dashboards for monitoring GPU temperatures and activity is another area for improvement, and adopting CoreWeave’s out-of-the-box monitoring solutions could be beneficial.<br>目前，SMC 缺乏 Slurm 和 Kubernetes 的自助部署选项，依赖于 SMC 自己的工程师来协助设置。建议他们开发一个用户界面或命令行界面，以简化部署。此外，实施自动被动健康检查和每周定期的自动主动健康检查，类似于 CoreWeave 使用的方式，将增强系统的可靠性。缺乏用于监控 GPU 温度和活动的基本 Grafana 仪表板是另一个改进的领域，采用 CoreWeave 的现成监控解决方案可能会带来好处。</p><p>SMC has shown receptiveness to customer and SemiAnalysis feedback, actively considering these recommendations to enhance their offerings and remain competitive in the AI cloud services market.<br>SMC 对客户和 SemiAnalysis 的反馈表现出开放态度，积极考虑这些建议以增强其产品并在 AI 云服务市场中保持竞争力。</p><h2 id="Scaleway"><a href="#Scaleway" class="headerlink" title="Scaleway"></a>Scaleway</h2><p>From our testing, Scaleway offers robust Slurm and Kubernetes solutions, complemented by a high-performance, managed file system powered by VAST Data. This integration ensures scalable and efficient data management for AI and HPC workloads. During our testing, we observed that Scaleway supports NVIDIA’s Pyxis plugin, which enables seamless container integration within Slurm. Their technical team demonstrates a strong understanding of these technologies.<br>根据我们的测试，Scaleway 提供强大的 Slurm 和 Kubernetes 解决方案，并配备由 VAST Data 提供支持的高性能托管文件系统。此集成确保了 AI 和 HPC 工作负载的可扩展和高效的数据管理。在我们的测试中，我们观察到 Scaleway 支持 NVIDIA 的 Pyxis 插件，该插件实现了 Slurm 中无缝的容器集成。他们的技术团队对这些技术表现出深刻的理解。</p><p>As a GDPR-compliant provider and an NVIDIA NCP partner, Scaleway emphasizes data privacy and leverages cutting-edge GPU technology. However, their use of gold-plated DGX Hopper chassis results in a higher total cost of ownership (TCO). This increased cost is often passed on to customers. We recommend that Scaleway explore OEM alternatives, such as Dell or Supermicro HGX SKUs, or consider ODM chassis options, which can deliver the same performance at a reduced cost. Note that it is not recommended to buy gold-plated chassis to be an NVIDIA NCP partner.<br>作为符合 GDPR 的供应商和 NVIDIA NCP 合作伙伴，Scaleway 强调数据隐私并利用尖端的 GPU 技术。然而，他们使用镀金的 DGX Hopper 机箱导致总拥有成本 (TCO) 较高。这一增加的成本通常会转嫁给客户。我们建议 Scaleway 探索 OEM 替代方案，如 Dell 或 Supermicro HGX SKU，或考虑 ODM 机箱选项，这可以以降低的成本提供相同的性能。请注意，不建议购买镀金机箱以成为 NVIDIA NCP 合作伙伴。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-35.-Scaleway-GIMP.png?resize=640,354&ssl=1"></p><p>Source: Scaleway 来源：Scaleway</p><p>Currently, Scaleway lacks self-service deployment options for Slurm and Kubernetes. Although their engineers assist with deployments, implementing a UI or CLI tool for self-service would enhance user experience. Additionally, the absence of automated passive health checks and automated weekly scheduled active health checks is unfortunate. We suggest that Scaleway examine CoreWeave’s approach to health checks and consider adopting similar practices.<br>目前，Scaleway 缺乏 Slurm 和 Kubernetes 的自助部署选项。尽管他们的工程师会协助部署，但实施自助服务的 UI 或 CLI 工具将提升用户体验。此外，缺乏自动被动健康检查和自动每周定期主动健康检查是令人遗憾的。我们建议 Scaleway 研究 CoreWeave 的健康检查方法，并考虑采用类似的做法。</p><p>Furthermore, Scaleway does not provide basic Grafana dashboards for monitoring GPU metrics such as temperature and SM activity. Implementing these dashboards would offer valuable insights into system performance. Encouragingly, Scaleway has been receptive to customer and SemiAnalysis feedback, actively seeking to address these gaps and enhance their offerings.<br>此外，Scaleway 并未提供用于监控 GPU 指标（如温度和 SM 活动）的基本 Grafana 仪表板。实施这些仪表板将为系统性能提供有价值的见解。令人鼓舞的是，Scaleway 对客户和 SemiAnalysis 的反馈持开放态度，积极寻求解决这些问题并增强其产品。</p><h2 id="ClusterMAX™-Bronze-Tier-GPU-ProvidersClusterMAX™-铜级-GPU-供应商"><a href="#ClusterMAX™-Bronze-Tier-GPU-ProvidersClusterMAX™-铜级-GPU-供应商" class="headerlink" title="ClusterMAX™ Bronze Tier GPU ProvidersClusterMAX™ 铜级 GPU 供应商"></a>ClusterMAX™ Bronze Tier GPU ProvidersClusterMAX™ 铜级 GPU 供应商</h2><p>The <strong>ClusterMAX™ Bronze</strong> tier includes GPU cloud providers that fulfill minimum essential criteria but consistently exhibit significant shortcomings in key evaluation areas. Common issues may include inconsistent technical expertise or support, subpar networking performance, unclear SLAs, limited integration with popular tools like Kubernetes or Slurm, or less competitive pricing. Providers in this category typically need considerable improvements to enhance reliability and customer experience. Another reason that GPU providers land themselves in this tier is if they have provided subpar solutions for the past couple of years<br>ClusterMAX™ 铜级包括满足最低基本标准但在关键评估领域持续表现出显著不足的 GPU 云服务提供商。常见问题可能包括技术专长或支持不一致、网络性能不佳、服务水平协议不明确、与 Kubernetes 或 Slurm 等流行工具的集成有限，或定价竞争力较差。此类别的提供商通常需要进行重大改进，以增强可靠性和客户体验。GPU 提供商落入此级别的另一个原因是他们在过去几年中提供了不合格的解决方案。</p><p>Some of the providers in this category are already making considerable effort to catch up. Google Cloud is one such example – and we believe GCP and some other providers are already on a fast path towards ClusterMAX™ Platinum&#x2F;Gold by our next ClusterMAX™ exercise in 3-6 months.<br>此类别中的一些提供商已经在努力追赶。谷歌云就是一个这样的例子——我们相信 GCP 和其他一些提供商在接下来的 3-6 个月内，已经在快速迈向 ClusterMAX™ 白金&#x2F;黄金级别。</p><p>For the longest time, GCP has provided inferior GPU offerings with worse networking and worse out of the box features. It has been in “catchup” mode since April 2024. Many customers have complained about their GPU offerings, but Google Cloud Platform (GCP) is taking in feedback, rapidly improving, and trying to catch up to its competition.<br>长期以来，GCP 提供的 GPU 产品质量较差，网络性能更差，开箱即用的功能也较差。自 2024 年 4 月以来，它一直处于“追赶”模式。许多客户对他们的 GPU 产品表示不满，但谷歌云平台（GCP）正在接受反馈，迅速改进，并努力赶上竞争对手。</p><p>To provide historical context, their first H100 offering, called “a3-high,” was <a href="https://cloud.google.com/blog/products/compute/announcing-cloud-tpu-v5e-and-a3-gpus-in-ga">released in August 2023</a> and featured 800Gbit&#x2F;s “Fastrak TCP” networking bandwidth per node. At that time, Oracle, Microsoft, all Neocloud giants, and most Emerging Neoclouds were offering 3200Gbit&#x2F;s of on-paper networking speeds. This means GCP had 25% of the networking bandwidth compared to their competitors. Most of the GCP customers that used a3-high were not very happy. We will call this phase of GCP’s GPU journey the “not good at all phase.”<br>为了提供历史背景，他们的第一款 H100 产品，名为“a3-high”，于 2023 年 8 月发布，每个节点的网络带宽为 800Gbit&#x2F;s 的“Fastrak TCP”。当时，Oracle、Microsoft、所有 Neocloud 巨头以及大多数新兴 Neocloud 都提供 3200Gbit&#x2F;s 的纸面网络速度。这意味着 GCP 的网络带宽仅为竞争对手的 25%。使用 a3-high 的 GCP 客户大多并不满意。我们将 GCP 的 GPU 旅程的这一阶段称为“完全不好的阶段”。</p><p>Google recognized this feedback from customers and <a href="https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture">in April 2024</a> they released their second and improved H100 offering called “a3-mega”, which doubles networking bandwidth per node from 800Gbit&#x2F;s to 1600Gbit&#x2F;s “Fastrak TCP”. While this was a significant improvement, it is still 50% slower than its competitors, such as Oracle, Microsoft, CoreWeave, and AWS.<br>Google 认识到客户的反馈，并在 2024 年 4 月发布了第二款改进版 H100 产品，名为“a3-mega”，将每个节点的网络带宽从 800Gbit&#x2F;s 提升至 1600Gbit&#x2F;s 的“Fastrak TCP”。虽然这是一个显著的改进，但仍比 Oracle、Microsoft、CoreWeave 和 AWS 等竞争对手慢 50%。</p><p>According to our NCCL tests, they are twice as slow as their competitors on real-world message sizes. On end-to-end training performance, by being twice as slow on networking nccl performance, it translates to 10% worse MFU on O(Llama 70B) size training and 15-20% worse on MFU of O(8x7B) mixture of experts spare models. For the longest time, this offering did not have LL128 nccl protocol, leading to even worse training and nccl networking performance and required the end user to set complex env vars to get their NCCL net&#x2F;tuner plugin to work. Furthermore, their Slurm recipe was buggy and hard to set up. We will refer to this as the “catch-up phase,” where GCP is clearly trying to improve, but it is still not yet on par with its competitors.<br>根据我们的 NCCL 测试，它们在实际消息大小上比竞争对手慢两倍。在端到端训练性能方面，由于在网络 NCCL 性能上慢两倍，这导致 O(Llama 70B) 大小训练的 MFU 差 10%，而 O(8x7B) 专家稀疏模型的 MFU 差 15-20%。很长一段时间以来，这个产品没有 LL128 NCCL 协议，导致训练和 NCCL 网络性能更差，并且需要最终用户设置复杂的环境变量才能使他们的 NCCL net&#x2F;tuner 插件正常工作。此外，他们的 Slurm 配方存在错误且难以设置。我们将把这称为“追赶阶段”，在这个阶段，GCP 显然在努力改进，但仍未达到与竞争对手的水平。</p><p>GCP continues to gather customer feedback, and in <a href="https://cloud.google.com/blog/products/compute/a3-ultra-with-nvidia-h200-gpus-are-ga-on-ai-hypercomputer">January 2025</a>, they launched their a3-ultra instance, which finally offers 3200Gbit&#x2F;s of RDMA Ethernet networking with ConnectX-7 NICs per node, effectively increasing the networking bandwidth per node. This update brings GCP closer to matching the capabilities of its competitors, including Oracle, Microsoft, and CoreWeave.<br>GCP 继续收集客户反馈，并在 2025 年 1 月推出了他们的 a3-ultra 实例，最终提供每个节点 3200Gbit&#x2F;s 的 RDMA 以太网网络，配备 ConnectX-7 网卡，有效提高了每个节点的网络带宽。此更新使 GCP 更接近于匹配其竞争对手的能力，包括 Oracle、Microsoft 和 CoreWeave。</p><p>In practice, they are still not quite on par with real-world NCCL collective networking, which we will explain more about below. With this new a3-ultra SKU, they have moved from TCP to RDMA over Ethernet. As most people are aware, RDMA is often chosen as the collective network protocol over TCP due to its lower latency and higher AI collective performance. We are glad that GCP finally moved towards a more industry networking setup for GPUs, but this comes late and 18 months after their competitors launched their 3200Gbit&#x2F;s RDMA networking offerings. We will refer to this as the “ <strong>almost caught up”</strong> phase. Note that currently the majority of their customers and their GPU fleet are A3-Mega, which means that the majority of their customers are still experiencing subpar networking and have anywhere from 10-20% worse performance when using A3-Mega.<br>在实践中，它们仍然与现实世界的 NCCL 集体网络不完全相当，我们将在下面进一步解释。通过这个新的 a3-ultra SKU，它们已经从 TCP 转向了以太网上的 RDMA。正如大多数人所知，RDMA 通常被选择作为集体网络协议，而不是 TCP，因为它具有更低的延迟和更高的 AI 集体性能。我们很高兴 GCP 终于朝着更符合行业标准的 GPU 网络设置迈进，但这来得太晚，距离他们的竞争对手推出 3200Gbit&#x2F;s RDMA 网络产品已经过去了 18 个月。我们将其称为“几乎赶上”阶段。请注意，目前他们的大多数客户和 GPU 队列都是 A3-Mega，这意味着他们的大多数客户仍然在经历不理想的网络，并且在使用 A3-Mega 时性能下降了 10-20%。</p><p>By mid-2025, GCP will be making their latest A4 B200 and A4X GB200 instances generally available, which will be competitive on paper with AWS, Azure, OCI, and the other Neoclouds that will offer 400Gbit&#x2F;s per GPU. GCP will also continue to improve and launch new software features, which will be setting industry standards. We will call this <strong>“setting the bar” phase</strong>.<br>到 2025 年中，GCP 将推出最新的 A4 B200 和 A4X GB200 实例，届时将普遍可用，这在纸面上将与 AWS、Azure、OCI 及其他提供每个 GPU 400Gbit&#x2F;s 的 Neoclouds 竞争。GCP 还将继续改进并推出新的软件功能，这将设定行业标准。我们将称之为“设定标准”阶段。</p><p>Due to the subpar experience and performance of A3-High and A3-Mega, they have lost a significant amount of customer confidence in their product, which will take time to regain. We believe that by mid 2025 that GCP will finish “catching up” and will soon be raising the bar across the industry and regaining customer confidence. We believe that Google GPU offering could lead towards a ClusterMAX™ Gold or ClusterMAX™ Platinum tier GPU cloud.<br>由于 A3-High 和 A3-Mega 的体验和性能不佳，他们在产品上的客户信任度大幅下降，这需要时间来恢复。我们相信到 2025 年中，GCP 将完成“追赶”，并很快将在整个行业中提升标准，恢复客户信任。我们认为 Google 的 GPU 产品可能会朝着 ClusterMAX™ Gold 或 ClusterMAX™ Platinum 级别的 GPU 云发展。</p><p>In January 2025, we reached out to Google, showing them our NCCL performance tests and a list of all the customer complaints and feedback GCP customers were telling us. The GCP team was quite receptive to the feedback and is working quickly to address it.<br>在 2025 年 1 月，我们联系了谷歌，向他们展示了我们的 NCCL 性能测试以及 GCP 客户向我们反馈的所有客户投诉和意见。GCP 团队对这些反馈非常积极，并迅速采取措施进行解决。</p><p>The first feedback they acknowledge is the subpar networking performance of A3-High and A3-Mega, which comprise the majority of their GPU fleet. They are working on addressing this with the launch of a3-ultra, which comes with the industry-standard 3200Gbit&#x2F;s of RDMA bandwidth per node. For their upcoming A4 B200 and A4X GB200 offerings, it will be competitive on paper in terms of speeds with other B200 and GB200 offerings in the industry.<br>他们承认的第一个反馈是 A3-High 和 A3-Mega 的网络性能不佳，这两者占据了他们 GPU 机队的大部分。他们正在通过推出 a3-ultra 来解决这个问题，该产品每个节点提供行业标准的 3200Gbit&#x2F;s RDMA 带宽。对于他们即将推出的 A4 B200 和 A4X GB200 产品，在速度方面将与行业内其他 B200 和 GB200 产品在纸面上具有竞争力。</p><p>A3-mega instances were also lacking LL128 protocol, which meant that their real-world NCCL performance for real-world message sizes was degraded. In January 2025, they released a fix to all of their customers enabling LL128 protocol on a3-mega. A3-ultra comes out of the box with LL128 NCCL protocol, so it was great to see the improvements in their newer SKUs. A3-ultra still has slightly worse performance than OCI Ethernet and Azure InfiniBand, but on end-to-end training performance, GCP is only 1-2% MFU less than a comparable InfiniBand reference offering. Note that each rail group size for GCP a3-ultra is still only 4 nodes, versus on OCI, Azure and most Neoclouds, it is 32 nodes. This means it will take more hops to do collectives leading to more congestion and slower performance. We will explain this more in our NCCL deep dive article. For a3-mega, they are currently still missing NVLSTree NCCL algorithm. NVLSTree NCCL algo allows for faster networking collective performance by utilizing the NVLS functions in the NVSwitch for multi-node performance. They are currently working on implementing it. For a3-ultra, they have NVLSTree &amp; NVLS &amp; RING &amp; TREE &amp; PAT algorithm support out of the box so it was good to see that GCP is shipping fully functioning products in their latest SKU. In our upcoming NCCL&#x2F;RCCL deep dive article, we will show the performance benchmarks we conducted across GCP instances and how that compares to other offerings.<br>A3-mega 实例也缺少 LL128 协议，这意味着它们在实际消息大小下的 NCCL 性能下降。2025 年 1 月，他们向所有客户发布了修复，使 a3-mega 启用 LL128 协议。A3-ultra 开箱即用 LL128 NCCL 协议，因此看到他们新 SKU 的改进非常好。A3-ultra 的性能仍然略逊于 OCI Ethernet 和 Azure InfiniBand，但在端到端训练性能上，GCP 仅比可比的 InfiniBand 参考产品低 1-2% MFU。请注意，GCP a3-ultra 的每个轨道组大小仍然只有 4 个节点，而在 OCI、Azure 和大多数 Neoclouds 上则为 32 个节点。这意味着进行集体操作需要更多的跳数，从而导致更多的拥堵和更慢的性能。我们将在我们的 NCCL 深度分析文章中对此进行更多解释。对于 a3-mega，他们目前仍然缺少 NVLSTree NCCL 算法。NVLSTree NCCL 算法通过利用 NVSwitch 中的 NVLS 函数来实现更快的网络集体性能，以支持多节点性能。他们目前正在努力实施它。 对于 a3-ultra，他们开箱即用地支持 NVLSTree、NVLS、RING、TREE 和 PAT 算法，因此看到 GCP 在其最新 SKU 中提供完全功能的产品是很好的。在我们即将发布的 NCCL&#x2F;RCCL 深度分析文章中，我们将展示我们在 GCP 实例上进行的性能基准测试，以及这些测试与其他产品的比较。</p><p>From talking to GCP customers, all of them complained about the activation energy needed to properly set NCCL environment variables and correctly link the GCP network&#x2F;tuner plugin and debug it to ensure it hours. This wastes expensive GPU time while their customers debug their NCCL env vars versus on Azure and OCI; NCCL works out of the box. GCP acknowledged this feedback and is looking into how they can make this experience smoother. The next thing that customers have complained about is that GCP does not automatically use <a href="https://slurm.schedmd.com/topology.html">slurm topology.conf</a> for Slurm topology-aware scheduling but instead makes the user do the topology ordering in their sbatch script instead. GCP has addressed this feedback and implemented the fix this year.<br>从与 GCP 客户的交谈中，所有人都抱怨需要激活能量来正确设置 NCCL 环境变量，并正确链接 GCP 网络&#x2F;调谐器插件并调试，以确保其正常运行。这浪费了昂贵的 GPU 时间，而他们的客户在调试 NCCL 环境变量时与 Azure 和 OCI 相比，NCCL 是开箱即用的。GCP 认可了这一反馈，并正在研究如何使这一体验更加顺畅。客户抱怨的下一个问题是 GCP 不会自动使用 slurm topology.conf 进行 Slurm 拓扑感知调度，而是让用户在他们的 sbatch 脚本中进行拓扑排序。GCP 已经解决了这一反馈，并在今年实施了修复。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-36.-GCP-GIMP.png?resize=1024,537&ssl=1"></p><p>Source: GCP 来源：GCP</p><p>The third feedback from GCP customers is that they currently don’t have a fully managed Slurm offering. GCP acknowledged this feedback and are actively working on investigating this. GCP currently has Cluster Toolkit which many customers use for their clusters but it currently does not have a GUI based option for setting up and is not managed and does not have options for setting up out of the box scheduled automated weekly active health checks. While Cluster Toolkit is a massive improvement over what they had 6 months for unmanaged Slurm recipe, it is still missing many features such as being managed.<br>GCP 客户的第三个反馈是，他们目前没有完全托管的 Slurm 服务。GCP 承认了这一反馈，并正在积极调查此事。GCP 目前有集群工具包，许多客户使用它来管理他们的集群，但它目前没有基于 GUI 的设置选项，并且不是托管的，也没有开箱即用的定期自动健康检查的设置选项。虽然集群工具包相比于 6 个月前的非托管 Slurm 配方有了巨大的改进，但仍然缺少许多功能，例如托管。</p><p>The fourth feedback from GCP customers, which GCP acknowledges, is that they are improving their customer technical support by assigning an engineer in charge to be responsible for the entire lifecycle of the customer and their tickets, from creation to resolution. Currently, GCP just sends a bunch of people to hop on a call, but what customers want is just one of their subject matter engineers to “own” the issue from triage to hotfix to long-term resolution. The issue of “having dozens of product managers and engineers” hopping on a call to customers is not just exclusive to GCP’s GPU offering but is a Google-wide issue that they need to address.<br>GCP 客户的第四条反馈，GCP 认可这一点，即他们通过指派一名负责整个客户生命周期及其工单的工程师来改善客户技术支持，从创建到解决。目前，GCP 只是派一堆人参加电话会议，但客户希望的是他们的主题专家工程师能够“拥有”从初步筛选到热修复再到长期解决方案的问题。“让数十名产品经理和工程师”参加客户电话会议的问题不仅仅是 GCP 的 GPU 产品所独有，而是 Google 需要解决的一个全公司范围内的问题。</p><p>Note that most of Google’s internal teams are doing GenAI training and inferencing on TPUs; as such, the GCP GPU experience is not the same as the internal Google ML infra experience. One of the few internal Google teams that utilizes cloud GPUs is DeepMind’s Isomorphic Labs. Although there is a tight back loop between GCP’s customers and GCP’s solution architect team that does dogfooding, it is nowhere near the level of dogfooding as what happens in a company such as AWS, which famously dog foods everything.<br>请注意，Google 大多数内部团队在 TPUs 上进行 GenAI 训练和推理；因此，GCP GPU 体验与内部 Google ML 基础设施体验并不相同。少数利用云 GPU 的内部 Google 团队之一是 DeepMind 的 Isomorphic Labs。尽管 GCP 的客户与进行自我测试的 GCP 解决方案架构师团队之间有紧密的反馈循环，但与 AWS 等公司发生的自我测试水平相比，仍然远远不够。</p><p>Unlike something like OCI, or CoreWeave, monitoring is not setup out of the box, although there an relatively easy to setup monitoring dashboard with <a href="https://cloud.google.com/ai-hypercomputer/docs/monitor">OpsAgent</a>, it is nowhere near as advanced as CoreWeave’s monitoring Grafana dashboard and metrics. Every single customer wants to monitor for GPUs; as such, we recommend this should be set up out of the box. In terms of health checks, GCP does run passive health checks on the VMs, but there is no out-of-the-box solution to run weekly scheduled active health checks on idle nodes, unlike CoreWeave and Nebius. GCP does have  <a href="https://github.com/GoogleCloudPlatform/cluster-health-scanner">cluster-health-scanner</a> it is not weekly automatedly scheduled and not an out of the box solution. We recommend that GCP spends some time and money trying out Corewave SUNK’s offering for themselves and seeing how they perform health checks and monitoring.<br>与 OCI 或 CoreWeave 不同，监控并不是开箱即用的，尽管有一个相对容易设置的 OpsAgent 监控仪表板，但它远不如 CoreWeave 的监控 Grafana 仪表板和指标先进。每个客户都希望监控 GPU；因此，我们建议这应该是开箱即用的。在健康检查方面，GCP 确实对虚拟机进行被动健康检查，但没有开箱即用的解决方案来对闲置节点进行每周计划的主动健康检查，这与 CoreWeave 和 Nebius 不同。GCP 确实有集群健康扫描器，但它不是每周自动调度的，也不是开箱即用的解决方案。我们建议 GCP 花一些时间和金钱尝试 Corewave SUNK 的产品，看看他们如何进行健康检查和监控。</p><p>GCP is not just a GPU cloud but also has all the other services of a cloud, such as Bigtable, databases, object storage, and parallel filesystem offering, which is needed for data proc and web scraping. By being a complete cloud, they mean you don’t need to copy (or stream) the data from a “main Hyperscaler cloud” where the data proc is done to a Neocloud cluster and all your data is just there.<br>GCP 不仅仅是一个 GPU 云，还拥有云的所有其他服务，如 Bigtable、数据库、对象存储和并行文件系统，这些都是数据处理和网络爬虫所需的。作为一个完整的云，他们的意思是您不需要将数据从“主超大规模云”复制（或流式传输）到 Neocloud 集群，您的所有数据都在那里。</p><p>In terms of security, <a href="https://cloud.google.com/security/compliance/offerings#/countries=United_States">GCP’s security</a>  is top-notch and world-class and including properly doing tenant  <a href="https://cloud.google.com/docs/security/encryption-in-transit">networking isolation and encryption in transit</a>. Any enterprises that have strict security requirements should probably go with a Hyperscaler.<br>在安全性方面，GCP 的安全性一流且世界级，包括正确进行租户网络隔离和传输加密。任何有严格安全要求的企业可能应该选择超大规模云。</p><h2 id="Other-Bronze-Providers-其他铜级提供商"><a href="#Other-Bronze-Providers-其他铜级提供商" class="headerlink" title="Other Bronze Providers 其他铜级提供商"></a>Other Bronze Providers 其他铜级提供商</h2><p>Other providers land themselves in the ClusterMAX™ Bronze tier by not having non-beta out-of-box Slurm and&#x2F;or Kubernetes offering or having buggy Slurm and&#x2F;or Kubernetes offerings that are not properly set up. We have given feedback to them, and most of them are receptive to the feedback and are currently building and launching Slurm and&#x2F;or Kubernetes out-of-the-box offerings. Some of these providers in the ClusterMAX™ Bronze tier have been running GPU cloud services for ages now but only recently obtained SOC2 compliance within the last month. While we are grateful that they have SOC2 compliance, we cannot place them any higher for now since they only recently got SOC2 compliance.<br>其他提供商通过没有非测试版的开箱即用的 Slurm 和&#x2F;或 Kubernetes 产品，或拥有设置不当的有缺陷的 Slurm 和&#x2F;或 Kubernetes 产品，进入 ClusterMAX™ 铜级。我们已向他们反馈，大多数人对反馈持开放态度，并正在构建和推出开箱即用的 Slurm 和&#x2F;或 Kubernetes 产品。ClusterMAX™ 铜级中的一些提供商已经运营 GPU 云服务很长时间，但在上个月才获得 SOC2 合规性。虽然我们对他们获得 SOC2 合规性感到感激，但由于他们最近才获得 SOC2 合规性，我们暂时无法将他们评定得更高。</p><p>For what it’s worth, for some of these providers, such as DataCrunch’s on-demand single-node offering, it is quite suitable for development work. We evaluated the DataCrunch on-demand single-node offering, and we quite enjoyed it. But unfortunately, their production cluster is not suitable for inferencing or training.<br>就其价值而言，对于一些提供商，例如 DataCrunch 的按需单节点产品，它非常适合开发工作。我们评估了 DataCrunch 的按需单节点产品，并且非常喜欢它。但不幸的是，他们的生产集群不适合推理或训练。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-37.-Datacrunch-GIMP.png?resize=1024,596&ssl=1"></p><p>Source: Datacrunch 来源：Datacrunch</p><p>TensorWave also has a beta managed Slurm and managed Kubernetes offering and are developing passive and active health checks. We believe that TensorWave’s offering has the potential to be ClusterMAX™ Sliver by the next time we evaluate them.<br>TensorWave 还提供了一个测试版的托管 Slurm 和托管 Kubernetes 服务，并正在开发被动和主动健康检查。我们相信，TensorWave 的产品有潜力在下次评估时成为 ClusterMAX™ Sliver。</p><h2 id="ClusterMAX™-UnderPerform-Tier-GPU-ProvidersClusterMAX™-低效层-GPU-提供商"><a href="#ClusterMAX™-UnderPerform-Tier-GPU-ProvidersClusterMAX™-低效层-GPU-提供商" class="headerlink" title="ClusterMAX™ UnderPerform Tier GPU ProvidersClusterMAX™ 低效层 GPU 提供商"></a>ClusterMAX™ UnderPerform Tier GPU ProvidersClusterMAX™ 低效层 GPU 提供商</h2><p>GPU providers placed in the <strong>UnderPerform</strong> category fail to meet critical basic industry and security requirements across multiple important evaluation metrics. Providers in this tier generally exhibit substantial issues, including inadequate security practices, poor reliability or uptime, unclear or misleading marketing, limited technical knowledge or customer support, and insufficient orchestration capabilities.<br>被归类为表现不佳的 GPU 供应商未能满足多个重要评估指标的关键基本行业和安全要求。这个层级的供应商通常表现出显著的问题，包括安全实践不足、可靠性或正常运行时间差、营销不清晰或误导、技术知识或客户支持有限，以及编排能力不足。</p><p>Most providers land themselves in this category by not having even basic security certifications, such as SOC2 or ISO 27001. Some of these providers also fall into this category by hosting underlying GPU providers that are not SOC 2 compliant either.<br>大多数提供商因未获得基本的安全认证（如 SOC2 或 ISO 27001）而被归入此类别。一些提供商也因托管不符合 SOC 2 标准的底层 GPU 提供商而落入此类别。</p><p>Security is a critical make-or-break factor for many GPU renters, as they store their proprietary model weights on GPU clouds, which have cost tens of thousands to tens of millions of dollars to train and are the core intellectual property of most genAI companies. Furthermore, training and&#x2F;or inferencing these ML models can involve the use of proprietary or personally identifiable information or other user data involved. Customers of these companies that rent from GPU clouds do not want their data to be leaked from using an insecure GPU cloud. In EU countries, the stakes are higher as there are heavy fines for leaking user data as per GPDR law. This is similar to an airline having FAA certification – some people might want to fly on airlines that don’t have FAA certification, but most won’t.<br>安全是许多 GPU 租赁者的关键因素，因为他们将自己的专有模型权重存储在 GPU 云上，这些模型的训练成本从数万美元到数百万美元不等，并且是大多数生成 AI 公司的核心知识产权。此外，训练和&#x2F;或推理这些机器学习模型可能涉及使用专有或个人可识别信息或其他用户数据。这些从 GPU 云租赁的公司的客户不希望他们的数据因使用不安全的 GPU 云而泄露。在欧盟国家，风险更高，因为根据 GDPR 法律，泄露用户数据会面临重罚。这类似于航空公司拥有 FAA 认证——一些人可能想乘坐没有 FAA 认证的航空公司，但大多数人不会。</p><p>Some of these GPU providers even told us that they have lost potential sales due to not having SOC2 and are in the process of gaining SOC2 compliance. We welcome providers in this category to obtain SOC2 compliance.<br>一些 GPU 提供商甚至告诉我们，由于没有 SOC2 认证，他们失去了潜在的销售机会，并且正在获得 SOC2 合规的过程中。我们欢迎这一类别的提供商获得 SOC2 合规。</p><p>Some GPU Providers in this category even admit on their public website that there may be security and privacy concerns, and traffic between the GPU servers and the internet may be heavily logged by a 3 <sup>rd</sup> parties networking equipment.<br>这一类别中的一些 GPU 提供商甚至在其公共网站上承认，可能存在安全和隐私问题，并且 GPU 服务器与互联网之间的流量可能会被第三方网络设备大量记录。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-38.-SaladCloud-GIMP.png?resize=1024,235&ssl=1"></p><p>Source: SaladCloud 来源：SaladCloud</p><p>Some GPU providers such as Massed Compute land themselves in the <strong>UnderPerform</strong> by being unhelpful to the community by inundating the internet with a bunch of AI-generated SEO junk articles with incorrect information. This is harmful to the ML community as it adds a bunch of noise to an already noisy internet and actively leads people astray.<br>一些 GPU 供应商，如 Massed Compute，通过在互联网上充斥大量 AI 生成的 SEO 垃圾文章和错误信息，使自己陷入了表现不佳的境地，这对社区并没有帮助。这对机器学习社区是有害的，因为它为已经嘈杂的互联网增加了更多噪音，并积极误导人们。</p><p>For example, when searching for “H100 vs A100 L2 Cache” on Google, the Massed Compute AI generated junk article with incorrect information shows up first. They actively are spreading misleading information, which is a horrible starting point for a GPU provider.<br>例如，当在 Google 上搜索“H100 与 A100 L2 缓存”时，Massed Compute 生成的错误信息垃圾文章首先出现。他们积极传播误导性信息，这对一个 GPU 供应商来说是一个糟糕的起点。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-39.-Massed-Compute-1-GIMP.png?resize=1024,537&ssl=1"></p><p>Source: Google Search 来源：谷歌搜索</p><p>If you click into the link, it starts the H100 L2 cache size is 256MB which is completely wrong. We recommend that Massed Compute stop spamming the internet with AI-generated junk.<br>如果您点击链接，它会显示 H100 L2 缓存大小为 256MB，这完全是错误的。我们建议大规模计算停止在互联网上发布 AI 生成的垃圾。</p><p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/03/Fig-40.-Massed-Compute-2-GIMP.jpg?resize=703,1024&ssl=1"></p><p>Source: Massed Compute 来源：大规模计算</p><p>Some providers in this category also lack the correct networking drivers and GPU drivers, leading to worse NCCL performance. Additionally, some providers in this category have potential known security issues, such as failing to implement proper tenant isolation using VLANs and pKeys.<br>该类别中的一些提供商也缺乏正确的网络驱动程序和 GPU 驱动程序，导致 NCCL 性能更差。此外，该类别中的一些提供商存在潜在的已知安全问题，例如未能使用 VLAN 和 pKeys 实施适当的租户隔离。</p><p>This is the conclusion of our first ever ClusterMAX™ Rating System update. Please stay tuned for further articles and additional ClusterMAX™ updates.<br>这是我们首次更新 ClusterMAX™评级系统的结论。请继续关注后续文章和更多 ClusterMAX™更新。</p><h2 id="AI-Neocloud-GPU-Rental-Pricing-Trends-in-20242024-年-AI-Neocloud-GPU-租赁价格趋势"><a href="#AI-Neocloud-GPU-Rental-Pricing-Trends-in-20242024-年-AI-Neocloud-GPU-租赁价格趋势" class="headerlink" title="AI Neocloud GPU Rental Pricing Trends in 20242024 年 AI Neocloud GPU 租赁价格趋势"></a>AI Neocloud GPU Rental Pricing Trends in 20242024 年 AI Neocloud GPU 租赁价格趋势</h2><p>Although many observing trends in GPU pricing may characterize AI Neocloud GPU rental pricing for H100s as “collapsing,” – we have not been surprised at all – and see this as a reasonable and logical decline in the cost of computing as the supply of H100s has improved. Equally important is the increasing availability of the B200 and GB200, where we are already seeing term rental contracts being signed, which is starting to push down the market cost of computing and, therefore, the rental price for H100s.<br>尽管许多观察 GPU 定价趋势的人可能会将 AI Neocloud H100 的 GPU 租赁定价描述为“崩溃”，但我们并没有感到惊讶——我们认为这是计算成本合理且合乎逻辑的下降，因为 H100 的供应有所改善。同样重要的是 B200 和 GB200 的可用性不断增加，我们已经看到租赁合同的签署，这开始推动市场计算成本的下降，因此也降低了 H100 的租赁价格。</p><p>In the following section, we will recap the GPU rental pricing trends seen in 2024, the outlook for 2025, and how to analyze the total cost of ownership and returns for AI Neoclouds. We will also discuss the upcoming CoreWeave IPO and how we apply our SemiAnalysis AI Total Cost of Ownership framework towards analyzing return on investment and unit economics for CoreWeave.<br>在接下来的部分中，我们将回顾 2024 年 GPU 租赁定价趋势、2025 年的展望，以及如何分析 AI Neocloud 的总拥有成本和回报。我们还将讨论即将到来的 CoreWeave IPO，以及我们如何将 SemiAnalysis AI 总拥有成本框架应用于分析 CoreWeave 的投资回报和单位经济。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;GPU-云集群-MAX™评级系统-如何租用-GPU-90-覆盖率通过租用-GPU-价值、GPU-泡沫破裂、CoreWeave-IPO、AI-新云经济学&quot;&gt;&lt;a href=&quot;#GPU-云集群-MAX™评级系统-如何租用-GPU-90-覆盖率通过租用-GPU-价值、G</summary>
      
    
    
    
    
    <category term="NVDA" scheme="http://example.com/tags/NVDA/"/>
    
    <category term="GPU" scheme="http://example.com/tags/GPU/"/>
    
    <category term="CoreWeave" scheme="http://example.com/tags/CoreWeave/"/>
    
  </entry>
  
  <entry>
    <title>关于亚马逊和蓝色起源 — 亚马逊创始人Jeff Bezos访谈</title>
    <link href="http://example.com/2025/04/03/Jeff%20Bezos-Amazon%20and%20Blue%20Origin%20-%20Lex%20Fridman%20Podcast/"/>
    <id>http://example.com/2025/04/03/Jeff%20Bezos-Amazon%20and%20Blue%20Origin%20-%20Lex%20Fridman%20Podcast/</id>
    <published>2025-04-03T03:09:30.884Z</published>
    <updated>2025-04-03T03:09:30.884Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=DcWqzZ3I2cY&ab_channel=LexFridman" title="访问地址，需要🪜，或者在小宇宙等播客平台搜索Lex Fridman">访问地址，需要🪜，或者在小宇宙等播客平台搜索Lex Fridman</a></p><p>Lex Fridman是一位主持人和研究人工智能和机器学习的专家。他主持着Lex Fridman Podcast，并在Instagram上分享关于机器人和人类的照片和视频。<br>Lex Fridman对人工智能和机器学习有着浓厚的兴趣，并在他的节目中与各种领域的专家进行访谈，包括马斯克、贝索斯、老黄等。他在Instagram上有超过1.1百万的粉丝，并且他的帖子数量已经超过749个。</p><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>亚马逊和蓝色起源公司的创始人杰夫·贝索斯在Lex Fridman播客#405中，分享了他的童年经历、对太空探索的热爱、对人类未来太空生活的设想，以及他的创新思维方式。贝索斯强调了在解决问题时的“漫步”思考方式，认为真正的创新需要时间和空间去探索和发现。他还分享了蓝色起源公司的一些太空基础设施项目，包括新格伦火箭和蓝环项目。</p><h3 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h3><ul><li>[🚀] 杰夫·贝索斯分享了他在德克萨斯州农场度过的童年，以及他的祖父对他的影响。他的祖父是一个真正的农场主，教会他解决问题的能力和自力更生的精神。</li><li>[🌌] 贝索斯在五岁时看到尼尔·阿姆斯特朗登月，从此对太空和太空探索产生了热爱。他希望看到数万亿的人类生活在太空中，这需要建造巨大的空间站。</li><li>[💡] 贝索斯强调了在解决问题时的“漫步”思考方式。他认为，真正的创新需要时间和空间去探索和发现，而不是直线思考。他鼓励人们在面对新的想法和挑战时，给自己充分的时间和空间去探索可能的解决方案。</li><li>[🚀] 贝索斯分享了蓝色起源公司的一些太空基础设施项目，包括新格伦火箭和蓝环项目。新格伦火箭是一种大型重型运载火箭，能将约45吨的有效载荷送入近地轨道。蓝环项目则是一种能将多达3000公斤的有效载荷送至地球同步轨道或月球附近的空间船。</li></ul><h2 id="分点总结如下："><a href="#分点总结如下：" class="headerlink" title="分点总结如下："></a>分点总结如下：</h2><h3 id="童年生活"><a href="#童年生活" class="headerlink" title="童年生活"></a>童年生活</h3><p>他母亲17岁时生的贝佐斯，4岁到16岁跟祖父在牧场生活。<br>祖父动手能力很强（修好几乎报废的推土机），对他影响最大。<br>干完牧场各种活儿，下午和祖父一起看肥皂剧《The Days of Our Lives》（我们的生活） </p><h3 id="自我认知"><a href="#自我认知" class="headerlink" title="自我认知"></a>自我认知</h3><p>来自斯里兰卡天才同学Yosanta，用10s解出困扰他和另外一个同学3小时的难题。（Youtube评论区此同学大神现身）<br>这让贝索斯意识到，即使再努力，他未来也只是一名平庸的物理学家，立马转学计算机科学专业。<br>著名传记作家Walter Isaacson 认为贝佐斯在“思想实验”水平上与爱因斯坦一个级别。 而贝佐斯对自己的认知：“我就是一个发明家。我善于观察事物。” </p><h3 id="蓝色起源"><a href="#蓝色起源" class="headerlink" title="蓝色起源"></a>蓝色起源</h3><p>尤里·阿列克谢耶维奇·加加林（Yuri Alekseyevich Gagarin）是苏联的一名宇航员，也是人类历史上第一个进入太空的人。 他在1961年4月12日进行了一次为期108分钟的太空旅行，以此完成了对地球的一次全轨道飞行。 加加林据说在太空看到地球时，说： “my God, it’s blue.” 贝佐斯的火箭公司“Blue Origin”的名字由来于此。 </p><ul><li>🚀 太空竞赛对人类历史产生了巨大影响，激发了人们对太空探索的兴趣。</li><li>🚀 Bezos希望未来数千年人类能在太阳系中居住，实现太空殖民和资源开发。</li><li>🚀 他认为建造巨大的太空站是实现这一愿景的关键。</li><li>🚀 Bezos强调了太空探索对保护地球的重要性，认为太空旅行是保护地球的一种途径。</li><li>🚀 Blue Origin正在研发新的火箭，计划在2024年进行首次发射。</li><li>🚀 Bezos对加快Blue Origin的发展进程充满期待，希望成为世界上最果断的公司之一。</li></ul><h3 id="Day-One思想"><a href="#Day-One思想" class="headerlink" title="Day One思想"></a>Day One思想</h3><blockquote><p>贝佐斯的Day One思想，应该是被新bd直接copy了。 </p></blockquote><p>核心理念：每天都像公司成立的第一天那样，带着重新开始的创业精神，快速迭代和革新，不被过往路径依赖或自我一致性限制，保持开放思维，与时俱进。<br>如何避免Day two（停滞&#x2F;衰退）： </p><ol><li>保持对客户的痴迷； </li><li>批判看待代理变量（不被过时的运营指标束缚）；积极重用外部新趋势；</li><li>保持高速决策（150w人的亚马逊，行动依然迅速）</li><li>六页纸开会 贝索斯在Amazon和Blue Origin开会，都使用6页纸memo，为什么不用PPT？ 用PPT开会的问题： <ol><li>PPT是一种说服工具，不利于“寻求真理”。</li><li>只给要点，容易藏匿模糊的思考。 </li><li>对演示者友好，对听众困难。 </li><li>中途容易打断提问，讨论低效。<br>六页纸开会好处：</li></ol></li><li>写6页memo需要投入大量时间和精力，迫使作者做系统思考。</li><li>memo以逻辑叙述方式展开，思考更明晰和严谨，不能藏匿思维漏洞。 </li><li>开会前阅读或开会时一起读，确保与会人在同一个起点，真正讨论问题、激发思考。 </li><li>部分疑问能随着阅读Memo得到解答，避免无效提问，节省时间。</li></ol><blockquote><p>新bd的“飞阅会”，也源于亚马逊的这套方法论，确实好用！</p></blockquote><blockquote><p>wangxin是最早的c2c之王，但是zhangyiming显然在短暂的共事过程中，学得更深入，找到了更好的赛道应用jeff 这一套。</p></blockquote><h3 id="决策技巧"><a href="#决策技巧" class="headerlink" title="决策技巧"></a>决策技巧</h3><p>贝佐斯非常善于决策，比如蓝色起源的目标是成为世界上”最具决策力的公司”。 最出名的是“单向门”和“双向门”决策：难逆转的重大决策是“单向门”，慎重决策；大多数决策是“双向门”决策，要快速决策。<br>除此之外，还有很多有趣的原则：</p><ol><li>“不同意但执行”原则：当团队成员意见跟他不一致，他会说自己不同意，但全力支持执行。</li><li>不要妥协，要寻求真理：妥协带不来真知，决策尽可能追求事物的本质真理。</li><li>当数据和叙事不一致时，相信叙事。（如客户抱怨时，即使数据正常，也要相信客户视角）</li></ol><p>最后一条原则，有个小故事： 亚马逊指标显示客服电话平均等待时间少于60秒，但客户抱怨明显要久得多。 在一次业务回顾会上，贝佐斯当场打客服电话，全场高管沉默等待，发现等待时间远超10分钟。 </p><p>另外，贝佐斯提到人是社会动物，而非理性动物。真相难听，但组织高绩效需要truth telling机制，明确告诉员工这不舒服很正常，鼓励大家直言不讳。 他一般在会议中最后发言，让大家客观表达自己的观点，不会被他的意见所影响。</p><h3 id="Papercut问题"><a href="#Papercut问题" class="headerlink" title="Papercut问题"></a>Papercut问题</h3><p>“papercut”指微小但令人烦恼的问题或困扰。就像一个纸割伤虽然看起来不大，却能引起不成比例的疼痛或不适，一些看似微不足道的问题或困扰也可能给人带来相当大的困扰或不便。 贝佐斯的做法：安排专门团队致力于修复小的缺陷（Papercut问题），其他人专注于大的改进。 </p><h3 id="他对AI的一些观点"><a href="#他对AI的一些观点" class="headerlink" title="他对AI的一些观点"></a>他对AI的一些观点</h3><p>贝佐斯认为 ChatGPT 这样的大语言模型更像是”发现”而不是”发明”。 AI模型不是设计完成的工程对象。我们仍不断被它们的新能力所惊讶。 他对AI 模型更有可能帮助人类而不是伤害我们持乐观态度。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DcWqzZ3I2cY&amp;ab_channel=LexFridman&quot; title=&quot;访问地址，需要🪜，或者在小宇宙等播客平台搜索Lex Fridman&quot;&gt;访问地址，需要🪜，或者在小宇宙等播</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="amzn" scheme="http://example.com/tags/amzn/"/>
    
  </entry>
  
  <entry>
    <title>20250403 多模态数据合成方案</title>
    <link href="http://example.com/2025/04/03/20250403%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90%E6%96%B9%E6%A1%88/"/>
    <id>http://example.com/2025/04/03/20250403%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%95%B0%E6%8D%AE%E5%90%88%E6%88%90%E6%96%B9%E6%A1%88/</id>
    <published>2025-04-02T16:00:00.000Z</published>
    <updated>2025-04-03T03:09:30.882Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-MMEvol"><a href="#1-MMEvol" class="headerlink" title="1. MMEvol"></a>1. MMEvol</h2><p>一个全新的多模态<strong>指令数据演化</strong>框架。它通过<strong>细粒度感知、认知推理</strong>和交互演化的精细组合，来迭代地提升数据质量，从而赋予多模态大模型更强的能力。</p><ul><li><p><strong>指令演化</strong>：在每次生成数据时，随机选择不同层次（感知层、认知推理层或交互层）的演化方式，保证数据分布多样且覆盖面更广。</p></li><li><p><strong>指令消除</strong>：为解决在演化过程中少数生成指令存在错误或无效的情况，系统会自动对这些不合格指令进行过滤和剔除。</p></li></ul><p><img src="https://s.draftai.cn/vent/202504021117683.png"></p><h4 id="论文摘要："><a href="#论文摘要：" class="headerlink" title="论文摘要："></a>论文摘要：</h4><p><a href="https://huggingface.co/papers/2409.05840"></a><a href="https://arxiv.org/pdf/2409.05840">https://arxiv.org/pdf/2409.05840</a><a href="https://huggingface.co/papers/2409.05840"></a></p><blockquote><p>多模态大型语言模型（MLLMs）的发展已经取得了显著进展。然而，多模态指令数据的数量和质量已成为其进展中的重大瓶颈。手动创建多模态指令数据既耗时又低效，给生成高复杂度的指令带来了挑战。此外，从黑箱商业模型（例如，GPT-4o，GPT-4V）提取指令数据通常会导致简单的指令数据，这限制了性能仅能达到这些模型的水平。策划多样化和复杂的指令数据的挑战仍然相当巨大。我们提出了 MMEvol，一个新颖的多模态指令数据演化框架，结合了细粒度感知演化、认知推理演化和交互演化。这种迭代方法突破了数据质量瓶颈，生成一个复杂且多样的图像-文本指令数据集，从而赋予 MLLMs 增强的能力。 从初始指令集 SEED-163K 开始，我们利用 MMEvol 系统地扩展指令类型的多样性，整合推理步骤以增强认知能力，并从图像中提取详细信息以改善视觉理解和鲁棒性。为了全面评估我们数据的有效性，我们使用进化数据训练 LLaVA-NeXT，并在 13 个视觉语言任务上进行实验。与使用种子数据训练的基线相比，我们的方法在这些任务中平均提高了 3.1 个百分点，并在 9 个任务上达到了最先进的（SOTA）性能。</p></blockquote><h2 id="2-STaR-Self-Teaching-AI-Reasonin"><a href="#2-STaR-Self-Teaching-AI-Reasonin" class="headerlink" title="2. STaR(Self-Teaching AI Reasonin)"></a>2. STaR(Self-Teaching AI Reasonin)</h2><p>STaR 旨在提升语言模型在<strong>复杂推理</strong>任务（如数学问题、常识问答）上的能力。</p><ul><li><p><strong>基本原理</strong>：通过迭代的方式，利用少量带有推理示例（rationales）的数据，再结合大量<strong>未带推理过程</strong>的大规模语料，来引导模型不断改进复杂推理能力。</p></li><li><p><strong>核心循环</strong>：先让模型对少量带推理过程的数据进行学习，进而在大规模无推理标注的数据上推断，再根据推断结果不断微调、修正，形成一个简单高效的迭代流程。<br><img src="https://s.draftai.cn/vent/202504021115058.png"></p></li></ul><h4 id="论文摘要：-1"><a href="#论文摘要：-1" class="headerlink" title="论文摘要："></a>论文摘要：</h4><p><a href="https://arxiv.org/pdf/2203.14465">https://arxiv.org/pdf/2203.14465</a></p><blockquote><p>复杂推理任务（如数学或常识问答）上的表现。然而，目前诱导语言模型生成推理需要构建大量的推理数据集，或者通过仅使用少量示例推理来牺牲准确性。我们提出了一种技术，可以迭代地利用少量推理示例和一个没有推理的大数据集，以启动执行越来越复杂推理的能力。这种技术称为“自我学习推理器”（Self-Taught Reasoner，STaR），依赖于一个简单的循环：生成推理以回答许多问题，使用少量推理示例进行提示；如果生成的答案是错误的，则尝试在给定正确答案的情况下再次生成推理；对所有最终产生正确答案的推理进行微调；重复这一过程。我们展示了 STaR 在多个数据集上的表现显著优于直接预测最终答案的微调模型，并且在 CommensenseQA 上与微调一个大 30 × 的最先进语言模型的表现相当。 因此，STaR 使模型通过学习自身生成的推理来改进自己。</p></blockquote><blockquote><ol><li><p><strong>自我教学机制</strong>： STaR利用一个简单的循环过程，通过生成推理理由来回答问题。模型首先从少量的推理示例开始，然后在此基础上生成更多的推理，以提高其推理能力[<a href="https://arxiv.org/abs/2203.14465">1</a>]。</p></li><li><p><strong>链式思维</strong>： STaR强调逐步生成“链式思维”推理，这种方法在处理复杂的推理任务（如数学问题或常识推理）时表现出色。通过这种方式，模型能够更清晰地展示其思考过程，从而提高回答的准确性[<a href="https://openreview.net/pdf?id=_3ELRdg2sgI">2</a>]。</p></li><li><p><strong>迭代改进</strong>： STaR的一个重要特点是其迭代学习能力。模型在每次生成推理后，会根据之前的错误进行调整和改进，从而不断强化正确的推理路径[<a href="https://medium.com/@sahin.samia/star-the-ai-that-teaches-itself-to-reason-a-game-changer-in-ai-development-9f7eba76c93a">3</a>]。</p></li><li><p><strong>小样本学习</strong>： STaR能够在仅依赖少量示例的情况下，提升模型的推理能力，而不需要庞大的数据集。这种方法使得模型在学习新任务时更加高效[<a href="https://highlearningrate.substack.com/p/teaching-ai-to-think-the-self-taught">6</a>]。</p></li></ol></blockquote><h2 id="3-Dyn-VQA-方法-数据集-评测）"><a href="#3-Dyn-VQA-方法-数据集-评测）" class="headerlink" title="3. Dyn-VQA(方法 + 数据集 + 评测）"></a>3. Dyn-VQA(方法 + 数据集 + 评测）</h2><p>某种程度上说，Dyn-VQA 更偏向一个<strong>Benchmark（</strong>方法 + 数据集 + 评测<strong>）</strong>而非单纯的模型或方法。</p><p>为评估多模态 RAG（Retrieval-Augmented Generation）技术在<strong>动态场景</strong>中的表现，阿里团队构建了全新 <strong>Dyn-VQA</strong>：</p><ol><li><p><strong>数据集构建</strong>：Dyn-VQA数据集由1452个问题组成，这些问题被分为三类，旨在测试机器在处理动态问题时的能力[<a href="https://arxiv.org/abs/2411.02937">1</a>][<a href="https://www.themoonlight.io/de/review/benchmarking-multimodal-retrieval-augmented-generation-with-dynamic-vqa-dataset-and-self-adaptive-planning-agent">9</a>]。<br><img src="https://s.draftai.cn/vent/202504020925193.png"></p></li><li><p><strong>动态问题</strong>：与传统的视觉问答（VQA）任务不同，Dyn-VQA的问题需要根据不断变化的知识背景进行回答。这意味着答案可能会随着时间的推移而变化，增加了问题的复杂性[<a href="https://arxiv.org/html/2411.02937v3">5</a>]。</p></li><li><p><a href="https://github.com/Alibaba-NLP/OmniSearch"><strong>多模态检索</strong></a>：该数据集特别设计用于评估多模态检索增强生成方法的有效性。这些方法结合了文本和图像信息，以生成更准确的答案[<a href="https://openreview.net/forum?id=VvDEuyVXkG">3</a>]。<br><img src="https://s.draftai.cn/vent/202504020925795.png"></p></li></ol><h4 id="论文摘要：-2"><a href="#论文摘要：-2" class="headerlink" title="论文摘要："></a>论文摘要：</h4><p><a href="https://arxiv.org/pdf/2411.02937">https://arxiv.org/pdf/2411.02937</a></p><blockquote><p>多模态检索增强生成（mRAG）在缓解多模态大语言模型（MLLMs）固有的“幻觉”问题中发挥着重要作用。尽管前景可期，现有的启发式 mRAG 通常预定义固定的检索过程，这导致了两个问题：（1）非自适应检索查询。（2）过载检索查询。然而，这些缺陷无法通过当前的知识寻求视觉问答（VQA）数据集充分反映，因为所需的知识可以通过标准的两步检索轻松获得。为了弥补数据集的差距，我们首先构建了 Dyn-VQA 数据集，包含三种类型的“动态”问题，这些问题需要在查询、工具和时间上变化的复杂知识检索策略：（1）答案快速变化的问题。（2）需要多模态知识的问题。（3）多跳问题。在 Dyn-VQA 上的实验表明，现有的启发式 mRAG 由于其僵化的检索过程，难以为动态问题提供足够且精确相关的知识。因此，我们进一步提出了首个用于多模态检索的自适应规划代理 OmniSearch。 基本思想是模拟人类在问题解决中的行为，动态地将复杂的多模态问题分解为带有检索动作的子问题链。</p></blockquote><h2 id="4-Florence-2"><a href="#4-Florence-2" class="headerlink" title="4. Florence-2"></a>4. Florence-2</h2><p>微软研究院曾推出视觉大模型 <strong>Florence</strong>，在图像分类、检测、分割和跨模态检索等方面都取得了不错的效果。<strong>Florence-2</strong> 则是该模型的升级版本，聚焦在以下几点：</p><ul><li><p><strong>更大规模的数据</strong>：结合文本、图像、视频等多源数据，进一步扩充了跨模态训练集的规模和多样性。</p></li><li><p><strong>改进的视觉骨干网络</strong>：在保证推理速度的前提下，使用更加高效的骨干网络结构，提升对图像&#x2F;视频内容的理解深度。</p></li><li><p><strong>多模态任务统一</strong>：在新版本中，可能将图像标注、图文检索、视觉问答等多模态任务纳入一个更统一的框架，以减少在任务间切换带来的模型适配成本。</p></li></ul><p><img src="https://s.draftai.cn/vent/202504020921219.png"></p><h4 id="论文摘要：-3"><a href="#论文摘要：-3" class="headerlink" title="论文摘要："></a>论文摘要：</h4><p><a href="https://github.com/kijai/ComfyUI-Florence2"></a><a href="https://arxiv.org/pdf/2311.06242">https://arxiv.org/pdf/2311.06242</a> <a href="https://github.com/kijai/ComfyUI-Florence2">https://github.com/kijai/ComfyUI-Florence2</a></p><blockquote><p>Florence-2，这是一种新颖的视觉基础模型，具有统一的基于提示的表示，适用于各种计算机视觉和视觉语言任务。虽然现有的大型视觉模型在迁移学习方面表现出色，但它们在处理简单指令的多样任务时却显得力不从心，这种能力意味着需要处理各种空间层次和语义粒度的复杂性。Florence-2 的设计旨在将文本提示作为任务指令，并生成所需的文本形式结果，无论是图像描述、物体检测、定位还是分割。这种多任务学习设置需要大规模、高质量的标注数据。为此，我们共同开发了 FLD-5B，包含了对 1.26 亿张图像的 54 亿条全面视觉注释，采用了自动图像注释和模型优化的迭代策略。我们采用了序列到序列的结构来训练 Florence-2，以执行多样化和全面的视觉任务。在众多任务上的广泛评估表明，Florence-2 是一种强大的视觉基础模型竞争者，具有前所未有的零样本和微调能力。</p></blockquote><h2 id="5-InternVL-2-5"><a href="#5-InternVL-2-5" class="headerlink" title="5. InternVL 2.5"></a>5. InternVL 2.5</h2><p><strong>InternVL</strong>（或 InternVideo&#x2F;Intern 模型家族）是多模态领域系列研究项目，通常着重于<strong>图像、视频与语言</strong>的统一建模。版本 <strong>2.5</strong> 可能包含：</p><ul><li><p><strong>多模态预训练与对齐</strong>：大规模的视觉-语言预训练，强调在图像和文本之间进行更细粒度的特征对齐（如区域级对齐或实体级对齐）。</p></li><li><p><strong>跨模态检索与推理优化</strong>：针对检索场景（例如带图片的电商商品检索）做了专项优化，提升在不同场景下的高准确度理解和回答能力。</p></li><li><p><strong>模块化设计与可扩展性</strong>：在框架上可能采用模块化设计，便于继续往视频、多语言等更多模态与场景扩充。</p></li></ul><p><img src="https://s.draftai.cn/vent/202504020918276.png"></p><h4 id="论文摘要：-4"><a href="#论文摘要：-4" class="headerlink" title="论文摘要："></a>论文摘要：</h4><p><a href="https://github.com/kijai/ComfyUI-Florence2"></a><a href="https://arxiv.org/abs/2412.05271">https://arxiv.org/abs/2412.05271</a> <a href="https://huggingface.co/OpenGVLab/InternVL2_5-2B">https://huggingface.co/OpenGVLab/InternVL2_5-2B</a></p><blockquote><p>InternVL 2.5，这是一个先进的多模态大型语言模型（MLLM）系列，基于 InternVL 2.0，保持其核心模型架构，同时在训练和测试策略以及数据质量方面引入了显著的增强。在这项工作中，我们深入探讨了模型规模与性能之间的关系，系统地研究了视觉编码器、语言模型、数据集大小和测试时间配置的性能趋势。通过对多种基准的广泛评估，包括多学科推理、文档理解、多图像&#x2F;视频理解、现实世界理解、多模态幻觉检测、视觉定位、多语言能力和纯语言处理，InternVL 2.5 展现了竞争力的性能，媲美领先的商业模型如 GPT-4o 和 Claude-3.5-Sonnet。值得注意的是，我们的模型是第一个在 MMMU 基准上超过 70%的开源 MLLM，通过链式思维（CoT）推理实现了 3.7 点的提升，并展示了在测试时间扩展方面的强大潜力。</p></blockquote><hr><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ol><li><p><strong>MMEvol</strong>：强调对多模态指令数据的多层次“演化”与筛选，提高数据质量。</p></li><li><p><strong>STaR</strong>：利用小规模带推理过程数据 + 大规模无推理数据的迭代训练策略，提升模型推理能力。</p></li><li><p><strong>Dyn-VQA</strong>：关注在“动态”知识、上下文实时变化下的多模态问答评测。</p></li><li><p><strong>Florence-2</strong>：微软视觉大模型 Florence 的进阶版本，数据更大、骨干网络更强，致力于统一多模态任务。</p></li><li><p><strong>InternVL 2.5</strong>：多模态预训练与推理框架的新迭代版本，突出图像、视频与语言的对齐和检索能力，并具备良好的可扩展性。</p></li></ol><p>通过以上 5 大方案的配合或组合，可以在多模态任务（如视觉问答、图文检索、复杂推理、动态场景问答等）中获得更完善、<strong>更强大的数据合成与模型训练体系</strong>。</p><p>它们分别在指令调优、推理能力迭代、动态知识处理以及大规模多模态数据利用等方面提供了可行的思路和实践范式。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-MMEvol&quot;&gt;&lt;a href=&quot;#1-MMEvol&quot; class=&quot;headerlink&quot; title=&quot;1. MMEvol&quot;&gt;&lt;/a&gt;1. MMEvol&lt;/h2&gt;&lt;p&gt;一个全新的多模态&lt;strong&gt;指令数据演化&lt;/strong&gt;框架。它通过&lt;strong</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="AI" scheme="http://example.com/tags/AI/"/>
    
    <category term="Multimodal" scheme="http://example.com/tags/Multimodal/"/>
    
    <category term="RAG" scheme="http://example.com/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>阿里巴巴技术面分析(截止到2024年4月21号)</title>
    <link href="http://example.com/2024/04/21/Alibaba%20Stock%20Trend%20in%202024/"/>
    <id>http://example.com/2024/04/21/Alibaba%20Stock%20Trend%20in%202024/</id>
    <published>2024-04-20T16:00:00.000Z</published>
    <updated>2025-04-02T08:13:12.850Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Apr-21-2024"><a href="#Apr-21-2024" class="headerlink" title="Apr 21 2024"></a>Apr 21 2024</h3><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>阿里巴巴集团控股的股票代码BABA的技术面，通过日线图和周线图的分析发现，自4月5日以来，股价显示出看跌不平衡，表现为连续失败的上涨尝试和较低的高点，目前股价跌破了之前的黄色区间，预示着未来可能会有更多的下行空间。目标价位设在63.32美元，预计短期内可能会有小幅反弹至70美元水平，但整体趋势看跌，除非能够突破当前下降趋势线并形成趋势反转，否则预计股价可能会继续<strong>下探至60美元或57美元的历史低点</strong>。</p><h3 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h3><ul><li>📉 股价从黄色区间跌破，显示出看跌不平衡，表明短期内看跌趋势仍将延续。</li><li>🔍 分析指出，自4月5日以来，阿里巴巴股票经历了两次上涨尝试都以失败告终，形成了较低的高点。</li><li>🎯 目标价位设在<strong>63.32</strong>美元，指出如果按照当前的范围大小进行估算，股价有可能达到此水平。</li><li>🔄 预计短期内可能会有小幅反弹至70美元水平，但除非股价能够大幅上涨并找到新的支撑，否则整体趋势仍看跌。</li><li>📊 周线图分析显示，当前的周K线非常看跌，连续几周的红色K线中只有一小根绿色K线，且股价仍然位于EMA和长期趋势线之下。</li><li>📉 分析最后指出，除非能够突破当前下降趋势线并形成趋势反转，否则预计股价可能会继续<strong>下探至60美元或57美元的历史低点</strong>。<br><img src="https://s.draftai.cn/vent/20240421082621.png" alt="image.png"></li></ul><h3 id="Apr-6-2024"><a href="#Apr-6-2024" class="headerlink" title="Apr 6 2024"></a>Apr 6 2024</h3><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>阿里巴巴股票技术分析的讨论，指出股价可能在短期内上涨，但在长期内仍然表现熊市。</p><p>箱体运动：支撑70，阻力77。<br><img src="https://s.draftai.cn/vent/20240406164243.png" alt="image.png"></p><h3 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li><p>[💼] 视频中提到阿里巴巴股票价格在短期内有望上涨至75.50美元。</p></li><li><p>[📉] 长期来看，股价仍处于熊市状态，仍有可能继续下跌。</p></li><li><p>[📈] 建议需要同时关注短期和长期的趋势，以便更好地制定交易策略。</p></li><li><p>阿里巴巴股票派对时间？00:01</p><ul><li>分析了阿里巴巴集团控股的日线图表和周线图表，目前处于黄色区间内。</li><li>预测可能会看到更高的股价走势，已经完成了一定程度的调整。</li></ul></li><li><p>阿里巴巴股票技术分析00:39</p><ul><li>股价可能会突破趋势线，形成更高的低点。</li><li>预测可能会看到更高的价格走势，甚至有可能出现更大的上涨。</li></ul></li><li><p>阿里巴巴股票展望01:10</p><ul><li>预测可能会向上突破，看到更高的价格。</li><li>长期来看仍然是熊市，但短期可能会看到上涨。</li></ul></li><li></li></ul><h3 id="Mar-2-2024"><a href="#Mar-2-2024" class="headerlink" title="Mar 2 2024"></a>Mar 2 2024</h3><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>这是一段关于阿里巴巴股票分析的视频内容，分析了该股票的日线图和周线图，预测了股价的走势。</p><h3 id="Highlights-1"><a href="#Highlights-1" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>[📉] 阿里巴巴股票自去年12月以来一直在一个价格区间内震荡，目前价格在70-80美元之间。</li><li>[📈] 股价呈上升趋势，有望再次测试77-78美元的价格，甚至可能突破80美元。</li><li>[📉] 股票处于熊市，并且有可能继续向下趋势，直到突破下降趋势线并回到黄色区间内。</li><li>[📉] 长期来看，股价有可能下跌至50美元左右的水平。需要看到价格突破下降趋势线并回到黄色区间，才有可能看到更高的价格。</li></ul><p><img src="https://s.draftai.cn/vent/20240302083831.png" alt="image.png"></p><h3 id="Feb-24-2024"><a href="#Feb-24-2024" class="headerlink" title="Feb 24 2024"></a>Feb 24 2024</h3><h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><p>本视频是关于阿里巴巴集团控股（BABA）的技术分析，主要讲解了日线图和周线图上的走势。视频中指出，价格在过去一周有所上涨，目前正试图突破80美元的阻力位。同时，视频也提到了价格处于一个下降趋势，并且需要突破多个关键位才能确认短期和长期的走势。总体而言，短期看涨，但长期看跌。</p><h3 id="Highlights-2"><a href="#Highlights-2" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>💹 价格在过去一周有所上涨，试图突破80美元的阻力位。</li><li>📈 短期看涨，但长期趋势仍然看跌。</li><li>🔍 需要突破多个关键位才能确认短期和长期的走势。</li></ul><p><img src="https://s.draftai.cn/vent/20240224102140.png" alt="image.png"></p><h3 id="Feb-3-2024"><a href="#Feb-3-2024" class="headerlink" title="Feb 3 2024"></a>Feb 3 2024</h3><h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p>这篇内容是关于对阿里巴巴集团控股（BABA）进行技术分析的视频。视频中提到了阿里巴巴的股价走势以及未来可能的趋势。目前股价走势不太乐观，可能会继续下跌。但也有可能在未来出现反弹。需要等待财报发布后再做决策。</p><h3 id="Highlights-3"><a href="#Highlights-3" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>💰 阿里巴巴的股价走势不太乐观，可能会继续下跌。</li><li>📉 目前股价可能不会出现第二次上涨，而是继续下跌。</li><li>📊 阿里巴巴的财报将于2月7日发布，可能会对股价产生影响。</li><li>📈 股价的短期上涨可能会在80美元附近得到测试，但长期趋势仍然看跌。</li><li>🔄 目前的趋势可能会逐渐平稳，甚至可能出现趋势反转。</li><li>⏳ 需要等待财报发布后才能做出决策。</li></ul><h3 id="Feb-1-2024"><a href="#Feb-1-2024" class="headerlink" title="Feb 1 2024"></a>Feb 1 2024</h3><h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><p>这篇文章是关于对阿里巴巴集团控股股票（BABA）的技术分析。作者通过每日和每周的图表来分析股票走势。目前的图表显示，阿里巴巴股价呈现下降趋势，短期内可能还会继续下跌。长期来看，股价仍然看跌，可能会再次达到历史低点。作者认为，虽然股价可能会有短期上涨，但整体来说，阿里巴巴股票仍然看跌。</p><h3 id="Highlights-4"><a href="#Highlights-4" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li><p>💼 阿里巴巴股票呈现下降趋势，短期内可能继续下跌。</p></li><li><p>📉 长期来看，阿里巴巴股票仍然看跌。</p></li><li><p>💰 股价可能会有短期上涨，但整体趋势仍然看跌。</p></li><li><p>📊 通过每日和每周的图表分析股票走势。</p></li><li><p>📉 阿里巴巴股票可能会再次达到历史低点。</p></li><li><p>在每日图表上，BABA股票价格下跌，跌破了21日指数移动平均线。</p></li><li><p>股价可能会继续下跌，目标是突破66美元的低点。</p></li><li><p>股价可能会回升到80美元的突破点，但可能会遇到阻力。</p></li><li><ul><li>股价仍然低于指数移动平均线和80美元的水平。</li></ul></li><li><p>预计股价将继续下跌，可能会达到50美元的目标。</p></li></ul><h3 id="Jan-28-2024"><a href="#Jan-28-2024" class="headerlink" title="Jan 28 2024"></a>Jan 28 2024</h3><p>这篇内容是关于阿里巴巴集团控股股票（BABA）的分析。作者通过每日和每周的图表分析，指出股价目前处于下降趋势，但短期内可能会有一些上涨。然而，总体来说，股价仍然处于下降趋势已经持续了三年以上。</p><h3 id="Highlights-5"><a href="#Highlights-5" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>💹 股价在每日图表上找到了临时支撑，并试图向上推升。</li><li>💹 短期内股价有可能达到80美元，但总体上仍然存在下降的动力。</li><li>💹 每周图表显示，这周的股价表现比较积极，但仍然处于下降趋势。</li><li>💹 如果短期内的上涨力度强劲，股价有可能继续向上推升，否则可能会继续下跌。</li></ul><h3 id="Jan-20-2024"><a href="#Jan-20-2024" class="headerlink" title="Jan 20 2024"></a>Jan 20 2024</h3><p>该视频分析了阿里巴巴集团控股有限公司（BABA）的股票走势。目前，阿里巴巴的股价处于下降趋势，但可能会出现短期反弹。然而，股票仍然处于熊市，有很多看跌的迹象。阿里巴巴的股价甚至低于其2014年的首次公开募股价格。虽然公司价值上升，但市场并不认同，技术因素主导了股价走势。预计短期内股价可能会有小幅上涨，但长期趋势仍然看跌。可能在今年底或明年初见底，然后开始反转。需要等待大量买盘和流动性进入市场才能确认反转。总之，阿里巴巴的股票走势目前看起来非常看跌。</p><h3 id="Highlights-6"><a href="#Highlights-6" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>📉 阿里巴巴的股价处于下降趋势，形成了连续的低点和低峰。</li><li>📉 股票仍然处于熊市，有很多看跌的迹象。</li><li>📉 阿里巴巴的股价低于其2014年的首次公开募股价格（70）。</li><li>📈 预计短期内股价可能会有小幅上涨，但长期趋势仍然看跌。</li><li>📈 可能在今年底或明年初见底，然后开始反转。</li><li>📈 需要等待大量买盘和流动性进入市场才能确认反转。</li><li>📈 阿里巴巴的股票走势目前看起来非常看跌。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Apr-21-2024&quot;&gt;&lt;a href=&quot;#Apr-21-2024&quot; class=&quot;headerlink&quot; title=&quot;Apr 21 2024&quot;&gt;&lt;/a&gt;Apr 21 2024&lt;/h3&gt;&lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="invest" scheme="http://example.com/tags/invest/"/>
    
    <category term="ali" scheme="http://example.com/tags/ali/"/>
    
    <category term="#VTrade" scheme="http://example.com/tags/VTrade/"/>
    
  </entry>
  
  <entry>
    <title>英伟达技术面分析</title>
    <link href="http://example.com/2024/04/21/NVDA%20tech%20analysis/"/>
    <id>http://example.com/2024/04/21/NVDA%20tech%20analysis/</id>
    <published>2024-04-20T16:00:00.000Z</published>
    <updated>2025-04-02T08:13:12.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Apr-21-2024"><a href="#Apr-21-2024" class="headerlink" title="Apr 21 2024"></a>Apr 21 2024</h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>技术分析显示，股价在接近1000美元处形成了双顶，随后突破了一个急剧上升的趋势线，并经历了两次下跌。最近的股价回调达到了目标<strong>767美元</strong>，预计短期内<strong>可能会有反弹</strong>，但是否能持续上涨还需观察。分析师预测，如果市场继续下跌，可能会看到更多的下跌，尤其是如果反弹力度不强的话。周线图表明，NVIDIA股价可能继续下跌，但500美元的水平可能会成为一个支撑点。</p><h3 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h3><ul><li>📉 NVIDIA股价在接近1000美元处形成双顶，随后突破急剧上升的趋势线，并出现了两次下跌。</li><li>📊 最近的股价回调达到了目标<strong>767美元</strong>，预计短期内可能会有反弹。</li><li>🔄 分析师认为，如果反弹力度不强，则可能导致进一步的下跌。</li><li>📈 如果市场继续下跌，NVIDIA可能会进一步下探，存在填补至<strong>680美元和582美元</strong>的潜力。</li><li>📉 周线图显示，NVIDIA股价从EMA指标上看已经过度延伸，显示出连续红周之后的连续绿周，暗示中期内可能继续下跌。</li><li>📊 日线和周线图均预示短期可能的反弹，但长期趋势仍然不明朗，特别是考虑到市场快速动荡的可能性。<br><img src="https://s.draftai.cn/vent/20240421090101.png" alt="image.png"></li></ul><p><img src="https://s.draftai.cn/vent/20240421091157.png" alt="image.png"></p><h2 id="Apr-8-2024"><a href="#Apr-8-2024" class="headerlink" title="Apr 8 2024"></a>Apr 8 2024</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>V trades在其YouTube视频中对NVIDIA Corporation（股票代码：nvda）进行了技术分析。他首先分析了日线图，然后分析了周线图。他发现自上周以来，NVIDIA的价格一直在下跌。他还解释说，股价下跌的趋势可能会延续下去，因为目前市场的情绪仍然偏空。然而，如果价格能够突破906美元的关键水平，那么股价可能会反弹到区间的高点。他还观察到，从周线图中可以看出，股价过度拉升，可能会开始回落。他建议投资者观望市场，以便在未来做出正确的决策。</p><h3 id="亮点-1"><a href="#亮点-1" class="headerlink" title="亮点"></a>亮点</h3><ul><li><p>[📊] 自上周以来，NVIDIA的价格一直在下跌。</p></li><li><p>[💡] 由于市场情绪仍然偏空，股价下跌的趋势可能会延续下去，到800位置。EMA在706。</p></li><li><p>[📈] 但是，如果价格能够突破906美元的关键水平，那么股价可能会反弹到区间的高点。</p></li><li><p>[⚠️] 从周线图中，他观察到股价过度拉升，可能会开始回落。</p></li><li><p>[👀] V trades建议投资者观望市场，以便在未来做出正确的决策。</p></li><li><p><strong>NVIDIA股票分析开始!</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">00:00</a></p><ul><li>这是V trades，今天我将为NVIDIA Corporation进行技术分析</li><li>本视频仅供娱乐目的，请自行承担交易风险</li><li>我将首先解析我在日线图上看到的情况，然后结束本视频的是周线图</li></ul></li><li><p><strong>NVIDIA价格动态</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">00:31</a></p><ul><li>NVIDIA股价继续下跌，达到黄色范围的底部</li><li>双顶和双底分别在857和957，大约100点范围</li><li>股价通常会从高点到低点，低点到高点波动</li></ul></li><li><p><strong>NVIDIA可能的走势</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">01:04</a></p><ul><li>现在NVIDIA仍处于熊市领域，因为我们有这个下降趋势线</li><li>如果牛市想要看到更高的价格，他们将需要突破这个趋势线</li><li>还有一个测量移动目标在，价格还没有达到，这个目标在839-838</li></ul></li><li><p><strong>NVIDIA价格状况评估</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">01:37</a></p><ul><li>我们看到测量目标和下降趋势线仍在发挥作用</li><li>我认为价格有很好的可能继续下跌</li><li>但如果我们突破了906的水平，价格可能会快速回升到范围的高点</li></ul></li><li><p><strong>NVIDIA周线图展示</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">02:09</a></p><ul><li>周线图显示了大幅超买，我们看到了连续两个红周</li><li>这可能开始回调，我认为它需要一个适度的回调</li><li>从今年1月开始，我们看到了近100%的上涨，这在这么大的股票中非常罕见</li></ul></li><li><p><strong>NVIDIA价格可能开始回落</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">02:44</a></p><ul><li>周线图显示了大幅超买，价格可能会开始回落</li><li>价格可能会回到EMA当前的706水平，可能会大幅回调</li><li>从2015年以来，我们一直处于这个渠道的顶部，这可能会作为阻力，将价格送回渠道的低点</li></ul></li><li><p><strong>NVIDIA在大周期角度看</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">03:19</a></p><ul><li>在宏观角度看，我们确实过度延伸，或者我们在其移动的顶部</li><li>短期来看，我们看到了一些熊市动能</li><li>有很多原因说明价格应该继续下跌</li></ul></li><li><p><strong>NVIDIA股票的交易策略</strong><a href="https://www.youtube.com/watch?v=hSFkEJSclE8&ab_channel=VeeTrades">03:56</a></p><ul><li>我们现在处于支持位，并且我并不想买入，因为测量目标尚未达到，这条趋势线仍在发挥作用</li><li>除非我们看到一个强劲的上涨，否则我不会做这个交易</li><li>我仍在等待这条趋势线的突破，然后我们可以从那里继续</li></ul></li></ul><h2 id="Apri-5-2024"><a href="#Apri-5-2024" class="headerlink" title="Apri 5 2024"></a>Apri 5 2024</h2><p> 双重顶，未来几周会在800左右。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>视频分析了Nvidia股票的日线和周线图，预测未来可能的走势。分析显示，尽管股价跌破短期上升趋势线，但整体上升趋势仍在继续。分析师预计股价将在800-900美元区间震荡，最终可能会再次上涨。长期来看，股价可能会达到历史最高点，但也可能会回落至通道底部。</p><p><img src="https://s.draftai.cn/vent/20240405205830.png" alt="image.png"><br> 阻力位：957  支持位 857<br><img src="https://s.draftai.cn/vent/20240405210153.png" alt="image.png"><br>在800左右盘整，让EMA追上 ，再涨，非常牛市。<br>从月线来看，可能有到300多的大回调。<br><img src="https://s.draftai.cn/vent/20240405210848.png" alt="image.png"></p><h3 id="亮点-2"><a href="#亮点-2" class="headerlink" title="亮点"></a>亮点</h3><ul><li><p>📉 股价跌破短期上升趋势线，可能会继续下跌至下一个支撑位。</p></li><li><p>📈 股价在800-900美元区间震荡，预计最终会再次上涨。</p></li><li><p>📊 周线图显示股价可能会加速下跌，但长期趋势仍然看涨。</p></li><li><p>📈 股价可能会在短期内回落至通道底部，但长期趋势仍然看涨。</p></li><li><p>技术分析Nvidia股票00:01</p><ul><li>分析Nvidia股票的日线图和周线图</li><li>价格未能突破高点，跌破短期趋势线</li><li>可能向中800美元附近的支撑线下跌</li></ul></li><li><p>价格区间交易00:33</p><ul><li><strong>价格在957-958美元处遇到阻力，在857美元处有支撑</strong></li><li>预计Nvidia股票将在一定范围内交易</li></ul></li><li><p>价格可能会横向移动01:09</p><ul><li>价格可能会横向移动至趋势线附近</li><li>一旦价格接近趋势线，可能会再次上涨</li></ul></li><li><p>周线图分析02:12</p><ul><li>价格可能会加速下跌，但目前仍处于上升趋势</li><li>价格可能会在EMA附近交易，等待EMA追上后再次上涨</li></ul></li><li><p>长期趋势分析04:00</p><ul><li>从月线图看，价格可能会有较大的回调</li><li>价格可能会在通道顶部附近交易，未来可能向下回调</li></ul></li><li><p>总结与展望04:35</p><ul><li>目前Nvidia股票整体仍处于牛市</li><li>预计未来可能会有横向交易，但需关注价格走势</li></ul></li></ul><h2 id="Mar-27-2024"><a href="#Mar-27-2024" class="headerlink" title="Mar 27 2024"></a>Mar 27 2024</h2><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>关于NVIDIA股票技术分析的讲解。视频中提到，NVIDIA股票价格近期表现非常强劲，呈现出上涨的趋势。目前价格已经接近历史最高点，预计会再次突破该点位并创造新的历史高点。视频还提到，在图表上有很多隐藏的信息，比如停损位和潜在的买卖点位。总体而言，NVIDIA股票在短期内看涨，预计会有进一步上涨的趋势。<br>从Log图来看，要测试1200的高点。</p><h3 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li><p>💹 价格走势：NVIDIA股票价格近期呈现强劲上涨趋势，接近历史最高点。</p></li><li><p>📈 突破：预计NVIDIA股票会再次突破历史最高点，并创造新的历史高点。</p></li><li><p>📊 图表分析：图表上有很多隐藏的信息，包括停损位和潜在的买卖点位。</p></li><li><p>📉 长期走势：NVIDIA股票自2022年10月以来一直呈现出上涨趋势。</p></li><li><p>📈 短期预测：预计NVIDIA股票在短期内会继续上涨。</p></li><li><p>💰 市场决定：市场决定了NVIDIA股票的价格，目前市场认为股票价值较高。</p></li><li><p>每日图表分析00:01</p><ul><li>价格一度看似将大幅下跌，但实际上价格表现强劲，准备创下新的历史高点。</li><li>空头陷入困境，有许多止损位于高位，市场可能会试图触及这些止损位。</li><li>预计价格将形成双顶，可能突破1000美元心理关口，然后再次反转。</li></ul></li><li><p>周线图表分析02:13</p><ul><li>自2022年10月以来，股价一直呈现强劲上涨趋势，预计短期内将继续上涨。</li><li>股价走势非常牛市，难以画出通道，但可以绘制趋势线来显示上涨势头。</li><li>在周线图上，连续多个绿色蜡烛显示出强劲的上涨势头，预计价格将继续攀升。</li></ul></li><li><p>市场估值和展望04:30</p><ul><li>市场决定了股价，当前市场看涨态势，预计价格将继续上涨。</li><li>无论市场估值是否合理，关键在于市场的判断，目前市场看好NVIDIA的价格走势。</li><li>预计NVIDIA股价短期内将继续上涨，市场始终是正确的</li></ul></li></ul><h2 id="Mar-19-2024"><a href="#Mar-19-2024" class="headerlink" title="Mar 19 2024"></a>Mar 19 2024</h2><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>这段视频是关于NVIDIA Corporation（股票代码：NVDA）的技术分析。视频中提到，根据日线图和周线图的走势，NVIDIA的股价可能会出现短期和中长期的回调。视频中还提到了一些技术指标和趋势线，暗示了股价可能的下跌趋势。虽然不能保证股价会出现回调，但根据图表分析，目前的走势显示出了一些警示信号。</p><h3 id="Highlights-1"><a href="#Highlights-1" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>💡 NVIDIA的股价可能已经达到短期高点，并有可能出现回调。</li><li>💡 股价在日线图上已经超出了移动平均线，暗示了可能的下跌趋势。</li><li>💡 根据趋势线的绘制，股价可能会回落到784美元的突破点。</li><li>💡 周线图显示股价已经连续上涨多个周期，但也存在一个未填补的跳空缺口，这也可能成为股价回调的目标之一。</li><li>💡 长期图表显示股价处于上升通道的高位，暗示了可能的回调趋势。</li><li>💡 使用对数图表进行技术分析，也显示出股价可能处于上升通道的高位，并有可能向下回落。</li><li>💡 总结来看，短期内股价走势看跌，中长期内可能出现更大的回调。</li></ul><h2 id="Mar-13-2024"><a href="#Mar-13-2024" class="headerlink" title="Mar 13 2024"></a>Mar 13 2024</h2><h3 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h3><p>在这个视频中，V trades对英伟达公司的股票进行了技术分析。他首先分析了日线图，然后分析了周线图。他注意到，尽管英伟达股票价格近期上涨，但现在看起来像是一个短期的顶部，可能会有更多的下行或回调，可能回调到788美元的水平。他认为，价格可能会回到这个水平进行测试，因为看起来我们已经超过了黄色的上涨通道的顶部。此外，他还指出，周线图显示出一个非常大的周K线，这可能预示着一个顶部。</p><h3 id="亮点-3"><a href="#亮点-3" class="headerlink" title="亮点"></a>亮点</h3><ul><li>[📈] V trades对英伟达公司的股票进行了技术分析，首先分析了日线图，然后分析了周线图。</li><li>[🔝] 尽管英伟达股票价格近期上涨，但V trades认为现在看起来像是一个短期的顶部。</li><li>[📉] V trades预计可能会有更多的下行或回调，可能回调到788美元的水平。</li><li>[⚠️] 他指出，价格可能会回到这个水平进行测试，因为看起来我们已经超过了黄色的上涨通道的顶部。</li><li>[📊] V trades还指出，周线图显示出一个非常大的周K线，这可能预示着一个顶部。</li></ul><h3 id="关键词"><a href="#关键词" class="headerlink" title="关键词"></a>关键词</h3><p>#英伟达 #股票分析 #技术分析</p><h5 id="Key-Moments-beta"><a href="#Key-Moments-beta" class="headerlink" title="Key Moments (beta)"></a>Key Moments (beta)</h5><ul><li><p>每日图表分析00:01</p><ul><li>价格朝着上周的目标继续上涨，几乎达到了1000美元。</li><li>价格可能会回落至788美元水平，因为出现了超买迹象。</li><li>可能会看到继续下跌，最终测试高700美元水平。</li></ul></li><li><p>每周图表分析01:45</p><ul><li>出现了一根巨大的周线蜡烛，可能预示着顶部。</li><li>看起来价格过度上涨，可能会出现短期回调。</li><li>今年以来价格一直上涨，可能会迎来短期回调。</li></ul></li></ul><h2 id="Mar-3-2024"><a href="#Mar-3-2024" class="headerlink" title="Mar 3 2024"></a>Mar 3 2024</h2><h3 id="概要-1"><a href="#概要-1" class="headerlink" title="概要"></a>概要</h3><p>V交易员在YouTube上进行了Nvidia股票的技术分析。他首先审视了每日图表，然后结束时查看了每周图表。他注意到，尽管Nvidia的股价已经达到了他设定的目标，但这并不意味着股价已经达到顶峰，他预测股价还会继续上涨。他警告说，尽管现在的图表看起来很乐观，但投资者应该记住，股价可能会大幅下跌，就像它快速上涨一样。</p><h3 id="亮点-4"><a href="#亮点-4" class="headerlink" title="亮点"></a>亮点</h3><ul><li><p>[🔮] 尽管Nvidia的股价YTD已经62%的涨幅，但仍在在上升趋势通道中。财报前经历了一次回调后，预测股价还会继续上涨，下一个目标价会是929美金。</p></li><li><p>[⚠️] 他警告说，即使现在的图表看起来很乐观，投资者也应该记住，股价可能会大幅下跌。</p></li><li><p>[📉] 他指出，股价的下跌可能会很大，就像它快速上涨一样。</p></li><li><p>[💡] 他建议，如果你是一名交易者，现在可能不是购买Nvidia股票的最佳时机，因为这可能是一个低概率的交易。</p></li><li><p>每日图表分析00:01</p><ul><li>技术分析是价格分析，通过过去的走势来预测未来可能的走势。</li><li>NVIDIA股价在上升趋势通道中，经历了一次回调后，目标价位达到并继续上涨。</li><li>尽管股价已达到目标，但仍有可能继续上涨，下一个目标可能是进入900美元区间。</li><li><img src="https://s.draftai.cn/vent/20240303153906.png" alt="image.png"></li></ul></li><li><p>周度图表展望02:24</p><ul><li><p>NVIDIA股价在周度图表上已连续八周上涨，看起来越来越过热。<img src="https://s.draftai.cn/vent/20240303152953.png" alt="image.png"></p></li><li><p>尽管股价看起来过热，但仍有可能继续上涨，但需要警惕可能的大幅回调。回调一定会发生。股价可能会出现垂直上涨后的暴跌，需要谨慎观察。<img src="https://s.draftai.cn/vent/20240303153243.png" alt="image.png"></p></li></ul></li></ul><hr><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>V交易在YouTube上进行了对英伟达股票（股票代码NVDA）的技术分析。他首先查看了日线图，然后查看了周线图。他发现，尽管英伟达股票可能会有一些回撤，但整体上看，股价仍有上涨空间。他预测英伟达的目标价格可能在786附近，这一预测基于过去类似的股价走势。他还预测，尽管英伟达的业绩报告可能会出现好的结果，但股价可能会在短期内下跌，然后再继续上涨。</p><h3 id="亮点-5"><a href="#亮点-5" class="headerlink" title="亮点"></a>亮点</h3><ul><li>[📈] V交易进行了对英伟达（NVDA）股票的技术分析，首先查看了日线图，然后查看了周线图。</li><li>[🔍] 他发现，尽管英伟达股票可能会有一些回撤，但整体上看，股价仍有上涨空间。</li><li>[🎯] V交易预测英伟达的目标价格可能在786附近，这一预测基于过去类似的股价走势。</li><li>[📊] 他预测，尽管英伟达的业绩报告可能会出现好的结果，但股价可能会在短期内下跌，然后再继续上涨。</li><li>[⚠️] V交易警告说，尽管英伟达的股价可能还有上涨空间，但由于股价已经非常高，风险也相应提高。</li></ul><p>财报后会回调然后反弹，看到786，峰值后会有大幅回调</p><h3 id="概要-2"><a href="#概要-2" class="headerlink" title="概要"></a>概要</h3><p>Nvidia计划于2月21日星期三市场收盘后公布2024财年第四季度财报。本视频将解析投资者在Nvidia公布财报时应注意的关键数据，包括预期的指导、报告、收入等，并讨论投资者是否应在财报公布前购买Nvidia股票或等待财报公布后再购买。此外，视频还将探讨Nvidia股价在公司公布财报后可能会下跌、上涨还是基本保持不变。</p><h3 id="亮点-6"><a href="#亮点-6" class="headerlink" title="亮点"></a>亮点</h3><ul><li>[📅] Nvidia计划于2月21日公布2024财年第四季度财报。</li><li>[💰] Nvidia的股价自上次公布财报以来已从每股约500美元上涨到726美元。</li><li>[📈] Nvidia预计2024财年的收入将达到200亿美元左右，毛利润将达到75%，第四季度的运营支出将约为31.7亿美元。</li><li>[🔎] 分析师预计Nvidia在2024财年的收入增长将达到63.4%，在2025财年的收入增长将达到20.4%。</li><li>[📊] Nvidia在刚刚结束的第三季度实现了181亿美元的收入，同比增长206%，毛利润率从70%上升到74%，运营支出为29.8亿美元，同比增长16%。</li><li>[🔮] Nvidia在公布财报时，投资者不仅关注收入，还关注公司对2025财年第一季度的预期指导。</li><li>[💹] Nvidia的股票以27.3倍的预期市盈率交易，尽管这在过去一年中处于较高水平，但考虑到其强劲的前景，股票并不昂贵。</li><li>[🔄] 尽管Nvidia被评为最值得购买的股票之一，但建议投资者等待财报公布后再购买。</li></ul><p><img src="https://s.draftai.cn/vent/20240221105457.png" alt="image.png"></p><p><img src="https://s.draftai.cn/vent/20240221105509.png" alt="image.png"></p><p><img src="https://s.draftai.cn/vent/20240221105520.png" alt="image.png"></p><p><img src="https://s.draftai.cn/vent/20240221110518.png" alt="image.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Apr-21-2024&quot;&gt;&lt;a href=&quot;#Apr-21-2024&quot; class=&quot;headerlink&quot; title=&quot;Apr 21 2024&quot;&gt;&lt;/a&gt;Apr 21 2024&lt;/h2&gt;&lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;head</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="NVDA" scheme="http://example.com/tags/NVDA/"/>
    
    <category term="VTrade" scheme="http://example.com/tags/VTrade/"/>
    
    <category term="Stock" scheme="http://example.com/tags/Stock/"/>
    
  </entry>
  
  <entry>
    <title>未来十年最好的投资-顶级AI ETF-枫叶洞见EP17</title>
    <link href="http://example.com/2024/02/25/Top%20AI%20ETF%20for%202024%20-%20Just%20in%20Vent%20EP%2017/"/>
    <id>http://example.com/2024/02/25/Top%20AI%20ETF%20for%202024%20-%20Just%20in%20Vent%20EP%2017/</id>
    <published>2024-02-24T16:00:00.000Z</published>
    <updated>2025-04-02T08:13:12.907Z</updated>
    
    <content type="html"><![CDATA[<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>人工智能（AI）领域的投资机会前所未有，根据最新预测，未来十年AI的复合年增长率将达到42%。投资者可以通过交易所交易基金（ETF）广泛投资于AI领域。</p><p>过去10年表现最好的AI ETF，包括Invesco QQQ信托系列(QQQ)、Invesco Semiconductors ETF (PSI)、iShares美国科技ETF（IYW）、iShares半导体ETF（SOXX）和VanEck半导体ETF（SMH）等。</p><p>过去的表现不保证未来的表现，投资者应根据自己的需求和风险承受能力选择适合自己的ETF。</p><p>我们主要关注两类：</p><h2 id="科技股"><a href="#科技股" class="headerlink" title="科技股"></a>科技股</h2><h3 id="Invesco-QQQ信托系列-QQQ"><a href="#Invesco-QQQ信托系列-QQQ" class="headerlink" title="Invesco QQQ信托系列(QQQ)"></a>Invesco QQQ信托系列(QQQ)</h3><ul><li>非常知名的ETF，专注于投资纳斯达克的大型公司，包括苹果、微软等。拥有较低的费用比率和较稳定的回报。资产规模2390亿美元，管理费用0.2%。5年年化回报率20.73%，10年年化回报率18.11%，近五年最大回撤为 22年的 -32.58%。</li><li><strong>具体回报率如下图：</strong><img src="https://s.draftai.cn/vent/20240225204918.png" alt="image.png"></li><li><strong>主要持仓：</strong><img src="https://s.draftai.cn/vent/20240225213016.png" alt="image.png"></li><li>**对比标普500 和 纳斯达克指数，过去5年的走势: **<img src="https://s.draftai.cn/vent/20240225213249.png" alt="image.png"></li></ul><h3 id="iShares美国科技ETF-IYW"><a href="#iShares美国科技ETF-IYW" class="headerlink" title="iShares美国科技ETF(IYW)"></a>iShares美国科技ETF(IYW)</h3><ul><li><p>旨在为投资者提供投资美国电子、计算机软硬件和信息技术公司的机会。资产规模155亿美元，管理费用0.4%。5年年化回报率24.46%，10年年化回报率20.23.11%，近五年最大回撤为 22年的 -34.83%。</p></li><li><p>**对比标普500 和 纳斯达克指数，过去5年的走势: **<img src="https://s.draftai.cn/vent/20240225220244.png" alt="image.png"></p></li><li><p><strong>过去五年，比QQQ也要强一些：</strong><img src="https://s.draftai.cn/vent/20240225224217.png" alt="image.png"></p></li></ul><h2 id="半导体"><a href="#半导体" class="headerlink" title="半导体"></a>半导体</h2><h3 id="Invesco-半导体ETF-PSI"><a href="#Invesco-半导体ETF-PSI" class="headerlink" title="Invesco 半导体ETF(PSI)"></a>Invesco 半导体ETF(PSI)</h3><ul><li>专注于投资半导体行业的公司，持仓包括Nvidia、AMD、Micron和Marvel等。资产规模7.21亿美金，管理费用0.5%。5年的年化回报率为24.9%，10年的年化回报率为28.9%，近五年最大回撤为 22年的 -34.43%。</li><li><strong>具体回报率如下图：</strong> <img src="https://s.draftai.cn/vent/20240225192942.png" alt="image.png"></li><li>**主要持仓如下: **<img src="https://s.draftai.cn/vent/20240225194034.png" alt="image.png"></li><li><strong>对比标普500 和 纳斯达克指数，过去5年的走势:</strong> <img src="https://s.draftai.cn/vent/20240225204319.png" alt="image.png"></li></ul><h3 id="VanEck半导体ETF-SMH"><a href="#VanEck半导体ETF-SMH" class="headerlink" title="VanEck半导体ETF(SMH)"></a>VanEck半导体ETF(SMH)</h3><ul><li><strong>主要投资半导体公司</strong>,资产规模132亿美元，管理费用0.35%。5年年化回报率32.10%，10年年化回报率26.09%，近五年最大回撤为 22年的 -33.52%。</li><li><strong>具体回报率如下图：</strong><img src="https://s.draftai.cn/vent/20240225215122.png" alt="image.png"></li><li><strong>主要持仓：</strong><img src="https://s.draftai.cn/vent/20240225215023.png" alt="image.png"></li><li><strong>对比标普500 和 纳斯达克指数，过去5年的走势:</strong> <img src="https://s.draftai.cn/vent/20240225215341.png" alt="image.png"></li></ul><h3 id="iShares半导体ETF-SOXX"><a href="#iShares半导体ETF-SOXX" class="headerlink" title="iShares半导体ETF(SOXX)"></a>iShares半导体ETF(SOXX)</h3><ul><li>专注于投资于美国的设计、制造和分销半导体的公司。SOXX 的资产规模约为 123.76 亿美元。该基金成立于 2001 年 7 月 10 日。管理费用0.35%。5年年化回报率29.08%，10年年化回报率26.09%，近五年最大回撤为 22年的 -35.09%。</li><li>**具体回报率如下图：<img src="https://s.draftai.cn/vent/20240225225329.png" alt="image.png"></li><li><strong>主要持仓：</strong><img src="https://s.draftai.cn/vent/20240225225422.png" alt="image.png"></li><li><strong>对比标普500 和 纳斯达克指数，过去5年的走势:</strong><img src="https://s.draftai.cn/vent/20240225225545.png" alt="image.png"></li><li><strong>对比其它两个半导体ETF</strong><img src="https://s.draftai.cn/vent/20240225225730.png" alt="image.png"></li></ul><p>人工智能（AI）是一次千载难逢的投资机会，未来十年AI的复合年增长率预计为42%，因此投资者有很多机会获得巨大的投资收益。投资AI的最简单方法之一是通过交易所交易基金（ETF）。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h3&gt;&lt;p&gt;人工智能（AI）领域的投资机会前所未有，根据最新预测，未来十年AI的复合年增长率将达到42%。投资者可以通过交易所交易基金（ETF）广泛投资</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="etf" scheme="http://example.com/tags/etf/"/>
    
  </entry>
  
  <entry>
    <title>英伟达2023年Q4财报前瞻-枫叶洞见EP16</title>
    <link href="http://example.com/2024/02/20/NVDA%202023Q4%20ER%20Forward-Just%20in%20Vent%20EP%2016/"/>
    <id>http://example.com/2024/02/20/NVDA%202023Q4%20ER%20Forward-Just%20in%20Vent%20EP%2016/</id>
    <published>2024-02-19T16:00:00.000Z</published>
    <updated>2024-02-20T09:42:33.049Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TLDR"><a href="#TLDR" class="headerlink" title="TLDR"></a>TLDR</h3><p>随着AI周期的启动，英伟达成为整个美股市场最靓的仔，今年以来股价上涨了47%，在过去12个月中上涨了约252%，市值超过1.7万亿美金，超过亚马逊和谷歌成为全球市值第三大公司。</p><p>尽管在这种惊人的上涨幅度，依然有大量分析师对NVDA今年的股价目标进行了上调，基本从600<del>650的阶段，上调到800</del>850。Loop Capital甚至将NVDA的目标价定到1200的高位。</p><p>然而不可忽视的是，由于大量做多期权建仓，隐含波动率太高。股权一旦进入实值阶段，波动率下行，会带来强大抛压。</p><p>除此以外，有数据分析指出，英伟达的上涨对美股指数的上涨贡献率达到35%。如果NVDA的财报不能大幅超出预期，估计美股市场的涨幅不可持续，可能看到比较剧烈的回调发生。</p><p><img src="https://s.draftai.cn/vent/202402201556157.png" alt="NVDA 1Y Price"></p><h3 id="分析师预期"><a href="#分析师预期" class="headerlink" title="分析师预期"></a>分析师预期</h3><p>根据公开数据显示，分析师普遍预测，Nvidia第四季将实现<strong>203.7亿美元营收</strong>，去年同期60.5亿营收。（恐怖啊，五倍的增长），且2024年的收入预期高达940亿美元。每股利润为<strong>4.3美元</strong>，去年同期0.88美元。<br><img src="https://s.draftai.cn/vent/202402201603542.png"></p><p>然而SpotGamma的分析师认为：市场对NVDA的财报预期已经基本Price in，NVDA财报后股价想维持涨幅，需要实现<strong>260亿</strong>营收才行，这意味着英伟达需要<strong>大超预期</strong>，才有可能保持涨幅。</p><p>与之同时，木头姐觉得英伟达是周期性股票，受库存和竞争影响。最近出售了价值450万美元的股票。</p><h3 id="对美股市场的冲击"><a href="#对美股市场的冲击" class="headerlink" title="对美股市场的冲击"></a>对美股市场的冲击</h3><p>可以看到纳斯达克100在过去一年涨幅也高达43%，有数据分析指出，英伟达的过去一段时间对NQ的涨幅贡献率高达35%，可以预想，如果英伟达财报如果不够理想，将会给美股带来较大的波动性。<br><img src="https://s.draftai.cn/vent/202402201646114.png"></p><p>Reddit上经典的图：<br><img src="https://s.draftai.cn/vent/nvda-earnings-next-week-v0-8mrz3ogrl6jc1.webp"></p><h3 id="业务情况"><a href="#业务情况" class="headerlink" title="业务情况"></a>业务情况</h3><p>虽然英伟达股价已经大涨，然后它的股价其实估值不贵。因为业务情况太好了。分析师认为这种增长是有道理的，因为公司今年的收入可能会增加一倍或更多。<br>概述列举下NVDA的业务情况：</p><ul><li>🆕 Nvidia有多个新产品周期即将来临，包括为中国市场推出新产品，以及在今年下半年推出名为Blackwell的全新架构。</li><li>💡 Nvidia芯片供应不顺瓶颈逐步缓解，尤其最缺的Nvidia AI芯片交货周期由先前的8到11个月，缩短为3至4个月</li><li>🤝 Nvidia与合作伙伴们建立了多个合作伙伴关系，包括与Cisco、AWS、Genentech等的合作</li><li>🌍 Nvidia可能会寻找新市场，如主权国家AI。</li><li>🚀 Super Micro的股价今年从95美元上涨到了昨天收盘时的800美元以上，而这家公司的大部分收入都依赖于Nvidia。Arm的股价也飙升，而Nvidia是其AI投资者之一。</li></ul><h3 id="分析师结论"><a href="#分析师结论" class="headerlink" title="分析师结论"></a>分析师结论</h3><h4 id="正面"><a href="#正面" class="headerlink" title="正面"></a>正面</h4><ul><li>部分分析师，认为使用贴现现金流模型进行估值，得出内在价值为<strong>795美元</strong>。较现在价格还有较大的上行空间。认为Nvidia的股价还将上涨21%。</li><li>部分分析师，提到超过2&#x2F;3的科技基金买入了NVDA的股票，预计在盈利报告发布后将有大幅上涨。因为英伟达在卖出一切它所能制造的东西(芯片的超级供不应求)</li></ul><h4 id="负面"><a href="#负面" class="headerlink" title="负面"></a>负面</h4><ul><li>SpotGamma 创始人认为，无论NVDA 财报好坏，美股涨势都会终结。因为当前期权市场隐含波动率太高，期权进入实值状态隐含波动率就会下降。看涨期权偏度较高的抛压更高。</li><li>他从期权数据和股票数据给出NVDA技术面分析：500大阻力位转成现在的大支撑位， 640阻力位，支撑位。 695小支撑位，750阻力位。</li></ul><p><img src="https://s.draftai.cn/vent/20240219231202.png" alt="image.png"></p><ul><li>木头姐觉得英伟达是周期性股票，受库存和竞争影响。最近出售了价值450万美元的英伟达股票。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;TLDR&quot;&gt;&lt;a href=&quot;#TLDR&quot; class=&quot;headerlink&quot; title=&quot;TLDR&quot;&gt;&lt;/a&gt;TLDR&lt;/h3&gt;&lt;p&gt;随着AI周期的启动，英伟达成为整个美股市场最靓的仔，今年以来股价上涨了47%，在过去12个月中上涨了约252%，市值超过1.</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="NVDA" scheme="http://example.com/tags/NVDA/"/>
    
  </entry>
  
  <entry>
    <title>科技早知道-出海年终总结4-播客摘要</title>
    <link href="http://example.com/2024/01/30/technew-podcast-oversea%20invest/"/>
    <id>http://example.com/2024/01/30/technew-podcast-oversea%20invest/</id>
    <published>2024-01-29T16:00:00.000Z</published>
    <updated>2024-02-04T04:30:31.648Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>日本</strong>走出30年的通缩，股价已经连续涨了十几年了。优衣库母公司涨薪，代表性样例，季度通胀4%。美国开始和日本合作供应链，全球战略转移。</li><li><strong>印度</strong>，人力比较多，美国供应链转移，苹果等精密加工转移。距离中国远，地缘政治上适合供应链安全的概念。孟买指数过去一年30%，过去20年20倍。主要是银行和工业。制造业企业大涨，银行是配套，美国供应链转移。过去10年一直涨，印度人信心强，预期好。印度底子，人口结构优秀，劳动力占比高，价格低廉，PMI一直在扩张区。外资一直在进入，政策稳定，铁了心振兴制造业。印度具备成为下一个世界制造工厂的底子，美国供应链转移最大赢家。</li><li><strong>墨西哥</strong>，友岸外包，汽车业发展比较好（美国电动车补贴行政令）。很多国内企业迁移过去，美国刻意扶持。股市由4~5家家族垄断，股市不算活跃，IPO几乎为零。墨西哥主要是债务融资。不是世界工厂，最多是美洲工厂。</li><li><strong>越南</strong>不行，主要靠房地产和金融，强监管和反腐败，22年大挫越南胡志明指数，23年恢复不如人意，甚至外资流出，不算新兴市场，只算前沿市场，透明度不够。没有发展出精密加工等，只是中国的低端加工外溢。不如墨西哥和印度。</li><li><strong>俄罗斯</strong>，不要进。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;日本&lt;/strong&gt;走出30年的通缩，股价已经连续涨了十几年了。优衣库母公司涨薪，代表性样例，季度通胀4%。美国开始和日本合作供应链，全球战略转移。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;印度&lt;/strong&gt;，人力比较多，美国供应链转移，苹果等精</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="pocast" scheme="http://example.com/tags/pocast/"/>
    
  </entry>
  
  <entry>
    <title>Obsidian-你的宝石笔记</title>
    <link href="http://example.com/2024/01/26/Obsidian-Your%20Gem%20Note%20App/"/>
    <id>http://example.com/2024/01/26/Obsidian-Your%20Gem%20Note%20App/</id>
    <published>2024-01-25T16:00:00.000Z</published>
    <updated>2024-01-26T09:10:01.627Z</updated>
    
    <content type="html"><![CDATA[<p>Obsidian是我的年度最佳笔记软件，不可替代的宝石，推荐给你们。</p><p>先来看看它官方的介绍：<strong>Obsidian</strong>是一款私密且灵活的笔记APP。这张官网示意图展现了它的几个核心功能：文件链接、知识图谱、多端支持等</p><p><img src="https://s.draftai.cn/vent/202401251528441.png"></p><h2 id="完全掌控"><a href="#完全掌控" class="headerlink" title="完全掌控"></a>完全掌控</h2><h3 id="文件私有"><a href="#文件私有" class="headerlink" title="文件私有"></a>文件私有</h3><p>你的笔记数据完全属于你自己。在Obsidian中，你的笔记文件实际上就是一个非常简单的Markdown(可以理解成带格式的TXT)，存储在你自己的硬盘或者你指定的云存储上。</p><p>在今时今日，笔记软件们，要么越卖越贵，要么跑路删档。比如10多年前就开始使用的EverNote，变得又昂贵且臃肿难用。还有一个最核心的是，你的笔记想从中导出，还非常困难。Obsidian这种能够完全掌控自己数据的软件并不多见。</p><p>Obsidian的出现，有点古典复兴的味道。虽然它也有收费的功能，但是主要是云同步和云发布，核心的笔记功能反而是免费的。</p><h3 id="界面简洁"><a href="#界面简洁" class="headerlink" title="界面简洁"></a>界面简洁</h3><p>你只需要关注写作，只需关注写作的心流而不是其他事情。</p><h3 id="功能灵活"><a href="#功能灵活" class="headerlink" title="功能灵活"></a>功能灵活</h3><p>Obsidian有强大的插件体系。你的笔记功能也由你来定制，如你心愿，保持极简还是变得强大。</p><h2 id="优雅且强大"><a href="#优雅且强大" class="headerlink" title="优雅且强大"></a>优雅且强大</h2><h3 id="Theme-主题"><a href="#Theme-主题" class="headerlink" title="Theme(主题)"></a>Theme(主题)</h3><p>有两百多个非常漂亮的主题，我自己最喜欢Things。<br><img src="https://s.draftai.cn/vent/202401251524017.png"></p><h3 id="Link-文件链接"><a href="#Link-文件链接" class="headerlink" title="Link(文件链接)"></a>Link(文件链接)</h3><p>可以看到文件链接是实现了对笔记的内部引用，还有预览功能。<br><img src="https://s.draftai.cn/vent/20240126082944.png" alt="image.png"></p><h3 id="Graph-知识图谱"><a href="#Graph-知识图谱" class="headerlink" title="Graph(知识图谱)"></a>Graph(知识图谱)</h3><p>通过可视化的方式，将你所有的笔记的内在关系展现出来。可以清晰看到你的笔记的重点，它们之间的链接。<br><img src="https://s.draftai.cn/vent/202401251623601.png"></p><p>放大来看看：<br><img src="https://s.draftai.cn/vent/202401251624673.png"></p><h3 id="Canvas-笔记画布"><a href="#Canvas-笔记画布" class="headerlink" title="Canvas(笔记画布)"></a>Canvas(笔记画布)</h3><p>可以将一个主题下的笔记，都通过画布联系起来，能够当做白板来用。也可以当做思维导图来用，只取决于你的脑洞。</p><p><img src="https://s.draftai.cn/vent/202401261659703.png"></p><h2 id="还有更多"><a href="#还有更多" class="headerlink" title="还有更多"></a>还有更多</h2><p><img src="https://s.draftai.cn/vent/20240126084801.png" alt="image.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Obsidian是我的年度最佳笔记软件，不可替代的宝石，推荐给你们。&lt;/p&gt;
&lt;p&gt;先来看看它官方的介绍：&lt;strong&gt;Obsidian&lt;/strong&gt;是一款私密且灵活的笔记APP。这张官网示意图展现了它的几个核心功能：文件链接、知识图谱、多端支持等&lt;/p&gt;
&lt;p&gt;&lt;i</summary>
      
    
    
    
    <category term="PKM" scheme="http://example.com/categories/PKM/"/>
    
    
    <category term="obsidian用法" scheme="http://example.com/tags/obsidian%E7%94%A8%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>深入分析英伟达-202401-成长与担忧✨-枫叶洞见EP13</title>
    <link href="http://example.com/2024/01/13/Dive%20deep%20into%20Nvidia-202401-Just%20In%20Vent-EP13/"/>
    <id>http://example.com/2024/01/13/Dive%20deep%20into%20Nvidia-202401-Just%20In%20Vent-EP13/</id>
    <published>2024-01-12T16:00:00.000Z</published>
    <updated>2024-01-26T09:00:51.006Z</updated>
    
    <content type="html"><![CDATA[<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>由于英伟达销售增长极其强劲，使得PE等一系列财务指标来看，当前股价依然便宜。大部分分析师高看到600美金美股以上，中位数在700美金，最高有看到1200美金的。</li><li>AI还处于早期扩张阶段。对英伟达芯片需求依然非常强盛，依然处于供不应求阶段。未来十年都值得关注。</li><li>NVDA的不利因素：一季度可能出现营收下滑和产品价格昂贵用户会转向AMD等代替品</li></ul><h3 id="先看截止到2024-01-12的数据"><a href="#先看截止到2024-01-12的数据" class="headerlink" title="先看截止到2024-01-12的数据"></a>先看截止到2024-01-12的数据</h3><p><img src="https://s.draftai.cn/vent/20240113173220.png" alt="image.png"></p><h3 id="Grey-Black"><a href="#Grey-Black" class="headerlink" title="Grey Black"></a>Grey Black</h3><blockquote><p>需要注意，这些指标都是一个维度。</p></blockquote><p><a href="https://twitter.com/search?q=$NVDA&src=cashtag_click">$NVDA</a>：投资者开始意识到，在26.5倍的2024年预调整每股收益（adj eps）和2024至2028年预期每年复合增长率（CAGR）达28%的情况下，<a href="https://twitter.com/search?q=$NVDA&src=cashtag_click">$NVDA</a> 的估值仍然看起来非常便宜。考虑到其一流的产品、出色的执行力、强大的管理团队、没有戏剧性事件以及低头条风险，这使得其吸引力加大。</p><p><img src="https://s.draftai.cn/vent/20240110221241.png" alt="image.png"></p><h3 id="Shay-Boloor"><a href="#Shay-Boloor" class="headerlink" title="Shay Boloor"></a>Shay Boloor</h3><p>听说过PEG比率吗？这是彼得·林奇非常青睐的一个衡量标准。如果PEG比率小于1.0，它暗示这个股票可能是个便宜货；而如果比率大于2.0，可能就有点贵了</p><p><img src="https://s.draftai.cn/vent/20240110221424.png" alt="image.png"></p><h3 id="陆行之关于半导体的判断"><a href="#陆行之关于半导体的判断" class="headerlink" title="陆行之关于半导体的判断"></a>陆行之关于半导体的判断</h3><p>陆行之是世界顶尖的半导体分析师，他近期的看法值得关注。</p><blockquote><p> 在我最近接受《今周刊》的专访中，我预测了2024年将会有一个“飙升”的趋势。许多粉丝和朋友对此表示疑惑，质疑在基本面仍旧不佳的情况下，为什么半导体股票表现如此强劲。我解释说，股价在一年前已经因为糟糕的基本面多次触底，现在的上涨是对明年基本面复苏的预期反应。这是否准确，将取决于未来半年内各方对产业的深入研究。半年前，一位半导体小公司的总经理曾询问我应该做空哪家公司，因为基本面实在太差。我劝阻了他，而他至今还没有请我吃饭表示感谢。</p></blockquote><ul><li><p>创下10年新高的领头羊包括：Nvidia、Amkor（一个之前不被关注的公司，因为抢到了一些原本由台积电独家处理的CoWoS业务）、KLA、Broadcom、Monolithic Power、Rambus、Lam Research、ADI。</p></li><li><p>与半导体指数表现相近、即将创新高的公司有：德州仪器、AMD、Axcelis Technologies、应用材料、Microchip、NXP Semi。</p></li><li><p>而需要管理层反思和可能迎头赶上的公司包括：Qorvo、Skyworks、Intel、Entegris、Marvell、Wolfspeed、Globalfoundries、On Semi、Qualcomm、Teradyne、Coherent、ASML、Micron、Lattice、TSM。</p></li></ul><p>汽车和工业芯片需求的爆发仅比其他行业晚了两到三个季度，因此反弹也将相应延后。他的这一观点给我留下了深刻印象。我还记得在2022年12月，我在一个视频中分享过他的观点。他当时认为芯片行业的低谷已经过去，市场总是提前两到三个季度进行预期。因此，去库存的担忧已经被市场消化。事实证明，他是正确的。他对2024年的展望是，年增长率将达到13%，出货量增长将在8%至10%之间。</p><h3 id="陆行之关于NVDA的判断"><a href="#陆行之关于NVDA的判断" class="headerlink" title="陆行之关于NVDA的判断"></a>陆行之关于NVDA的判断</h3><p>关于英伟达，陆行知在11月份财报发布后提出了三个担忧：<br>一是好消息可能已经反映在股价中；<br>二是明年第一季度是否会出现营收下滑；<br>三是由于产品价格昂贵（成本的九倍），客户是否会转向AMD的MI300X。<br>网络上的测评显示，MI300X在性能上已超过H100，并与英伟达即将发布的H200相当，但价格更便宜。</p><p>他在另一篇文章中也提到，他们对英伟达明年的营收预期比彭博的统计高出30%以上，认为彭博对2024年英伟达的营收展望过于悲观。</p><p><img src="https://s.draftai.cn/vent/20240106143946.png" alt="image.png"></p><h3 id="美国银行"><a href="#美国银行" class="headerlink" title="美国银行"></a>美国银行</h3><p>美国银行（Bank of America）最新发布的一份报告显示，随着英伟达（Nvidia）继续利用其在开发和销售人工智能芯片方面的成功，该公司去年股价的大幅上涨可能会延续到2024年。</p><p>该行表示，英伟达快速增长的收入和利润状况将使其在未来两年产生1,000亿美元的自由现金流。相比之下，英伟达过去两年的自由现金流还不到300亿美元。</p><p>美国银行投资总监 Arya周五在接受采访时表示，大量预期的自由现金流“只是一块非常美味的蛋糕上的樱桃”。这个蛋糕的层次包括生成式人工智能的早期周期、英伟达强大的竞争地位，以及即将推出的新人工智能芯片。</p><p>“这些周期不会在一年内开始和结束。这可能是一个长达十年的周期，前期硬件部署至少需要三到四年的时间，而我们才刚刚进入第二年。”他说。</p><p><strong>Arya重申了他对英伟达的“买入”评级和700美元的目标价</strong>，这意味着该公司股价较当前水平有42%的上涨潜力。他还说，迅速增长的现金储备应该有助于英伟达开始新的增长计划，这将大大提高其估值倍数。</p><p><img src="https://postimg.futunn.com/170452244829738609391.png?imageMogr2/ignore-error/1/format/webp"></p><p>“在约1,000亿美元的自由现金流中，我们估计只有约300亿至350亿美元可以用于回购，剩下的650亿至700亿美元可用于新的增长计划，”他补充道。</p><p>Arya表示，英伟达保持增长并保持高估值的关键将是其建立经常性收入来源的能力，因为根据2025年的盈利预期，英伟达对硬件销售的依赖是其市盈率仅为20倍的一个重要原因。这一估值远低于大型科技股同行，也低于其35倍至40倍的历史市盈率中位数。</p><p>他强调：“虽然英伟达被归为‘科技七巨头’，但我们注意到，<strong>尽管英伟达的自由现金流利润率是其他六家‘卓越’同行的两倍，销售额复合年增长率是其他六家同行的三倍，但其市盈率和企业价值与自由现金流的比率仍有20%至30%的折扣</strong>。”</p><p>但英伟达快速增长的现金储备应该会让它<strong>有机会继续快速增长，并提高其估值倍数</strong>。</p><p>“我们预计英伟达会考虑那些有助于创造更有意义的经常性收入的资产。虽然英伟达在人工智能领域拥有稳固的领先优势，但硬件导向型业务的价值并不高。”Arya说。</p><h3 id="老虎投资"><a href="#老虎投资" class="headerlink" title="老虎投资"></a>老虎投资</h3><p>今年，英伟达CEO“老黄”将ChatGPT的发布视为AI的“iPhone时刻”。“iPhone时刻”意味着一项革命性的技术实现突破，以此为起点，这项技术将在接下来的实际应用中实现爆发式的增长，引领人类社会和经济发展。</p><p>移动网络虽然在萌芽期就遭受次贷危机的重创，但自2009年3月起带领美股走出史上最长的一次牛市。期间大型科技股成为股市的主要动力，指数CAGR高达22.51%。</p><p>根据历史经验，假如AI技术在接下来的10年间真的能改变生产模式，那当前的增长或许只是个开始。<br><img src="https://s.draftai.cn/vent/202401111428100.png"></p><p>市场对AI概念的强预期，并不只是空中楼阁，甚至已落实到个股财务数据中。以英伟达为例，虽然今年股价翻了2倍，但受惠于AI芯片业务的收入爆发，盈利也实现了与股价同等规模的扩张，使得当前PE已经降低到了ChatGPT发布前的水平。如果引入分析师对明年英伟达收入的增长预期，Forward P&#x2F;E只有23倍左右，考虑AI的巨大前景，这一数值也并不算很高。可以说英伟达目前的估值并不单纯来自资金对概念的炒作，还有比较坚实的基本面支撑。</p><p><img src="https://s.draftai.cn/vent/202401111409265.png"></p><h3 id="一些其它信息"><a href="#一些其它信息" class="headerlink" title="一些其它信息"></a>一些其它信息</h3><p>全球大约有8,000个数据中心。<br>假设它们每个会购买16,000个英伟达的GPU（H100, GH200或更高级别的产品）<br>那么英伟达最初会售出总计1.28亿个GPU（这还不包括扩展和升级）。<br>英伟达在2023年大约售出了50万个H100，在2024年大约会售出200万个。<br>潜在的总需求是2024年出货量的64倍。<br>假设每个GPU的价格大约为30,000美元，那么在未来几年总销售额将达到3.84万亿美元。<br>詹森提到在未来四年内有1万亿美元用于升级数据中心的AI设施。按照3.84万亿美元计算，达到这个目标可能需要大约10-15年。这与Lisa Su预测的每年4000亿美元的数据相符。</p><p><img src="https://s.draftai.cn/vent/20240113173440.png" alt="image.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;由于英伟达销售增长极其强劲，使得PE等一系列财务指标来看，当前股价依然便宜。大部分分析师高看到600美金美股以上，中位数在700</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="ai" scheme="http://example.com/tags/ai/"/>
    
    <category term="NVDA" scheme="http://example.com/tags/NVDA/"/>
    
  </entry>
  
  <entry>
    <title>tesla and AI</title>
    <link href="http://example.com/2024/01/13/Elon%20Musk%20&amp;%20Cathie%20Wood%20On%20Tesla,%20AI,%20Optimus%20&amp;%20Dumb%20Investors/"/>
    <id>http://example.com/2024/01/13/Elon%20Musk%20&amp;%20Cathie%20Wood%20On%20Tesla,%20AI,%20Optimus%20&amp;%20Dumb%20Investors/</id>
    <published>2024-01-12T16:00:00.000Z</published>
    <updated>2025-04-03T03:09:30.883Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>这是一场关于特斯拉、人工智能和金融市场的讨论，Elon Musk和Cathie Wood分享了他们对特斯拉、公共市场和AI的看法。他们还讨论了金融市场的问题和未来的发展方向。</p><h3 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>[💡] 讨论了特斯拉在生产和交付车辆方面的困难以及股东和客户的参与</li><li>[📈] 提到了传统金融世界的局限性和公共股权市场的问题</li><li>[🚀] 讨论了AI和人工智能对经济增长的潜在影响</li><li>[🌍] 强调了多元文明化的重要性和将人类生活延伸到其他星球的愿景</li><li>[🤖] 讨论了自动驾驶汽车和星链卫星网络对未来的影响</li><li>[💪] 强调了创业者应该专注于提供有用的产品和服务，而不是追求所谓的”伟大挑战”。</li></ul><p>Elon Musk &amp; Cathie Wood On Tesla, AI, Optimus &amp; Dumb Investors - YouTube <a href="https://www.youtube.com/watch?v=mUQwT8yimIw">https://www.youtube.com/watch?v=mUQwT8yimIw</a> Transcript: (00:00) this is taking me back to early 2019 uh we were uh at Fremont uh doing a podcast with you when oh yeah remember when Tesla was in production hell with the model 3 and uh I think you were sleeping under your desk at that time uh and uh and you gave us a lot of time to to really help us understand the story and what was going to happen and uh the rest is history so um uh just want to to recount that do you think do you think that going back to like the model 3 ramp when you had customers and shareholders going out and facilitating delivery of vehicles do you think that could have happened without people having like skin (00:43) in the game in the same way yeah absolutely most most of the people who help were not shareholders that’s interesting I mean not that many shareholders um I don’t know you know Elon so many people have come up to me and all of us over the years because we were out there you know pounding the table uh for Tesla against you know against the traditional Financial world and they they read our research uh on Tesla and um and bought into it and so many people have come up and said thank you for doing that research alerting us to the opportunities what’s happened to the public markets uh Elon is I think uh (01:26) is I think they’re kind of broken I agree with you I think part part of is regulatory but part of it is the way the traditional Financial world work and I saw I’ve seen this time and again is if a company is not in a broad-based benchmark then then Financial analysts and uh portfolio managers very often say okay not relevant not relevant to me not in an index I’m not competing against it that’s how broken it is instead of focusing on the future and you know which companies are going to be trans formative and are going to scale brilliantly they’re very nicely situated in the past you know in stocks that are (02:08) where they are in benchmarks because of past performance that’s the other thing that’s happened here uh sure but it’s important we change that Elon and and Tesla has been a change maker in that regard sure you didn’t get into an index because of the way they construct them until Tesla was $500 billion dollar so a lot of it was pretty crazy you know how many more how many companies are bigger than that you know it’s just crazy but you did you you made it and you opened people’s eyes and we’re trying to change this shift away from passive uh back to active so that there is an efficient (02:45) allocation of capital yeah you’re you’re touching on a very important point which is that I think the the percentage of the market that is passive is simply is too great at this point um so at the end of the day somebody actually has to make an active decision um the passive invest s are rioting on the decisions of the active investors um and so the passive passive you know Investments like index funds whatnot are they’re like a an they’re just they’re an amplifier it’s like have you know um like an electric guitar amp or something so but but you you don’t want that amplifier gain to be (03:19) too high um or you you you get essentially massive movements of the stock uh based on the decisions of maybe four or five uh active major uh stock Pickers well you know actually it’s it’s it’s a a little more complicated so if you if you yeah if you if you look if you’re in a riskof market a bare market like we were in 22 what actually happens is these managers are very fearful and they just get closer and closer to their benchmarks and they sell the stocks that the kind of stocks that we’re invested in because we’re completely active we don’t we’re we’re Benchmark agnostic we (04:00) ju we just don’t even look at them we’re all about the future so that’s in a riskof market it punishes stocks that are not in indexes um more than otherwise would be the case and then to your point it rewards stocks that uh you know in a risk on period uh that uh that that are growing very quickly like yours and as you say yes there’s a pylon effect if they get into a benchmark now everybody has to consider them so it’s really it’s it it’s a really distorted way of um that the public Equity markets have evolved yeah I mean it seem getting into any kind of index will passive fund will I think increase the volatility (04:41) significantly um because uh you know just now it’s Amplified that stock is not Amplified by the by the passives um so you know and I think bog was was it was a good idea to to create U you know Vanguard and and and passives but it’s gone it’s just gone too far it has um so you just don’t want to have the amp the amp sort of the amp the amp is turned up too high as way to think of it um yeah so there should be more more active uh and less less passive totally agree and absolutely and the other thing that’s happening I didn’t know we were going to go here Elon but the other thing that really is gripping the markets is you (05:20) know 75% plus uh uh of trading every day um is algorithmic and and we see their very simple algorithm you know in a bare Market okay do they have cash are they burning cash uh and and those two variables will will dominate uh the the stock action so yeah we’ve watched this over the last three years it’s a very simplistic way of managing money and um and I I really do think it has it has led to the most massive misallocation of capital in history a lot more private companies should be public but we understand why they don’t go public because of the kind of behavior you’re subjected to and now you know all about it right I mean at at (06:06) SpaceX we never think about um what the quarter we never think about it and we don’t think we don’t think about the stock price so uh you know there’s um there’s a lot of pressure like immense pressure on a public company to not have a bad quarter so this can actually result in a less efficient operation where you you’re going to Great Lengths at the end of a quarter to not disappoint people um and um you know that’s just that’s just how how how it goes um so uh and and then you know you’ve also got a sort of a long-term challenge with public markets where a lot of the analysts following companies (06:40) uh have a Time Horizon that is maybe only a year or two um it’s you know rarely more than a few years um so and so they literally don’t care because then their incentive structure is a shortterm they do not care about what your long-term outcome will be because their career is dependent on how you do in the short term Elon you have been been great at I think standing up to short-term oriented shareholders that’s one of the scores in our scoring system as as we’re evaluating companies is this a Visionary management and will they stand up to short-term shareholders who want their profits now their dividends (07:18) their share repurchases will they invest aggressively enough in the future and I think you’ve done a great job uh well thank you um and credit to the Tesla team as well um so but that that said we at Tesla we do um go to Great Lengths to ensure that our quarter is is good as um that that we don’t disappoint because it’s it’s not so much about the it’s just that we feel like we have a sort of you know moral obligation to you know I don’t know that to to not have a bad quarter um and and disappoint people so we you know the the number of New Year’s EES that I’ve spent in delivery centers (07:53) is like I don’t know six I don’t know seven I don’t know I’ve less count how many times I was in I was delivering cars until like basically midnight on Year’s Eve personally so um you know um people at the company so I believe in in in um providing stock to the people at the company the employees and creators of the company it’s important to provide them stock to have the incentive be aligned with the company uh outcome and uh and then you know you they at various points want to gain liquidity so that you you want to then establish an outside valuation um and you know so it’s not it’s not just me deciding it (08:28) and and able liquidity for those who have been awarded stock and options at the companies and and it’s generally been my approach to award everyone at the company stock um you know Tesla we’ve awarded uh we awarded everyone uh stock um you know matter how Junior they were um so that actually obviously made a lot of people millionaires I I think Tesla may have created more employee millionaires than any company in history yeah um so I would say like if there a company that’s done that’s created more employee millionaires than Tesla you know it’s maybe there’s maybe it’s there’s one or two others but but really (09:07) the sh T us around 140,000 people um and uh you know so just I think I think we might be number one though I think I think you are but now ironically cuz fate loves irony uh opening eye is super closed source and um and for maximum profit it should be renamed close for maximum profit AI um won’t be more accurate so because that’s that’s you know it’s literally the polar opposite of what of how it was started um I guess I’m I generally have a bias in favor of Open Source um and you can see that certainly with the xplatform where we’ve open sourced the algorithm uh and for example for Community notes we open (09:47) source uh not just the algorithm but all the data as well so you can see exactly how a note was created can be audited by Third parties there’s no there’s no mystery nothing hidden at all um think that’s one of that seems to be one of the confusing things about open source as pertains to large language models is it’s it’s not as simple and I think this has been lost in some of the debate as just open sourcing the code it’s open sources the Shades of Gray here of course with parameter weight data there’s there’s all of that spectrum and I think that’s been maybe lost from some (10:15) of the debate so it’s great to hear yeah there’s there’s actually a very little code um very little actual traditional lines of software certainly at the at the inference level there’s there’s very little it’s a remarkably tiny number of lines of code um so it’s just these giant you know weight files and you know what your hyper parameter you know numbers are and um it’s basically a giant comma separated value file um so I always find it amusing to contemplate that perhaps our digital God will be a CSV file you know like and uh and then you make a new CSV file that has maybe more weights and the weights are better (10:52) um and then you pretty much delete the old one so just the AI is evolving and deleting its prior self constantly um so so we’re going to worship a giant Excel file essentially that’s per uh kind of I mean you need a very big yeah it’s a lot of cells but um it’s really just a bunch of numbers and and then those numbers are they’re getting smaller in magnitude too you know going from fp16 to FP fp32 to fp16 to in6 to in 8 and and now it looks like things are trending towards uh mostly in four um so they’re not particularly large numbers um yeah and back to yeah I’m not sure if I’m (11:30) answering I’m probably not answering the question uh but I mean I I I think it’s true that closed source is not that far in like from a Time standpoint not that far ahead of of Open Source um but let’s say and if you assume open source is you have six months because of the immense rate of improvement of AI that 6 months is a is actually a massive amount of time um I mean or at least the delta in a an exponentially improving situation um 6 months well I think feel like an eternity um so I think uh closed will outperform open by a meaningful amount at any given point in time and given I just gon to say given the questions we (12:11) were talking about before about truth and not truth and do you think in 10 years time AI will have been a net Truth uh improver or detractor from where we are today I think it’ll be well I speaking for at least for Gro I think it will be a significant uh truth improver um that that is literally our goal is to be maximum truth SE maximally curious um minimizing the the the error between between perceived reality and described reality and actual reality um and always acknowledging the error not being too confident about it um so I and I think there will be a competition for truth and you will go people will uh tend (12:48) towards the one that they think is most accurate so and and if there’s at least one AI that is uh aiming for maximum accuracy I think it pushes all of the AIS to aim for maximum accuracy um at just just as with um with the xplatform FKA Twitter um as soon as as soon as you know X pushes for maximum truth and accuracy it it sort forces the hand of others they the others now also have to do that right the truth the truth arms race among the csvs and is yeah is one of the one of our thesis I think with ar is that Tesla is one of the most undervalued AI companies there is because of the insane amount of data (13:23) you’ve got on all you know to feed into autonomous I don’t know if there’s any commentary on that because I think you know obviously with the rise of these llm companies which are by and large working off essentially available data that is not proprietary the benefits coming to the companies like Tesla that understand how to use the AI benefits but do have these enormous proprietary data pools seems like that’s going to be the next Frontier yeah um I think that’s accurate an accurate description um Tesla is one of the leading AI companies in the world and with respect to real world AI it is obviously by Far and Away (13:53) the leader mhm um so you know the word large language model with phrase wrong large language model model LM is is massively overused um but what what I do see uh happening is somewhat of a convergence towards intelligence um you know it’s um Tesla like really to to make full cell driving work you kind of need baby AGI in the car um because you need to understand reality and reality is messy and complicated um and just as as a side effect the car AI has to be able to read for example it has to be able to read read read arbitrary sign as a little just a little side effect of understanding reality in in every (14:32) language um so you know uh think everything is coming down to you know different layers of Transformers and diffusion and how you put together the Transformers and the diffusion um does that’s what I sort of made that that somewhat Niche AI joke uh on the xplatform who do you think will be president in 2032 Transformers or diffusion do you think that do you think that switching to full stack um AI for for Tesla’s full self driving reduces the kind of forecast error for you in terms of when the problem will be solved as in you famously have you know said it’s a year away for a few years now uh do you think that like actually line of (15:11) sight wherever it ends up you’ll have a better like line of sight on on when you’re going to cross a particular threshold of performance where it can go without human intervention yeah I mean the car is already incredibly good at driving itself um so uh now especially if you say like okay drive in California what which is um you know both generally easy driving U because you don’t have heavy rain and snow and that kind of thing in most parts of California um and Ts that engineering is primarily in California so you’re going to get it’s going to overfit for solving California but if you’re just driving say around (15:47) Palo Alto the probability that you will need an intervention at this point is incredibly low um in fact even if you’re driving through San Francisco the probability of intervention at this point is very low so we’re really just uh going through a March of nines like how many nines of reliability do you need before somebody does not need to monitor the system you know it’s so interesting um GM basically shutting down Crews um Tasha has done the work on uh safety and um it seems that Tesla with FSD uh has an accidents once every 3. (16:24) 2 million miles at Tesla without FSD 600,000 miles the average vehicle on the road I think this is surface streets 190,000 and cruise was 40,000 so there really was a safety issue there do you worry that um it has potentially set back Regulators uh you know from from considering uh autonomous Transportation or is this data given that accidents uh on the streets have gone up in the last 10 years fairly dramatically actually because it Tex do you think yeah do you think that Regulators are going to look at the data which I mean we’ve done the research Tasha has done the research on on this and is pretty compelling well you know our regulator in the US is the uh (17:13) enforce enforcement division of Nitsa for whom I actually have a lot of respect I think those guys are they they really um have a I mean they’re they’re quite I think actually in my experience thus far have been quite sensible um you know that that that doesn’t mean that they’re on silly things that happen from time to time but uh they’ve really been quite sensible because they they see the actual truth of the the accidents and they see the the fact that there are on the order of 40,000 people killed every year in um in motor vehicle accidents and um so so so they they actually do they do care about uh the the safety (17:46) statistics and they do recognize that you that there’s always going to be some risk with cars um uh they’re well aware of the fact that that our cars are much safer than other vehicles M so you know we really I mean there have been a few things where you know maybe we have had some disagreements but you know like like for example coming to an absolute full stop at stop streets even when there you know it’s very clear there are no Cars anywhere in sight um that could possibly cause an impact no no no cars no pedestrians completely clear and and actually almost no uh humans stop at stop streets at stop sign almost no humans (18:23) actually come to a Full Stop they’re typically they may think they came to a full stop but typically they’ll be going at least a few miles an hour um so that you know they they they said well the Lord does say that you have to stop at a stop Street well even though no almost no humans do so that actually created a bit of a challenge for us because uh we had to select the rare cases where from the fleet where humans actually stuffed at stuff streets stop signs um so and and then actually get our our QA team to stop at stop streets and then overit the stopping stopping at stop signs um in order to get the car to stop at stop (18:57) signs because it was trying to be behave like a a sort of good normal human driver who does not stop at stop sign so you know um we had a disagreement there on it but we ultimately you know have made the car stop as stop signs even though it’s a little Annoying um so uh but you know overwhelmingly you know so one of the things the media will try to create this that that I’m some sort of lawbreaking Maverick uh who just does does what he wants and doesn’t care about legal matters um this is actually not true of the Lally millions of laws that I am subject to and my companies are subject to there are a handful over (19:33) the years that we’ve disagreed with almost but the vast majority 99 I don’t know 9% of of laws and regulations uh we do abide F and we do agree with um well maybe maybe don’t agree with we certainly abide by um you know there’s once in a while there disagreement but it’s it’s really um minor and but if you sum up the disagreements across over the you know 20 years it it can just sound like a lot but but in fact my companies are incredibly law biting so that’s the that’s the that’s the truth of it um anyway I I I think I I think it’ll be I think it’ll be fine from a regulatory (20:08) standpoint when we can show that uh there’s even completely without supervision there is a a massive statistical uh case for self-driving being actually safer than if someone had their hands on the wheel or or was even potentially having their hands on the wheel then I I think The Regulators will accept that you know it’s been really interesting uh to as AI we we were watching the breakthroughs as everybody has of course in Ai and and we had agreed with you our original our original research had autonomous U mobility in pretty much full force this was our original research by now but if we didn’t have the breakthroughs in AI (20:49) that that we’ve seen particularly Transformer architecture we it it’s it wouldn’t have been possible right but we didn’t know that is is that how you’re viewing it now uh well it’s certainly true that Transformers um are transformational yes so uh you know pretty much everyone’s using Transformers uh and it’s really just how many Transformers what kind of Transformers are you using autoaggressive Transformers which are very memory bandwidth intensive um you know there’s but you you I think you really I’m not sure you can really do AI without Transformers in a meaningful way (21:25) um diffusion is also very important um so I mean just really looks like AGI is some combination of Transformers and diffusion um interesting would does diffusion help Elon in the things that Transformers are NE not necessarily that good at yet like planning and remembering rather than sort of a current state uh I Transformers are important for all aspects of AI um yeah I mean computers are very good at remembering things I your phone remembers the video that you took down to the last pixel um whereas you know most humans cannot remember who they met last week so memory is the easy I mean we’ve already outsourced memory to (22:02) computers uh in that like you say how how much of human knowledge is in digital form versus in contained in human neurons it’s overwhelmingly in Silicon uh rather than biological neurons right so I there there this I me I think it’s always you know it’s good to to think of um like try to consider one of the most fundamental ratios um this is like in physics you always think about try to looking at fundamental ratios um and some of the you know one of the fundamental ratios is the ratio of digital to biological compute um just you know just just in raw raw compute um and and how much and the ratio of (22:42) digital memory to biological memory so you know at what point it like I think at this point I think we’re probably over 99% of all memory is digital as opposed to biological so youve got over I think over a 100 digital to biological ratio on memory at this point um trending towards 000 you know and many or of magnitude beyond that um then um then you’ve got the digital biological compute ratio and and since the number of humans is more or less constant I’m worried about you know humans declining quite significantly in the years to come because the birth rate is so low um but you know so so human human computers (23:17) more or less it looks like a flatline versus time whereas the digital compute is uh exponential right um and um you know at some point it’s that that will also be 100 meaning more than 99% of all compute will be digital instead of biological and if we’re not there already we will be within a year or two although it is amazing how much data biology can store right I reckon I believe it’s it’s estimated that a gram of dried DNA can store between four and 500 exabytes so maybe there’s still still things to learn from biology but yeah incredible the pace of chain yeah I mean if you encode memories uh in DNA like a tiger (23:54) tape then the DNA um memory potential is immense um but I’m not sure that’s actually what’s happening or if it is it’s I don’t know I’m not sure why my memory is so terrible but actually I mean your memory in terms of the raw details like you have an amazing search function right I me it’s kind of like yeah well for for any I mean I would argue kind of in some ways like the the Transformer architecture enables um effective search of a a very very broad amount of data um and or that’s a or or it’s like data compression it seems like the um yes we have there’s many more like databytes encoded or gigabytes (24:34) encoded in disk but but actually being able to effectively search that space is is pretty still primitive with compute at least At Large Scale relative to human’s ability to access information no I mean there’s still some things that humans do better than computers um I mean if if you look say like say like computers have not yet discovered any fundamental physics but humans have discovered a lot of fundamental physics um the computers have not yet invented a use useful technology um humans have invented many uh useful useful Technologies um so you know um you think that do you uh ascribe to the notion (25:08) that like you essentially training training AI systems on language is not enough and that you have to have some kind of embodied source of data like through an Optimus robot um to actually get kind of the the raw learning in that a AI system would need to understand fundamental physics yeah so that’s what I think is coming I I think um I think what’s coming is is uh AI understanding fundamental physics um and and AI inventing new technologies it it definitely definitely feels like rather even though some things now apparently pass the touring test most importantly passing the money Penny test as in can (25:43) an AI actually pay bills do expense reports to the extent James Bond did those and actually pay parking fines and things like that that would actually be a very good use of AI that I think would have a lot of users immediately and and maybe the the uh Billy Connelly test of can an AI actually be consistently funny those Gro is pretty funny that’s fair um I think you know one of our goals for grock is to be the funniest AI so I mean if you ask grock to provide a vulgar roast it’s really good and speaking of Gro to not to not let down our future AI overlords the final question on the grock summary that that that Kathy touched on was the (26:17) future of Bitcoin not sure if we want to touch on that or not or whether that’s out out of out yeah that that and um and then just be before uh I don’t know if we’re wrapping up here but uh love to talk about uh Bitcoin and and your your view of crypto assets and then maybe back into um kind of tying things up uh uh with convergences among Technologies you mentioned that a little earlier uh Elon and the impact ultimately of all of this on economic growth longer term you know the surprises that might come from there but first first Bitcoin okay sure what’s the question regarding you know there’s when we first um when you put it (27:02) uh or Tesla put it on on its uh balance sheet there was I I remember uh afterwards there was this backlash uh in terms of its economic impact uh all of the energy usage and so forth and then and then as part of the the seminar that we did that you participated in Elon we we basically we and blocked had come up with this um notion that wait you put uh Bitcoin mining into a utility ecosystem uh so that you know if uh if there’s a storage unit and you know the the intermittent energy you know the storage units full sun is still shining the Excess power goes over to bitcoin and uh and you can mint Bitcoin and (27:52) overbuild solar and wind and Renewables and all of that so so that that um that argument against Bitcoin and Larry think himself used it uh quite vifer for a long time until until recently um that that is going away what do you think about uh bitcoin’s um potential impact on the financial system the monetary uh monetary system Financial Services generally well I have to say I I don’t spend a lot of time thinking about cryptocurrency um hardly any hardly any at all um I I have thought for a long time about money and you know the nature of money what is money yes um and you know it’s really a database for resource (28:38) allocation is the way to think about money my view um so um now fiat currency is actually fine um as a database for resource allocation um if you have a predictable money supply and it it it doesn’t um you know it doesn’t get inflated or deflated too much if it’s rules based rules based if it’s rules based and then and and then and provided government does not uh too much abuse the privilege to to create more money um you know some amount of of abuse is almost it’s guaranteed to happen but but you know you can go too far um so uh you know the the Temptation over the ages to debase money is I it goes back to (29:16) probably the ancient Sumerians you know as soon as they at the point which money was invented in any form with including in Gold um the there was there was the the unavoidable temptation to debase money supply so so when you think of any given money system again I sort of apply information Theory to a money problem um just think of it like it’s a it’s an information system you want to minimize noise um minimize latency minimize packet loss um and um and so any like like inflation would be adding noise to the system um if if the if the transaction rate if if the if what it takes to conclude a transaction is a long time uh that adds (29:57) latency to the system uh you know fraud is kind of like packet loss uh you know it’s it’s you know so so so I I guess I I kind of think of money like like I think of uh information moving on a on a network uh bandwidth uh latency jutter which is the variance in latency uh packet loss um those fundamental elements yeah it’s been interesting to watch um how human beings in this case the fed you know the words they say whip around markets and there’s just something not right about that um I mean that’s that’s my point of view I think our point of view uh certainly on on the crypto team and you know um art laugher (30:39) who was my professor in school he’s ply side you know uh Austrian School of Economics you know when he first he he collaborated on our first uh Bitcoin paper in 2015 and after he got through reading it he said you know I’ve been waiting for this since they closed the gold window in 1971 I’ve been waiting for a rules a new rules-based Global monetary stem yeah uh and I think that’s where our optimism comes from this is truly rules based and so he’s gotten more and more excited about it the more he’s learned about it so you know um anyway um back to convergence though H and you’re you’re (31:25) you’re the Maestro here you know when I think of um when when we think of you know uh autonomous taxi platforms Robo taxi networks and so forth you know that the that’s the convergence of three of the major platforms around which we have centered our research uh robotic um energy storage and artificial intelligence each one of those uh Technologies you know basically gearing up for or in the in The Sweet Spot of a um of the scurve and I I sort of think it of s-curves feeding s-curves and creating the potential for explosive growth that no one is expecting and and Brett has just written a paper it should (32:09) come out in the next few weeks around this topic um I just want to get your thoughts about that and Brett if you wanted to ask any questions specifically about that but the the the the bottom line from an economics point of view is we could see you know very surprising up uh uptick not just uptick a step function up in economic a real economic growth if these Technologies are as big as we think they are oh yeah absolutely well um I mean the the real economy is not money it’s goods and services um so you know as was saying earlier money is really just a database for resource allocation um and uh you know it’s but (32:50) if there no resources to allocate or if the actual goods and services output have is is fixed then you really you I guess try to allocate that same pie bit more efficiently but it’s really not nearly as good as as growing the pie um so growing the the the this output of goods and services um you know productivity is really the the thing that leads to Prosperity um and uh obviously with with robotics and AI um I think digital medicine through through synthetic RNA um and uh you know the I think these things are what will just massively improve productivity um you know just and and if if you’re got if you take the the the car as an example (33:30) um and I think you understand this quite well um although not many people do um that you know if you’ve got say a passenger car uh it will typically see 10 to 12 hours of usage like maybe one and a half hours a day over the course of seven days um but there are 100 there are 168 hours in a week so it’s really used for a very small percentage of the week normally and then you’ve got a pocket somewhere as well so you’ve got parking costs storage costs maintenance costs and you know all these these these costs associated with a car and and you know you’re at 5 to 10% of asset utilization for a car and whereas if if (34:04) you’ve got an autonomous vehicle that same car um that cost basically about the same um could now I think be used as a rough approximation for about a third of the hours of the week if you say like productive hours and this I think that’s a I think that’s a conservative but I think it’s not far wrong so if you say like it goes from say 10 hours a week to 50 hours a week out of 168 of useful hours that that I think is not might it’s not going to be exactly right but I don’t think it’s far wrong you now have the same asset being able to having five times the utility so that is that is that is a (34:38) such a staggering leap of of productivity um you know and and if Tes is able to do as as I think we will be able to do which is upgrade the fleet um so to make it autonomous it will be the biggest step change in asset value in history I I think um overnight we we agree we agree converted here obviously and I that’s why I preface it by I realize I’m preaching to converted but but you’re also you’re also underestimating the utility uplift because that 10 hours that the car is driving a week somebody is not being paid to drive it mostly right right but but they’re devoting their valuable brain to doing it as then (35:15) kind of it’s actually there’s the the human labor loss um that that accompanies that as well and so freeing up those hours is actually I think on Tasha’s work it’s as meaningful you know as the capital u a benefit absolutely um and I I think you’ve also predicted massive increase in traffic uh you know because once you take away the pain of driving um people will will go more places uh so so there’ll actually be more cars on the road driving more places because if you can s sit in the car and um you know be watching a movie or be texting legally um then then then what you know that that’s really much (35:51) less pain than than having to navigate ajo’s traffic for an hour and a half yeah many people missed that I remember I I just frustrated me that um GM and its marketing material and presentations would say zero congestion because I think yeah our work would suggest threefold the amount of traffic and that’s what pushed pushed us into doing more um with drones and and you know delivery of food and Parcels with drones and even air taxis because um cannot imagine the commute from JFK into Manhattan being you know four hours or five hours or six hours you know so yes well fortunately um I have a proposed solution to that uh which consists of (36:38) tunnels yes yes and we’re we’re now in uh St Petersburg if you could build a tunnel from here to Tampa which is just sand you’re not going to run into anything that’ be that’d be very cool we can absolutely do that the B compan has actually made made tremendous it has I think at this point by far the most advanced tunneling machine the thing that is the biggest inhibitor of Waring Company by far is regulatory approval um it’s you know we are in in the United States and particularly in some states like California the we are being strangled by regulation yeah um and this is an important point to make which is I (37:14) think obvious when you think about it uh that if if we keep making more rules and regulations and laws every year um and those laws and regulations are Immortal but humans are not Immortal then over time you you you you’ll be like Gula where you’re tied down by a millions of strings and you can’t move it’s true it’s true that’s why your first principal approach to your business is so interesting I wish we had first principal approach to to policymaking and you know because you’re right the the the regulations are strangling the the the only good news here in this and we’ve seen it is other countries want (37:52) Innovation you know us has been you know has been the source of a lot of innovation but some of our you you look at the FAA and drones they they the that that business has done much better elsewhere in the world uh you know it’s been very frustrated here and now the FAA is starting to play ball because of it do you think that that uh you know that competitive dynamic or regulatory Arbitrage will change things anytime soon or do we just need someone to come in first principles and kind of figure out a way to conin everyone we need to uh get a white sheet of paper and and not start all over but you know what I (38:33) mean I think we need to have a process for garbage collection of rules and regulations um that that if you don’t have some means of deleting rules and regulations and you only ever add them then then we’ll obviously just end up in a situation where everything’s illegal yes yes which is where we where we are and and and uh kind of the bigger and more complex the state is like like California and New York uh you just have I guess first approximation the number of regulators you have is proportionate to your population so although Texas and Florida do a lot do do better in that regard um but there’s just so many rules (39:06) and regulations in California New York um and if you say you know LA County um you know City of La it’s now now you’ve got you you it’s it can take you several months simply to get a permit to to change the tiles in your bathroom literally yeah um so you know that’s things things are we we’ve got to do something here or we’re just doomed um for this reason Elon do you think China is at a competitive Advantage it’s command and control I’ve heard you you know uh say that that they’re very techsavvy down to the mayor of every city do you think or do you think what has happened over that there’s a dynamic (39:46) changing maybe they’ve been they’ve overleveraged after 20 years of you know basically buying the dip was the right thing to do until it wasn’t do you think think that uh it is now entered more of a Japanese style you know you know they have to readjust to a new reality or do you think you know the regulatory uh or less regulation over there is going to give them a a competitive advantage and innovation’s going to you know help bail them out I I don’t think China is not immune to the same forces that have driven excess regulation and laws in the United States or Europe um they um you know I’ve certainly seen just in the (40:29) course of our operations in China tells in China we’ve seen a quite a substantial increase in uh the regulatory burden um so you know the it’s it’s a lot harder to get things done now than it was 10 years ago um that’s that’s I I so what I’m saying is I think this is just a natural part of uh the evolution of of any any country is is the growth of rules and regulations um now in in history the forcing function for cleaning up uh bad rules and regulations or or or sort of archaic rules and things that don’t really matter anymore has been war that has been what has driven has been the (41:03) cleansing function for rules and regulations um so now we would like to avoid war as ideally we can do this without war that would be nice um um so um you know has it ever has it ever happened has it ever happened before I know you’re a student of war and battles and that you really understand that Dynamic have we ever cleansed the system without it I am not aware of I’m not aware of a situation like that that yes now the thing is in history wars were very common so um you know that we’re we’re in this odd odd bit of History where war is quite rare we’re talking about you know total population of the (41:40) world divided by number of by by percentage of the population at War if you say look what percentage of Earth’s population is actively at War it’s very tiny today uh compared to any time in history um the people confuse hearing about war from being in war mhm um yeah I think exposing uh and I know you did in in Ukraine with starlink exposing what’s going on is half of the is half of the solution in a sense right or or maybe the reason that uh you know that that we’re not going through the horrors of War because you can see it In Living Color and and and how devastating it is yeah I mean that that is certainly (42:24) uh something that is possible it was not possible in the past um although it clearly doesn’t stop the Ukraine war um right uh that’s still going on um and uh you know I think a lot of a lot of things if there had been social media it’s not clear they would have happened at all um like the creation of the United States if if um I always think of like if if there had been social media at the time of of ronoke um I think people would have been dissuaded from future voyages you it’s like we’re we’re dying of Salvation we’re being killed by locals um we’ got we’re ripe with disease (43:00) that’s a good point very good point but that is that is another way to reset the regulatory landscape is is kind of exploration right I mean like who knows what I’m sure there’s some set of laws that apply on on the moon and and potentially Mars but you know it’s uh much more difficult to enforce laws from from that long a distance yeah I think ultimately if there is a self-sustaining City on moon or Mars it that self-sustaining City will achieve autonomy at some point um so uh I think it’s quite important that we not quite extremely important that we uh become a multiplet civilization um and create a (43:36) self-sustaining City uh ideally on Mars because that’s quite far away from Earth and I think if something were to take down Earth civilization then it would it’s less likely to take take out of a Mars civilization because Earth and Mars are only in the same quadrant of the solar system you know for 6 months every 2 years um so’ got a lot of Separation um and uh and I think you know I think a lot about the FY Paradox of where where are the aliens I I’ve seen no evidence of aliens people always ask me if I’ve seen any evidence of aliens and I I would immediately shot from the rooftops and from the xplatform of any any (44:10) evidence whatsoever about aliens I would be you I would be talking about it immediately and I have thus far seen none um that’s a concern cuz it means that we have our Alone um and uh you know and and maybe there were many other civilizations but they failed to pass the great filter of of going from one planet to being a multiplet civilization um so I guess my concern is that we need to keep civilization going at least long enough to be be a multi civilization at that point I think we we stabilize civilization and Consciousness and greatly extend the probable lifespan of civilization and Consciousness as we (44:43) know it um so um I do want to emphasize this is not some escape hatch from Earth because people misinterpret this as like some Escape hat from Earth the and this and this really doesn’t apply to me personally cuz you know I will probably die and in I don’t know 40 or 50 years hopefully you know living that long maybe I don’t live even that long um but uh but civilization can last for tens of thousands of years perhaps hundreds of thousands perhaps millions of years um so I think it’s very important that we make life multiplanetary while the window is open and this is the first time in the history of Earth that for in (45:16) the in four and a half billion years of of Earth’s um existence that it’s been possible to extend um life and Consciousness to another planet so that window is now open for the first time in 4 and a half billion years now it may stay open for a long time but it may also close um and I think so we should just take advantage of the time when the window is open to make life multiplanetary um and ensure that the the tiny candle of Consciousness in a vast Darkness continues to continues to burn does not go out do you think that starlink you you’ve said kind of the advanc the idea that you needed Hypersonic travel to basically (45:51) capitalize um colonizing Mars is is starlink enough or does that still do you still need that second kind of like business model built on top of SpaceX to make it happen I think Sonic is enough I think Sonic is enough you know that there’s there’s a little diagram on the stalic router um it’s a people probably didn’t notice but it’s it’s a home and transfer from Earth to Mars um so if you look on your starink router you’ll see that home and transfer diagram um that stall link is the means by which life becomes multiplanetary so well El you’re you as you are you are pioneering um Us (46:32) in an amazing way a lot of a lot of the hope for the future is so much as it of it you know has um has has started with uh you know with projects you’ve started and and dreams you have and um I think the world’s a a better place and I’ll I’ll end where I started I I I know that media clicks H you know maybe negative is it worked but long run long run truth wins out and uh you know you’re doing amazing things and um I think uh the world will thank you um for years and years and years and maybe just a close Charlie oh sorry I click back on M just as a a closing thought to build off that maybe as well how would you think about (47:19) inspiring the next generation of entrepreneurs to do the really big hard things does it make most sense to try and do something more consumer internet or something earlier build up capital and then go and do something very hard or do you think like what would you want to have heard as a as a starting out entrepreneur thinking about that journey and also to inspire the Next Generation to do great things including building all the cottage industry no doubt of things that we’ll need to survive on Mars great question well I actually think that you know anyone that has a you know a company that’s providing (47:48) goods and services um is is is doing great I mean it’s not that everyone should tackle so-called you know in quotes great challenges um I I I would recommend that they they don’t actually because the probability of failure is very high um I mean I I think just just try to be useful is my advice uh it’s very difficult to be useful um you know if you see like total area under the curve of how useful have you been to other people you know that cumulatively that’s that’s it’s very hard to make that a big number you know so I it’s really just try to be useful um and make you know create products and services (48:22) that people want and make their lives better um that’s it and you know as far as uh you know encouraging uh entrepreneurs to start companies uh my advice would be that if if you need encouragement to start a company don’t do it right right um y because you’ll probably I mean the probability of failure is very high um so you really should not if you if you need encouragement uh yeah yeah don’t do it thank you yeah thank you so much Elon you know as I I think as it’s clear if you have P if you have an idea that will uh fill an met need that the world you know something the world really needs (48:59) and you have a passion to go for it then yes but agreed the probability of failure is uh Brett I think you know is it 90% for new businesses 90 80 90% depends on what stage you measured at but yeah yeah from startup yeah so but that’s that said anyone that doesn’t need encouragement we’d love to hear for the that’s good I mean I just have a lot of respect for anyone who who works and and you know builds things provides products and services to their you know fellow humans um and uh you know I think that’s that’s great um you know just a lot of respect for an honest day’s work effectively um right so all right well (49:40) it’s been a great discussion yeah thank you Elon so much this is great he you’re welcome yeah and and and you know what Elon I think the world needs more of this kind of conversation so maybe every once in a while we can we can uh we can do this again sure sounds good happy thanks so much want more content Early Access bunch of perks click the links in the pin comment ag1 has given me a massive meaningful boost in energy allowing me to do a lot more every day including using my brain more and using my body more highly recommend you guys and girls check it out is an excellent way to fill in nutritional gaps it’s got (50:15) 75 high quality vitamins minerals and Whole Food Source nutrients plus prebiotics and probiotics and digestive enzymes and adaptogens to help you deal with stress plus if you click the link at the pin comment or head to drink a1.com &#x2F; smmr you can get yourself a 1-year free supply of vitamin D3 and K2 but don’t take my word for it here’s what some of you guys and girls have to say ag1 has changed my life I was as you described treating myself like a circus ate like trash rarely exercised use alcohol as a stress crutch cannabis also ag1 is what gave me the kick in the ass got me back to the gym motivated me to (50:47) do more for myself family my business Etc keep doing what you do now I know there’s some Skeptics the same kind of people who think Elon Musk is a fraud reading this G what do you thought there’s no way that’s possible bro it must be a placebo effect believe it or not this is a recurring theme if you give your body everything it needs to feel and performance best including having a lot more energy you’ll need ways to use that energy for me personally that includes more exercise moving my body more more social activity and more cognitively demanding tasks including producing a f ton of exclusive (51:15) content over on Twitter and on patreon plus my daily YouTube uploads the proof in the pudding on to another testimonial from a viewer of this channel SMR you asked me to provide feedback on ag1 here it is it has helped with mental acuity stamina and intestinal Waste Management uh can’t bre between the lines certainly helps with regularity and digestion that’s what the digestive enzymes are for it has also dramatically reduced my cravings for sugar you guys need to stop eating sugar it’s poison I’m 50 59 and overweight AK a fat I think that’s a technical term for overweight isn’t it is it fat (51:47) or obese I can’t remember I average 100 hours a week in the west Texas oil fields as a safety supervisor Jesus Christ dude no wonder you’re struggling to keep your weight under control 100 hours a week brutal it has helped me lose weight it is not an appetite suppressant it can help fat people suppress cravings and motivation to be healthier is critical for changing your diet love you brother again this is a great point and something people really don’t seem to grasp if you have more energy everything becomes easier it’s like turning on easy mode for Life few years ago before I was taken ag1 my (52:17) health was trash I was struggling to get through the day had afternoon fatigue the last thing I wanted to do was either use my brain or move my body didn’t have the energy now my biggest struggle every day is figuring out ways to use that energy I’m exercising way more doing a lot more with my friends and family and of course my work output has increased substantially and you can fact check me check out the average length of my videos I was posted on YouTube 3 years ago need I say more and one final testimonial love this one okay here’s the deal for me with this ag1 I’m 41 years old and not the type to eat (52:47) drink smoke or sleep healthy so I was skeptical that being said here’s what I experienced day one me day two afternoon fatigue was about 45 5 minutes late day three zero afternoon fatigue day four zero afternoon fatigue plus extra energy day five again zero afternoon fatigue plus energy wondering what the really see this is the thing right the results for many people are just almost too good to be true this this is the same experience I had my afternoon fatigue just vanished out of nowhere I’m like wait what the why am I not tired in the afternoons anymore surely it’s not that ag1 is it turns out it was (53:20) day six and 7even same thing day eight same thing plus I had the want to get things done around the house that I normally would slack off and not get done again the point extra energy you’ll need to use it you’ll find ways to use it day 9 10 and 11 and today is day 12 I love it so however you managed to get me to buy it I’m so glad you did thank you so much SMR it really changed me so far guys this really works just try it by the way this is the reason I continue to relentlessly promote ag1 a lot of people get real mad in the comments oh my God SN allone sold out oh my God he’s a scammer this is fraud constantly I’m pretty sure (53:55) everyone making these comments is also currently short Tesla stock I’m not particularly concerned about people having a negative perception those folks suffering from small brain syndrome still living in my bum’s basement syndrome Etc writing mean comments claiming a1’s a scam or it doesn’t work I mean bro when I get feedback like this this is what keeps me going just try this stuff for a month and if you don’t get these results get your money back see it’s a literal no-brainer it’s an IQ test at this point in time testimonial after testimonial after testimonial like this get your money back if it doesn’t (54:22) just try it for a month and if it doesn’t work get your money back today is the day it’s finally time be like this guy who was a massive skeptic but finally after a thousand promotions in a row caved in tried ag1 and has results like this head to drink a1.com smmr or click the link at the pin comment and please let me know how you’re feeling in a few weeks time now if you’ll excuse me time to put my extra energy to good use I’ll be recording some more exclusive content for patreon and my Twitter subscribers so click the links to pin comment see you over on Twitter and or patreon and don’t forget to grab your (54:51) ag1 love you Generate with Glarity.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Summary&quot;&gt;&lt;a href=&quot;#Summary&quot; class=&quot;headerlink&quot; title=&quot;Summary&quot;&gt;&lt;/a&gt;Summary&lt;/h3&gt;&lt;p&gt;这是一场关于特斯拉、人工智能和金融市场的讨论，Elon Musk和Cathie Wood分享了他们对</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="ai" scheme="http://example.com/tags/ai/"/>
    
    <category term="tesla" scheme="http://example.com/tags/tesla/"/>
    
  </entry>
  
  <entry>
    <title>AI科研工具推荐-风爷推荐EP09</title>
    <link href="http://example.com/2024/01/05/My%20Top%20Recommendation%20-%20Academic%20Tool%20for%20AI%20Research/"/>
    <id>http://example.com/2024/01/05/My%20Top%20Recommendation%20-%20Academic%20Tool%20for%20AI%20Research/</id>
    <published>2024-01-04T16:00:00.000Z</published>
    <updated>2024-01-05T04:32:40.117Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Immersive-translator"><a href="#Immersive-translator" class="headerlink" title="Immersive translator"></a><a href="https://immersivetranslate.com/">Immersive translator</a></h3><p>Immersive translator 沉浸式翻译这款软件无论是交互体验设计还是翻译质量都做的非常不错。另外，支持对 PDF 做对照翻译，大大加速了论文的阅读效率。也能直接翻译EPUB电子书。<br><img src="https://s.draftai.cn/vent/202401051224355.png"></p><p>产品做得很细，在arxiv上还加了双语版本。<br><img src="https://s.draftai.cn/vent/202401051220383.png"></p><h3 id="Connected-Papers"><a href="#Connected-Papers" class="headerlink" title="Connected Papers"></a>Connected Papers</h3><p>这个产品有用，输入一篇论文，它会将这篇论文的依赖和被依赖项以知识网络的形式全部呈现出来。类似的产品还有 litmaps 、researchrabbit <a href="https://www.connectedpapers.com/">https://www.connectedpapers.com/</a><br><img src="https://s.draftai.cn/vent/202401051226774.png"></p><h3 id="Aminer"><a href="#Aminer" class="headerlink" title="Aminer"></a><a href="https://www.aminer.cn/">Aminer</a></h3><p>Aminer 是智源旗下的一款 AI 能力的论文检索平台，它提供的「必读论文」板块从领域&#x2F;机构&#x2F;期刊&#x2F;会议等多视角收集了很多最新最热的论文集锦，适合作为学习和研究的入口索引。通过AI能力集成了不少使用的科研功能。</p><p><img src="https://s.draftai.cn/vent/202401051227382.png"></p><h3 id="Monica-All-in-One"><a href="#Monica-All-in-One" class="headerlink" title="Monica All-in-One "></a><a href="https://monica.im/">Monica All-in-One </a></h3><p>AI 效率工具集，从 Chat&#x2F;Read&#x2F;Search&#x2F;Write 等多个场景切入，提供了设计美观、交互强大、功能丰富的趁手工具箱。每篇论文基本都是让它先读一遍，再提问式学习。</p><p><img src="https://s.draftai.cn/vent/202401051229236.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Immersive-translator&quot;&gt;&lt;a href=&quot;#Immersive-translator&quot; class=&quot;headerlink&quot; title=&quot;Immersive translator&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://immersive</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="ai" scheme="http://example.com/tags/ai/"/>
    
    <category term="Academic" scheme="http://example.com/tags/Academic/"/>
    
  </entry>
  
  <entry>
    <title>AI科研工具推荐-风爷推荐EP09</title>
    <link href="http://example.com/2024/01/05/Top%20Recommendation%20%20Tool%20for%20AI%20Research/"/>
    <id>http://example.com/2024/01/05/Top%20Recommendation%20%20Tool%20for%20AI%20Research/</id>
    <published>2024-01-04T16:00:00.000Z</published>
    <updated>2025-04-02T19:29:46.238Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Immersive-translator"><a href="#Immersive-translator" class="headerlink" title="Immersive translator"></a><a href="https://immersivetranslate.com/">Immersive translator</a></h3><p>Immersive translator 沉浸式翻译这款软件无论是交互体验设计还是翻译质量都做的非常不错。另外，支持对 PDF 做对照翻译，大大加速了论文的阅读效率。也能直接翻译EPUB电子书。<br><img src="https://s.draftai.cn/vent/202401051224355.png"></p><p>产品做得很细，在arxiv上还加了双语版本。<br><img src="https://s.draftai.cn/vent/202401051220383.png"></p><h3 id="Connected-Papers"><a href="#Connected-Papers" class="headerlink" title="Connected Papers"></a>Connected Papers</h3><p>这个产品有用，输入一篇论文，它会将这篇论文的依赖和被依赖项以知识网络的形式全部呈现出来。类似的产品还有 litmaps 、researchrabbit。<br><img src="https://s.draftai.cn/vent/202401051226774.png"></p><h3 id="Aminer"><a href="#Aminer" class="headerlink" title="Aminer"></a><a href="https://www.aminer.cn/">Aminer</a></h3><p>Aminer 是智源旗下的一款 AI 能力的论文检索平台，它提供的「必读论文」板块从领域&#x2F;机构&#x2F;期刊&#x2F;会议等多视角收集了很多最新最热的论文集锦，适合作为学习和研究的入口索引。通过AI能力集成了不少使用的科研功能。</p><p><img src="https://s.draftai.cn/vent/202401051227382.png"></p><h3 id="Monica-All-in-One"><a href="#Monica-All-in-One" class="headerlink" title="Monica All-in-One "></a><a href="https://monica.im/">Monica All-in-One </a></h3><p>AI 效率工具集，从 Chat&#x2F;Read&#x2F;Search&#x2F;Write 等多个场景切入，提供了设计美观、交互强大、功能丰富的趁手工具箱。每篇论文基本都是让它先读一遍，再提问式学习。</p><p><img src="https://s.draftai.cn/vent/202401051229236.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Immersive-translator&quot;&gt;&lt;a href=&quot;#Immersive-translator&quot; class=&quot;headerlink&quot; title=&quot;Immersive translator&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://immersive</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="ai" scheme="http://example.com/tags/ai/"/>
    
    <category term="Academic" scheme="http://example.com/tags/Academic/"/>
    
  </entry>
  
  <entry>
    <title>2024年终极加密货币投资组合-枫叶洞见EP11</title>
    <link href="http://example.com/2024/01/04/Millionaire%20Crypto%20Portfolio!%20(2024%20Ultimate%20Guide)-Just%20In%20Vent%20EP11/"/>
    <id>http://example.com/2024/01/04/Millionaire%20Crypto%20Portfolio!%20(2024%20Ultimate%20Guide)-Just%20In%20Vent%20EP11/</id>
    <published>2024-01-03T16:00:00.000Z</published>
    <updated>2025-04-02T19:12:11.382Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://s.draftai.cn/vent/vent1924_A_conceptual_collage_representing_three_major_blockcha_781ca605-3fa0-4156-b350-98745f305be4.png" alt="vent1924_A_conceptual_collage_representing_three_major_blockcha_781ca605-3fa0-4156-b350-98745f305be4.png"></p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>开篇的话<br><strong>注意，这是油管上Web3博主的视频总结，仅供学习和信息传递，不构成任何投资建议！！</strong></p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>Discover Crypto 作者分享了他在2024年的终极加密投资组合，包括比特币、以太坊和L1项目、De FI项目等。他提到了比特币减半、比特币ETF和联邦官员降低利率等因素对加密市场的影响。此外，他还详细介绍了他看好的加密货币项目，以及他的投资策略和理念。<br><a href="https://www.youtube.com/watch?v=IDxELCiilS4&ab_channel=DiscoverCrypto">https://www.youtube.com/watch?v=IDxELCiilS4&amp;ab_channel=DiscoverCrypto</a></p><ul><li>[📈] 作者分享了他在2024年的终极加密投资组合，包括比特币、以太坊和其他加密货币项目。</li><li>[💰] 他提到了比特币减半、比特币ETF和联邦官员降低利率等因素对加密市场的影响。</li><li>[📊] 作者详细介绍了他看好的加密货币项目，以及他的投资策略和理念。</li></ul><h3 id="更多细节"><a href="#更多细节" class="headerlink" title="更多细节"></a>更多细节</h3><ul><li><p>2024年加密货币百万富翁投资组合！00:00</p><ul><li>介绍了2024年加密货币市场的三个重要推动因素：比特币减半、比特币ETF和降息。</li><li>强调了现在是时候开始寻找2024年的大趋势，并开始积累在这些领域中的顶级项目。</li><li>提到了成功的指标：炒作周期、网络效应和代币经济学。</li></ul></li><li><p>**L1项目：卡尔达诺、ICP和Celestia ** 04:46</p><ul><li>卡尔达诺被认为是比特币和以太坊之后最有潜力的L1项目，因其稳定的发展和强大的生态系统而备受关注。</li><li>ICP在推出时引起了很大的关注，虽然价格有所下跌，但其技术实力和合作伙伴关系使其成为一个有潜力的项目。</li><li>Celestia是一个新兴的L1项目，其在手机挖矿和轻节点方面的创新可能会吸引零售投资者。</li></ul></li></ul><p><img src="https://s.draftai.cn/vent/vent1924_A_conceptual_collage_representing_three_major_blockcha_eba13f65-606f-4a66-a46c-9c43b5a78cfa.png" alt="vent1924_A_conceptual_collage_representing_three_major_blockcha_eba13f65-606f-4a66-a46c-9c43b5a78cfa.png"></p><ul><li><p><strong>DeFi项目：Chainlink、BENQ和GMX</strong>  09:34</p><ul><li>Chainlink是一个必备的项目，作为Web 3.0的中间层，将离链数据引入链上，具有巨大的潜力。</li><li>BENQ是一个在Avalanche上的DeFi项目，具有潜力，特别是在轻节点部署方面。</li><li>GMX是一个衍生品交易平台，具有强大的财务数据和合作伙伴关系，有望在机构投资者进入加密货币市场时获得巨大的交易量。</li></ul></li><li><p>**DSI项目：VADL ** 14:05</p><ul><li>VADL是一个专注于生命延长的去中心化科学项目，尽管该类别在市场上不太知名，但它具有巨大的潜力。</li></ul></li><li><p><strong>游戏和娱乐项目：Rollbit、IMX和Beam</strong>  18:34</p><ul><li>Rollbit是一个游戏项目，具有潜力，特别是在线游戏领域。</li><li>IMX是一个在Polygon上的游戏项目，与许多知名品牌合作，有望在游戏行业获得广泛采用。</li><li>Beam是一个具有巨大潜力的游戏项目，其在游戏生态系统中的投资和合作伙伴关系使其成为一个有吸引力的选择。</li></ul></li><li><p>**L2项目：Arbitrum和Polygon ** 23:06</p><ul><li>Arbitrum是一个L2项目，已经获得了大量的DeFi总锁定价值，有望成为机构投资者选择的L2解决方案。</li><li>Polygon是一个具有强大商业发展团队的L2项目，与许多知名品牌合作，有望在零售市场上获得广泛采用。</li></ul></li><li><p><strong>Meme代币：Snake和Pepe</strong>  27:54</p><ul><li>Snake是Cardano上的顶级Meme代币，具有实际产品和强大的社区支持。</li><li>Pepe是以太坊上的Meme代币，具有较小的市值，但在政治话题上具有潜力。</li></ul></li></ul><p>请注意，这些总结是根据视频的内容进行的，不代表观点的准确性或投资建议。<br><strong>最后强调，不构成任何投资建议！！</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://s.draftai.cn/vent/vent1924_A_conceptual_collage_representing_three_major_blockcha_781ca605-3fa0-4156-b350-98745f305be4.</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="Crypto" scheme="http://example.com/tags/Crypto/"/>
    
    <category term="Cryptocurrency" scheme="http://example.com/tags/Cryptocurrency/"/>
    
    <category term="BitCoin" scheme="http://example.com/tags/BitCoin/"/>
    
  </entry>
  
  <entry>
    <title>想在股市赚钱，2024年你只需要看懂这些！-枫叶洞见EP10</title>
    <link href="http://example.com/2024/01/03/The%20Only%20Study%20Guide%20You%20Need%20In%202024%20To%20Become%20A%20Profitable%20Trader-Just%20In%20Vent%20EP10/"/>
    <id>http://example.com/2024/01/03/The%20Only%20Study%20Guide%20You%20Need%20In%202024%20To%20Become%20A%20Profitable%20Trader-Just%20In%20Vent%20EP10/</id>
    <published>2024-01-02T16:00:00.000Z</published>
    <updated>2025-04-02T19:23:52.801Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://s.draftai.cn/vent/trend_investing.png"></p><h3 id="开篇的话"><a href="#开篇的话" class="headerlink" title="开篇的话"></a>开篇的话</h3><p>Steven Hart是资深交易员兼教练，他的<a href="https://www.youtube.com/@thetradingchannel">频道</a>主要分享各种交易策略的教程和技巧，包括趋势跟踪和技术分析。频道有238万订阅会员，内容比较扎实、易懂。</p><blockquote><p>趋势投资是一种投资策略，它依赖于分析和识别金融市场中的趋势来做出投资决策。趋势投资的关键在于识别这些趋势，并在其发展的早期阶段进行投资。</p></blockquote><p>趋势投资通常涉及以下几个步骤：</p><ol><li><strong>市场观察</strong>：投资者不断观察市场，包括股票、债券、商品和货币等，以识别可能出现的趋势。</li><li><strong>趋势识别</strong>：使用技术分析工具，如移动平均线、相对强弱指数（RSI）和宏观经济指标，来识别正在形成的趋势。</li><li><strong>投资决策</strong>：一旦识别出明确的趋势，投资者会进行买入或卖出操作。如果趋势是上升的，投资者可能会买入；如果趋势是下降的，他们可能会卖出或做空。</li><li><strong>风险管理</strong>：趋势投资者会密切监控市场动态，并准备好在趋势逆转时迅速调整其仓位。</li><li><strong>退出策略</strong>：确定何时退出市场同样重要。这通常涉及到设定止损点，以限制潜在的亏损。</li></ol><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>作者分享了一份2024年成为盈利交易员的学习指南和课程计划。这份学习指南包括了基本知识的掌握、图表分析、交易策略的开发、风险管理、交易心理学的掌握、制定交易计划和模拟交易等内容。</p><h3 id="核心观点"><a href="#核心观点" class="headerlink" title="核心观点"></a>核心观点</h3><ul><li>[📚] 月份计划：学习基本知识、图表分析、交易策略、风险管理、交易心理学、制定交易计划和模拟交易</li><li>[📉] 月份一：掌握市场基础知识，包括货币对、点值、杠杆、经纪人类型等</li><li>[📈] 月份二：学习图表分析，包括蜡烛图、趋势市场、结构水平、图表模式和突破模式</li><li>[💼] 月份三：开发或学习盈利交易策略</li><li>[📊] 月份四：进行回测和优化交易策略</li><li>[💰] 月份五：学习风险管理，包括风险资本、每笔交易的风险和总体风险暴露</li><li>[🧠] 月份六：掌握交易心理学，包括情绪控制、风险管理和模拟交易</li><li>[✍️] 月份七：制定完整的交易计划</li><li>[📈] 月份八：开始模拟交易，测试交易计划的实际执行情况</li></ul><h3 id="第一个月：基础知识掌握"><a href="#第一个月：基础知识掌握" class="headerlink" title="第一个月：基础知识掌握"></a>第一个月：基础知识掌握</h3><ul><li><strong>市场基础</strong>：了解所交易市场的基本原理和运作机制,包括货币对、点数（pip）、杠杆和保证金等基本概念。</li><li><strong>交易平台熟悉</strong>：学习使用特定的交易平台（如TradingView），包括其功能和操作。</li><li><strong>经纪商类型</strong>：了解不同类型的经纪商（如市商Market Maker和ECN经纪商）及其运作方式。</li><li><strong>订单类型</strong>：学习不同的交易订单类型，例如限价单、止损单和市价单。</li><li><strong>交易操作</strong>：学习放置入场点、止损点和止盈点的方法，以及不同的订单类型（限价单、止损单和市价单）。</li><li><strong>持仓量与价差</strong>：掌握如何计算交易的持仓量和理解价差的概念。</li></ul><p><img src="https://s.draftai.cn/vent/202401031115668.png"></p><h3 id="第二个月：技术分析与图表理解"><a href="#第二个月：技术分析与图表理解" class="headerlink" title="第二个月：技术分析与图表理解"></a>第二个月：技术分析与图表理解</h3><ul><li><strong>蜡烛线图表</strong>：学习理解蜡烛线图的构成及其表示的市场信息。</li><li><strong>趋势识别</strong>：学会识别市场的趋势，如上升趋势和下降趋势。</li><li><strong>结构级别</strong>：识别并理解重要的市场结构级别，如支撑位和阻力位。</li><li><strong>蜡烛形态</strong>：学习不同的蜡烛形态，如吞噬形态、锤子线和射击之星。</li><li><strong>图形形态</strong>：识别和理解各种图形形态，如双底、双顶、头肩顶和反头肩形态。</li><li><strong>突破形态</strong>：了解和识别突破形态，如旗形和楔形。</li><li><strong>指标学习</strong>：如果选择使用指标，学习如相对强弱指数（RSI）、指数移动平均线（EMA）和平均真实波动幅度（ATR）等。<br><img src="https://s.draftai.cn/vent/202401031115000.png"></li></ul><h3 id="第三个月：交易策略开发"><a href="#第三个月：交易策略开发" class="headerlink" title="第三个月：交易策略开发"></a>第三个月：交易策略开发</h3><ul><li><strong>策略开发</strong>：结合前两个月学到的技术分析，创建自己的交易策略。</li><li><strong>条件设定</strong>：设定交易策略的先决条件，如市场趋势和特定指标的配置。</li><li><strong>入场点识别</strong>：根据规则确定交易的入场点。</li><li><strong>止损和止盈规则</strong>：制定明确的止损和止盈规则，以管理交易风险。<br><img src="https://s.draftai.cn/vent/202401031116915.png"></li></ul><h3 id="第四个月：策略回测"><a href="#第四个月：策略回测" class="headerlink" title="第四个月：策略回测"></a>第四个月：策略回测</h3><ul><li><strong>回测理解</strong>：学习什么是回测以及其重要性。</li><li><strong>历史数据测试</strong>：在特定货币对和时间框架上，用历史数据测试交易策略。</li><li><strong>策略评估</strong>：确保策略在过去的交易中具有盈利性。</li><li><strong>交易心理提升</strong>：通过回测过程提高交易心态和纪律。<br><img src="https://s.draftai.cn/vent/202401031117916.png"></li></ul><h3 id="第五个月：风险管理"><a href="#第五个月：风险管理" class="headerlink" title="第五个月：风险管理"></a>第五个月：风险管理</h3><ul><li><strong>制定风险管理计划</strong>：根据个人风险承受能力制定风险管理计划。<br><img src="https://s.draftai.cn/vent/202401031117203.png"></li></ul><h3 id="第六个月：掌握交易心理学"><a href="#第六个月：掌握交易心理学" class="headerlink" title="第六个月：掌握交易心理学"></a>第六个月：掌握交易心理学</h3><ul><li><strong>交易心理学</strong>：阅读、观看和倾听交易心理学相关的资料，特别是Mark Douglas的作品。</li><li><strong>心理强化策略</strong>：实施风险管理、回测等策略来改善交易心理。<br><img src="https://s.draftai.cn/vent/202401031117244.png"></li></ul><h3 id="第七个月：制定完整交易计划"><a href="#第七个月：制定完整交易计划" class="headerlink" title="第七个月：制定完整交易计划"></a>第七个月：制定完整交易计划</h3><ul><li><strong>交易计划制定</strong>：将交易策略和风险管理计划整合成书面的完整交易计划。</li></ul><h3 id="第八个月：模拟交易"><a href="#第八个月：模拟交易" class="headerlink" title="第八个月：模拟交易"></a>第八个月：模拟交易</h3><ul><li><strong>模拟交易实践</strong>：使用完整的交易计划进行模拟交易，适应真实市场交易环境。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://s.draftai.cn/vent/trend_investing.png&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;开篇的话&quot;&gt;&lt;a href=&quot;#开篇的话&quot; class=&quot;headerlink&quot; title=&quot;开篇的话&quot;&gt;&lt;/a&gt;开篇的话&lt;/h3&gt;</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="Trader" scheme="http://example.com/tags/Trader/"/>
    
  </entry>
  
  <entry>
    <title>Coin Bureau提出了2024年加密货币的10大预测-枫叶洞见EP09</title>
    <link href="http://example.com/2024/01/02/Coin%20Bureau%202024%20CRYPTO%20Predictions-Our%20Top%2010!!-Just%20In%20Vent%20EP09/"/>
    <id>http://example.com/2024/01/02/Coin%20Bureau%202024%20CRYPTO%20Predictions-Our%20Top%2010!!-Just%20In%20Vent%20EP09/</id>
    <published>2024-01-02T02:20:57.658Z</published>
    <updated>2024-01-02T02:20:57.659Z</updated>
    
    <content type="html"><![CDATA[<h3 id="开篇的话"><a href="#开篇的话" class="headerlink" title="开篇的话"></a>开篇的话</h3><p>CoinBureau 是youtube上最大的加密货币博主，有238万订阅用户。他对2024年加密货币进行了10大预测。值得关注，可以直接访问以下链接。<br><a href="https://www.youtube.com/watch?v=I_VPjZdEHm0&ab_channel=CoinBureau">Coin Bureau 2024 CRYPTO Predictions: Our Top 10!!</a></p><p><img src="https://s.draftai.cn/vent/vent1924_an_illustration_of_individuals_being_held_up_in_front__8b6834a1-e1da-49dd-8a49-e41c22af25a1.png" alt="Bitcoin Rise up"></p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>Coin Bureau提出了2024年加密货币的10大预测，包括比特币ETF的批准、比特币创下新的历史高点、比特币减半对价格的影响、下一轮加密货币牛市的主题、以太坊的第二层将被广泛采用等。</p><h3 id="核心观点"><a href="#核心观点" class="headerlink" title="核心观点"></a>核心观点</h3><ul><li>[📈] Coin Bureau提出了2024年加密货币的10大预测，包括比特币ETF的批准、比特币创下新的历史高点、比特币减半对价格的影响、下一轮加密货币牛市的主题、以太坊的第二层将被广泛采用等。</li><li>[💼] 比特币ETF预计会在2024年获批准，这可能导致比特币价格的大幅波动。</li><li>[📈] Coin Bureau预测比特币将在2024年创下新的历史高点，价格可能超过8万美元。</li><li>[💱] 比特币减半对价格的影响可能不会立即显现，甚至可能导致价格下跌。</li><li>[🐂] 下一轮加密货币牛市的主题可能与去中心化社交媒体、游戏或人工智能有关。</li><li>[💸] Coin Bureau认为以太坊的第二层将在2024年得到广泛采用，可能对以太坊及相关代币价格产生影响。</li><li>[🌐] 非美元稳定币可能开始在全球范围内用于支付，这可能对加密货币市场产生积极影响。</li><li>[🏦] Coin Bureau预测一些国家将开始在2024年使用比特币进行国际贸易，这可能推动比特币的国家级采用。</li><li>[📉] Coin Bureau认为公开交易的公司将在2024年开始增加比特币在其资产负债表上的持有量。</li><li>[🚀] Coin Bureau预测FTX将在2024年重新启动，并成为其他交易所的主要竞争对手。</li><li>[🗳️] Coin Bureau预测2024年将有更多支持加密货币的政治人物当选。</li><li>[🎁] Coin Bureau预测Coin Bureau Club在2024年将达到1万名会员，并将举办赠品活动。</li></ul><h3 id="更多细节"><a href="#更多细节" class="headerlink" title="更多细节"></a>更多细节</h3><ul><li><p>2024加密货币预测：比特币ETF获批，BTC创新高  00:00</p><ul><li>⭐ 2024年将批准比特币ETF</li><li>⭐ BTC将创下新的历史高点</li><li>⭐ 2024年BTC价格预计超过80,000美元</li><li>⭐ 比特币ETF的批准取决于机构投资者和宏观环境</li></ul></li><li><p>比特币减半对价格的影响不大，2024年将出现下一个加密货币牛市的主题  05:21</p><ul><li>⭐ 根据Black Rock的评论，至少有一些机构投资者认为比特币是“数字黄金”</li><li>⭐ 比特币减半对价格的影响不大，因为市场已经预期到了</li><li>⭐ 2024年将出现下一个加密货币牛市的主题，可能与去中心化社交媒体、游戏或人工智能有关</li><li>⭐ 以太坊的第二层将得到更多关注</li></ul></li><li><p>2024年以太坊将挑战其他一层区块链，稳定币将用于全球支付，部分国家将开始使用比特币进行国际贸易10:10</p><ul><li>⭐ 2024年以太坊将挑战其他一层区块链</li><li>⭐ 稳定币将用于全球支付</li><li>⭐ 部分国家将开始使用比特币进行国际贸易</li></ul></li><li><p>2022年伊朗和俄罗斯将开始使用比特币进行国际贸易15:08</p><ul><li>⭐ 伊朗和俄罗斯计划在2022年开始使用比特币进行国际贸易</li><li>⭐ 比特币的国家级采用曲线中的必要阶段</li><li>⭐ 公开交易公司将在2024年开始持有比特币</li><li>⭐ FTX交易所计划在2024年重新启动</li></ul></li><li><p>2024年将在全球选举中选出亲加密货币的政治家20:28</p><ul><li>⭐ FTX破产清算可能与IRS的指控无关</li><li>⭐ 2024年将有许多重大选举，加密政策可能在其中发挥关键作用</li><li>⭐ 人们开始意识到政府想要推出对人们金融自由有限制的数字货币</li><li>⭐ Coin Bureau Club计划在2024年达到1万会员</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;开篇的话&quot;&gt;&lt;a href=&quot;#开篇的话&quot; class=&quot;headerlink&quot; title=&quot;开篇的话&quot;&gt;&lt;/a&gt;开篇的话&lt;/h3&gt;&lt;p&gt;CoinBureau 是youtube上最大的加密货币博主，有238万订阅用户。他对2024年加密货币进行了10大预测。值得</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="Crypto" scheme="http://example.com/tags/Crypto/"/>
    
    <category term="Cryptocurrency" scheme="http://example.com/tags/Cryptocurrency/"/>
    
    <category term="BitCoin" scheme="http://example.com/tags/BitCoin/"/>
    
    <category term="Predictions" scheme="http://example.com/tags/Predictions/"/>
    
  </entry>
  
  <entry>
    <title>Meta AI实验室2023年最重要的10个工作兼风爷快评</title>
    <link href="http://example.com/2024/01/01/Meta%20AI%202023%20best%2010%20job%20and%20vent&#39;s%20comment/"/>
    <id>http://example.com/2024/01/01/Meta%20AI%202023%20best%2010%20job%20and%20vent&#39;s%20comment/</id>
    <published>2023-12-31T16:00:00.000Z</published>
    <updated>2024-01-02T02:20:57.659Z</updated>
    
    <content type="html"><![CDATA[<h2 id="开篇的话"><a href="#开篇的话" class="headerlink" title="开篇的话"></a>开篇的话</h2><p>ChaGPT的诞生，成为人类的AGI元年，犹如按下了AI研究的快进键。2023短短一年，AI社区诞生了数之不尽的明星项目和璀璨的开源成绩。在OpenAI不再Open之际，开源社区一度陷入对AI巨头不可追赶的绝望，直到Meta平地一声雷，接连开源包括 SAM、Llama2等重磅开源产品，让社区快速跟上了对大模型最前沿工作的进展。<br>风爷心目中，Meta地位迅速超赶谷歌成为，世界上最伟大的大企业之一。</p><h2 id="Meta-AI-2023年度十佳"><a href="#Meta-AI-2023年度十佳" class="headerlink" title="Meta AI 2023年度十佳"></a>Meta AI 2023年度十佳</h2><p>接下来是 Meta自己选的：</p><p>在 2023 年的尾声，我们为您呈现了今年我们分享的十大最令人瞩目的人工智能研究成果，每一项都开辟了探索新天地的大门。</p><p>   1️⃣ Segment Anything (SAM)<br>   这是通往图像分割领域首个全面模型的重要一跃。<br>   探索细节：<a href="http://bit.ly/3tyeJKu">bit.ly&#x2F;3tyeJKu</a><br>   <img src="https://s.draftai.cn/vent/20240101141917.png" alt="image.png"></p><blockquote><p>SAM 确实非常强大，文章一发布，基于它的应用和改造，数之不尽，风起云涌。大幅推进了CV在分割领域的进展。</p></blockquote><p>   2️⃣ DINOv2<br>   利用自我监督学习训练计算机视觉模型的首创方法，其成就匹配甚至超越了业界标准。<br>   探索细节：<a href="http://bit.ly/3TGTEIb">bit.ly&#x2F;3TGTEIb</a><br>   <img src="https://s.draftai.cn/vent/20240101142249.png" alt="image.png"></p><blockquote><p>和SAM不同的是DinoV2实际上是真正意义的CV大模型，旨在覆盖所有CV子领域的下游任务。文章还是比较谦虚，是非常重要的工作</p></blockquote><p>   3️⃣ Llama 2<br>   我们最新一代的开源大型语言模型，为研究和商业应用提供免费使用的新选择。<br>   探索细节：<a href="http://bit.ly/3RY66C6">bit.ly&#x2F;3RY66C6</a><br>   <img src="https://s.draftai.cn/vent/20240101142427.png" alt="image.png"></p><blockquote><p>Llama 2 的开源，让国产大模型的自主研发成为可能。赶英超美不是梦。在ClosedAI的大模型下溃不成军的开源大模型，正式走上了AI安全的道路。</p></blockquote><p>   4️⃣ Emu Video &amp; Emu Edit<br>   在文本到视频生成和文本指令控制的图像编辑方面的创新 AI 研究，推动生成性技术的质量新高。<br>   探索细节：<a href="http://bit.ly/3RZVZwU">bit.ly&#x2F;3RZVZwU</a></p><p>   5️⃣ I-JEPA<br>   一种自我监督的计算机视觉模型，它通过预测来学习理解世界，是 杨立昆老师 愿景中的一部分，旨在使 AI 如同动物和人类一样学习和推理。<br>   探索细节：<a href="http://bit.ly/3TA9oNk">bit.ly&#x2F;3TA9oNk</a><br>   6️⃣ Audiobox<br>   我们在音频生成领域的全新基础研究模型，为音频魔法的未来奠定了基础。<br>   探索细节：<a href="http://bit.ly/47ib6pQ">bit.ly&#x2F;47ib6pQ</a></p><p>   7️⃣ 脑解码 - 迈向实时重构视觉感知<br>   使用 MEG 技术，这个 AI 系统能够以史无前例的时间分辨率解码大脑中的视觉表征。<br>   探索细节：<a href="http://bit.ly/3vpgDNR">bit.ly&#x2F;3vpgDNR</a><br>   8️⃣ 开放催化剂演示<br>   这项服务为材料科学研究者提供了一个新平台，使他们能够快速模拟催化材料的反应性，超越现有计算方法。<br>   探索细节：<a href="http://bit.ly/3vphiij">bit.ly&#x2F;3vphiij</a><br>   9️⃣ 无缝通信<br>   这一新型 AI 翻译模型系列，提供准确保真和近乎实时的流式翻译服务。<br>   探索细节：<a href="http://bit.ly/3toBDE8">bit.ly&#x2F;3toBDE8</a></p><blockquote><p>很强大TTS&#x2F;ASR开源，没仔细测，这个领域也有效逼迫OpenAI对Whisper的开源和改进。</p></blockquote><p><img src="https://s.draftai.cn/vent/20240101162109.png" alt="image.png"></p><p>   🔟 ImageBind<br>   这是首个能同时处理六种模式数据的 AI 模型，它的出现使机器更接近于人类综合多感官信息的能力。<br>   探索细节：<a href="http://bit.ly/3NLUaBc">bit.ly&#x2F;3NLUaBc</a>“</p><blockquote><p>为多模态模型打开了一条不错的思路，但是对齐方法有点勉强。但是确实为社区的多模态模型实现了对GPT的短暂“领先”。（它的出现催生了类似于LLaVa的多模态模型，出生在GPT4-V之前。）可以看到这是多么疯狂的一年，数之不尽的模型进展，如长江只浪，延绵不绝。</p></blockquote><p><img src="https://s.draftai.cn/vent/20240101161926.png" alt="image.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;开篇的话&quot;&gt;&lt;a href=&quot;#开篇的话&quot; class=&quot;headerlink&quot; title=&quot;开篇的话&quot;&gt;&lt;/a&gt;开篇的话&lt;/h2&gt;&lt;p&gt;ChaGPT的诞生，成为人类的AGI元年，犹如按下了AI研究的快进键。2023短短一年，AI社区诞生了数之不尽的明星项目和璀</summary>
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="ai" scheme="http://example.com/tags/ai/"/>
    
    <category term="Meta" scheme="http://example.com/tags/Meta/"/>
    
  </entry>
  
  <entry>
    <title>美股2024年必赚的10只好股🔥(Top 10 AI Stocks to Get Rich in 2024)-枫叶洞见EP07</title>
    <link href="http://example.com/2023/12/29/Top%2010%20AI%20Stocks%20to%20Get%20Rich%20in%202024-Just%20In%20Vent%20EP%2007/"/>
    <id>http://example.com/2023/12/29/Top%2010%20AI%20Stocks%20to%20Get%20Rich%20in%202024-Just%20In%20Vent%20EP%2007/</id>
    <published>2023-12-28T16:00:00.000Z</published>
    <updated>2024-01-02T02:20:57.659Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://s.draftai.cn/vent/Top%2010%20Stocks%20to%20Get%20Rich%20in%202024%20.jpg" alt="Top 10 Stocks to Get Rich in 2024 .jpg"></p><h3 id="开篇的话"><a href="#开篇的话" class="headerlink" title="开篇的话"></a>开篇的话</h3><p><strong>Top 10 AI Stocks to Get Rich in 2024</strong><br>来自Alex 的推荐，关键是他对股票的分析背后的逻辑。不代表任何投资建议！以下是这十只股票。</p><table><thead><tr><th>股票代码</th><th>公司名</th><th>市值(美元)</th><th>净利润率(GAAP美国会计标准)</th></tr></thead><tbody><tr><td>QQQ</td><td>NASDAQ-100</td><td>Index of 100<br><br>Companies</td><td></td></tr><tr><td>MSFT</td><td>Microsoft</td><td>$2.8 万亿</td><td>39%</td></tr><tr><td>GOOG</td><td>Alphabet (Google)</td><td>$1.8 万亿</td><td>26%</td></tr><tr><td>AMZN</td><td>Amazon</td><td>$1.6 万亿</td><td>7%</td></tr><tr><td>NVDA</td><td>Nvidia</td><td>$1.2 万亿</td><td>51%</td></tr><tr><td>AMD</td><td>Advanced<br><br>Micro Devices</td><td>$2250 亿</td><td>5%</td></tr><tr><td>PANW</td><td>Palo Alto Networks</td><td>$940 亿</td><td>10%</td></tr><tr><td>FTNT</td><td>Fortinet</td><td>$450 亿</td><td>24%</td></tr><tr><td>PLTR</td><td>Palantir</td><td>$380 亿</td><td>13%</td></tr><tr><td>PATH</td><td>UiPath</td><td>$140 亿</td><td>-10%</td></tr></tbody></table><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>这篇内容是关于2024年投资股票的视频，作者分享了自己的研究和观点。他认为投资人应该持有一些能够赢得未来市场的股票，而不是依靠运气。他提到了一些股票，包括科技巨头和人工智能相关公司，以及一些新兴市场的潜力。</p><h3 id="Highlights"><a href="#Highlights" class="headerlink" title="Highlights"></a>Highlights</h3><ul><li>💼 持有一些能够赢得未来市场的股票是建立长期投资组合的基础。</li><li>💰 S&amp;P 500指数增长稳定，但NASDAQ 100指数增长更快，因为它包含更多科技公司。</li><li>🌐 Microsoft、Google和Amazon占据全球云服务市场的65%市场份额。</li><li>🖥️ Nvidia和AMD在数据中心GPU市场占据主导地位。</li><li>🛡️ 市场预测全球云安全市场将在未来十年内增长7倍以上。</li><li>📈 AI市场预计未来十年将增长15倍，对硬件和云服务提供商都有利。</li><li>📊 Palantir和UiPath是两家企业软件公司，提供与AI相关的解决方案。</li><li>🚀 市场将奖励那些快速发展、灵活适应变化的公司。</li></ul><h3 id="更多细节"><a href="#更多细节" class="headerlink" title="更多细节"></a>更多细节</h3><p>Detailed Summary for <a href="https://www.youtube.com/watch?v=jG0gBi7Ps4U&ab_channel=TickerSymbol:YOU">Top 10 Stocks to Get Rich in 2024 (Without Getting Lucky)</a> by <a href="https://monica.im/">Monica</a></p><p><a href="https://www.youtube.com/watch?v=jG0gBi7Ps4U&ab_channel=TickerSymbol:YOU&t=0.04">00:00</a> 这部分视频介绍了一个2024年的股票清单，其中每只股票都与人工智能相关，并且这些股票是作者自己的投资组合的基础。</p><ul><li>视频作者不是金融顾问，而是电气工程和数据科学背景，有10年的行业经验。</li><li>视频作者只在股票价格低于目标价位时购买股票。</li><li>视频作者提供了一个调查链接，以了解观众的需求，并在其他视频中详细介绍特定公司。</li><li>视频作者认为每个长期投资组合的基础应该是一个基金。</li></ul><p><a href="https://www.youtube.com/watch?v=jG0gBi7Ps4U&ab_channel=TickerSymbol:YOU&t=177.72">02:57</a> NASDAQ 100几乎翻了一番，原因是AI技术的早期采用者获得了优势，而更多的投资者也将关注于微软、谷歌和亚马逊等公司。</p><ul><li>NASDAQ 100在过去5年几乎翻了一番，原因是AI技术的早期采用者获得了优势。</li><li>高波动性对于长期投资者来说也是好事，因为更高的高点意味着更好的回报，更低的低点意味着更大的折扣。</li><li>微软、谷歌和亚马逊是全球云服务市场的巨头，拥有65%的市场份额。</li><li>这三家公司正在投资数十亿美元来建设世界先进的人工智能工作负载的基础设施。</li></ul><p><a href="https://www.youtube.com/watch?v=jG0gBi7Ps4U&ab_channel=TickerSymbol:YOU&t=357.319">05:57</a> 云服务提供商的硬件基础设施非常昂贵，而亚马逊、微软和谷歌等公司正在投入数十亿美元来研究和开发数据中心的GPU加速器。</p><ul><li>亚马逊在AWS上已经花费了超过1000亿美元，其中一半是在过去两年内。</li><li>Nvidia目前在数据中心GPU市场占有92%的份额，而AMD则是唯一的竞争对手，占有4%的份额。</li><li>企业加速器市场预计在未来十年内将以24%的复合年增长率增长，因此即使AMD无法赶上Nvidia，他们也有机会随着市场的增长而增长。</li></ul><p><a href="https://www.youtube.com/watch?v=jG0gBi7Ps4U&ab_channel=TickerSymbol:YOU&t=536.6">08:56</a> 通过持有几只股票，可以在多个快速增长的市场上获得赢家，包括数据中心GPU市场和CPU市场，以及人工智能和云安全市场。</p><ul><li>Nvidia和AMD之间的竞争可以占据整个数据中心GPU市场。</li><li>人工智能市场预计未来10年将增长15倍，云安全市场预计将增长7倍。</li><li>人工智能的发展将导致更多的网络威胁，因此网络安全市场将会更快增长。</li></ul><p><a href="https://www.youtube.com/watch?v=jG0gBi7Ps4U&ab_channel=TickerSymbol:YOU&t=714.56">11:54</a> Palar Solutions和UiPath是两个灵活且快速的人工智能公司，它们发布了新的工具来满足客户需求。</p><ul><li>Palar Solutions发布了人工智能平台AIP，成为最受欢迎的平台。</li><li>UiPath建立了Chat GPT集成，用于从长文档中提取信息，使自动化过程更快更简单。</li><li>这两家公司目前面临来自微软和亚马逊等科技巨头的竞争。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://s.draftai.cn/vent/Top%2010%20Stocks%20to%20Get%20Rich%20in%202024%20.jpg&quot; alt=&quot;Top 10 Stocks to Get Rich in 2024 .jpg&quot;&gt;</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="ai" scheme="http://example.com/tags/ai/"/>
    
    <category term="#NVDA" scheme="http://example.com/tags/NVDA/"/>
    
    <category term="#QQQ" scheme="http://example.com/tags/QQQ/"/>
    
    <category term="#PLTR" scheme="http://example.com/tags/PLTR/"/>
    
  </entry>
  
  <entry>
    <title>凯西·伍德刚刚向特斯拉投资者发出警告-枫叶洞见EP05</title>
    <link href="http://example.com/2023/12/28/Cathie%20Wood%20Just%20WARNED%20Tesla%20Investors-Just%20In%20Vent-EP05/"/>
    <id>http://example.com/2023/12/28/Cathie%20Wood%20Just%20WARNED%20Tesla%20Investors-Just%20In%20Vent-EP05/</id>
    <published>2023-12-27T16:00:00.000Z</published>
    <updated>2025-04-02T19:55:28.263Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://s.draftai.cn/vent/202312281443292.png"></p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>Cathie Wood在YouTube上警告了特斯拉的投资者，她认为特斯拉的估值取决于电动车和自动驾驶出租车平台的发展，同时也提到了特斯拉的财务状况和经营风险。</p><h3 id="核心观点"><a href="#核心观点" class="headerlink" title="核心观点"></a>核心观点</h3><ul><li><p>[🚗] 特斯拉的估值取决于电动车和自动驾驶出租车平台的发展</p></li><li><p>[📉] 特斯拉的毛利率下降引起了投资者的担忧</p></li><li><p>[💰] 特斯拉在研发上花费巨大，对盈利造成影响</p></li><li><p>[📈] 特斯拉股价在2023年上涨了110%</p></li></ul><h3 id="更多详情"><a href="#更多详情" class="headerlink" title="更多详情"></a>更多详情</h3><p>(00:00) ARK Invest CEO表示特斯拉的估值可能达到1400美元</p><ul><li><p>⭐ 特斯拉的估值中有三分之一与电动汽车和扩大规模有关</p></li><li><p>⭐ 特斯拉在自动驾驶领域处于领先地位</p></li><li><p>⭐ 监管机构对自动驾驶的支持有助于降低风险</p></li><li><p>⭐ 特斯拉的估值可能会更快实现</p></li></ul><p>(03:10) 特斯拉股票在2023年是否值得购买？</p><ul><li><p>⭐ 特斯拉股票在2023年上涨了110%</p></li><li><p>⭐ 特斯拉的自动驾驶和全自动驾驶使驾驶更安全</p></li><li><p>⭐ 特斯拉在最新财务报告中超过了华尔街的预期</p></li><li><p>⭐ 特斯拉在该时期生产了48万辆汽车</p></li></ul><p>(06:12)特斯拉季度毛利率下降至18%，投资者担忧</p><ul><li><p>⭐ 特斯拉季度毛利率从去年的25%下降至18%</p></li><li><p>⭐ 特斯拉多次降价以刺激需求</p></li><li><p>⭐ 研发费用达到9.43亿美元，同比增长41%</p></li><li><p>⭐ 特斯拉希望通过研发投入在电动车领域取得领先地位。</p></li></ul><p><img src="https://s.draftai.cn/vent/202312281447681.png"></p><h3 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h3><p><a href="https://www.youtube.com/watch?v=mC-shura-34">Cathie Wood Just WARNED Tesla Investors</a></p><p>让我们请来ARK投资公司首席执行官凯西·伍德。在ARK创新ETF中,特斯拉是她持有量最大的股票。凯西,很高兴在Fast上与您见面。</p><p>我很高兴能和你在一起,梅利莎。谢谢你邀请我。你知道,这里的交易员和我讨论了特斯拉,你对特斯拉的目标价是每股1400美元。这里有个人说,他可以接受1400美元,因为这仍然相当看涨。到2027年,为了达到每股1400美元,对其业务做了什么假设呢?</p><p>嗯,我们估值的约三分之一与电动汽车EV相关,以及扩大规模,就像你提到的土耳其工厂,许多国家都希望拥有一家特斯拉工厂,因为这是新的世界秩序,对吧?所以这一切都在朝着正确的方向发展。然后三分之二的估值与自动驾驶和自动驾驶出租车平台有关。我们认为特斯拉在美国具有领先地位。它收集的关于我们道路甚至世界其他道路的数据,比所有其他公司加在一起还要多。因此,它拥有更多的极端情况,可能会成为让人们从A点到B点尽可能快速和安全的公司。所以它会赢得大部分市场份额。</p><p>无论是我们的基本情况,这只意味着好的,自动驾驶可能需要更长的时间来实现,或者乐观情景是它的发展速度更快。即使花更长时间才能实现,1400美元的目标价也依然有效。这似乎非常乐观,即使是特斯拉在自动驾驶方面需要的监管批准,因为特斯拉车主拥有他们的汽车,然后白天开车带人赚钱的模式,在接受程度方面也似乎很乐观,凯西。那么对于估值的三分之二,您觉得基本情况下是否存在任何风险呢?</p><p>嗯,实际上,风险正在下降,因为监管机构非常依赖数据。监管机构特别是在运输部门遇到的情况是,美国的汽车死亡人数在过去5至10年中从每年3万人增加到4.5万人,这与几十年来因汽车安全措施而下降的趋势形成反差。所以国家高速公路交通安全委员会和其他运输主管部门希望扭转这一趋势。这是为什么?很大一部分原因是发短信,因此不成比例地更多的年轻人死于车祸。</p><p>所以,你应该在2023年买特斯拉股票吗?顶级电动汽车业务总是吸引投资界的关注。包括特斯拉在内的许多科技股票在2023年表现异常出色,特斯拉股价至8月第一周已涨幅高达110%。同时,有利的市场环境也有帮助,正如纳斯达克综合指数全年上涨34%可以看出的那样。纳斯达克综合指数并不每年上涨35%,这表明2023年不仅仅是特斯拉表现出色。</p><p>许多科技股票在今年也表现异常出色,正如我在上一段关于Facebook在今年也产生了大量回报的视频中所介绍的那样,几乎相当于特斯拉。投资者可能正在重新看好增长股,2022年是非常困难的一年。我最喜欢的许多股票大幅下跌,2022年下跌了30%至40%。这对许多不知道这种情况不可避免的投资者来说可能是一个可怕的情况。</p><p>只因为特斯拉股票在2023年表现非常出色,我认为一个成熟的人,一个见多识广的人会告诉你,你不应该感到惊讶,如果几年后特斯拉有一个艰难的4到6个月,股票下跌20%至30%。一旦你有了经验,看到这些情况更频繁,你就会习以为常,不会像以前那样恐慌。</p><p>那么该公司的业务今天是否值得加入您的投资组合呢?我们将讨论如果您还没有买特斯拉,现在是不是买特斯拉股票的好时机,特斯拉股票已经上涨了110%。</p><p>数据支持特斯拉正在做的事情。道路上80%至90%的事故都是人为错误引起的。如果您把人从方程中取出,使用AI让人们尽可能快速安全地从A点到B点,我认为当局和监管机构会被数据说服。事实上,就特斯拉汽车中的死亡人数而言,他们已经被说服了。他们检查了死亡情况,说大部分不是特斯拉的错。哦,顺便说一下,开特斯拉车的人启用自动驾驶和FSD时比其他车安全40%以上。</p><p>如果您看最近的趋势,特斯拉确实打败了华尔街的预测,最新财报显示收入为250亿美元,摊薄后的每股收益为0.78美元。该公司在这一时期还生产了48万辆汽车,同比增长86%。但是,股东并不高兴看到特斯拉的毛利率从2022年第二季度的25%下降到最近一个季度的18%。这与去年相比有所下降。特斯拉去年的毛利率为25%,现在是18%。这引起了投资者的担忧。</p><p>管理层今年多次被迫削减汽车价格,这也是营业利润率萎缩的一个原因。我认为您应该理解他们为什么不得不这样做。他们不得不这样做,以在消费者面临高利率和通货膨胀压力的时期刺激需求。沃伦·巴菲特总是说,在分析一家公司时,您还应该查看经济形势。随着更高的利率和通货膨胀压力,大宗商品价格大幅上涨。这种情况下,有多少人会提前买一辆特斯拉呢?</p><p>即使那些通常想买特斯拉的人,也可能会推迟购车计划。在那种时候,如果您以折扣价出售汽车,那么更多的人可能会购买。但当您这样做时,销量虽然会上升,但毛利率也会受到影响。我不认为特斯拉在这里做错了什么。沃伦·巴菲特总是说,您应该根据国家的经济形势运营。所以定价削减当然应该为萎缩的运营利润率承担很大一部分责任。</p><p>然而,特斯拉本季度的研发费用为9.43亿美元,同比增长41%,也损害了盈利能力。特斯拉想快速增长,想领先于电动车,这意味着要在研发上花费大量资金。这些费用高达9.4亿美元,几乎10亿美元。特斯拉在一个季度的研发上花了近10亿美元,同比增长41%。即使利润率很高,任何人都会意识到一个季度就花10亿美元用于研究。这对盈利能力也产生了很大影响。</p><p>但是从长远来看,这10亿美元的研发投入将非常有利可图,虽然人们今天看不到,但这10亿美元的研发投入最终会非常有价值。非常感谢您的关注,希望大家有美好的一天!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://s.draftai.cn/vent/202312281443292.png&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h3&gt;&lt;p&gt;Cathi</summary>
      
    
    
    
    <category term="Investment" scheme="http://example.com/categories/Investment/"/>
    
    
    <category term="ARK" scheme="http://example.com/tags/ARK/"/>
    
    <category term="Tesla" scheme="http://example.com/tags/Tesla/"/>
    
  </entry>
  
</feed>
