<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>关于亚马逊和蓝色起源 — 亚马逊创始人Jeff Bezos访谈 - 2023年12月15</title>
    <link href="/2023/12/26/Jeff%20Bezos-Amazon%20and%20Blue%20Origin%20-%20Lex%20Fridman%20Podcast/"/>
    <url>/2023/12/26/Jeff%20Bezos-Amazon%20and%20Blue%20Origin%20-%20Lex%20Fridman%20Podcast/</url>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=DcWqzZ3I2cY&ab_channel=LexFridman">访问地址</a></p><p>Lex Fridman是一位主持人和研究人工智能和机器学习的专家。他主持着Lex Fridman Podcast，并在Instagram上分享关于机器人和人类的照片和视频。<br>Lex Fridman对人工智能和机器学习有着浓厚的兴趣，并在他的节目中与各种领域的专家进行访谈，包括马斯克、贝索斯、老黄等。他在Instagram上有超过1.1百万的粉丝，并且他的帖子数量已经超过749个。</p><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>亚马逊和蓝色起源公司的创始人杰夫·贝索斯在Lex Fridman播客#405中，分享了他的童年经历、对太空探索的热爱、对人类未来太空生活的设想，以及他的创新思维方式。贝索斯强调了在解决问题时的“漫步”思考方式，认为真正的创新需要时间和空间去探索和发现。他还分享了蓝色起源公司的一些太空基础设施项目，包括新格伦火箭和蓝环项目。</p><h3 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h3><ul><li>[🚀] 杰夫·贝索斯分享了他在德克萨斯州农场度过的童年，以及他的祖父对他的影响。他的祖父是一个真正的农场主，教会他解决问题的能力和自力更生的精神。</li><li>[🌌] 贝索斯在五岁时看到尼尔·阿姆斯特朗登月，从此对太空和太空探索产生了热爱。他希望看到数万亿的人类生活在太空中，这需要建造巨大的空间站。</li><li>[💡] 贝索斯强调了在解决问题时的“漫步”思考方式。他认为，真正的创新需要时间和空间去探索和发现，而不是直线思考。他鼓励人们在面对新的想法和挑战时，给自己充分的时间和空间去探索可能的解决方案。</li><li>[🚀] 贝索斯分享了蓝色起源公司的一些太空基础设施项目，包括新格伦火箭和蓝环项目。新格伦火箭是一种大型重型运载火箭，能将约45吨的有效载荷送入近地轨道。蓝环项目则是一种能将多达3000公斤的有效载荷送至地球同步轨道或月球附近的空间船。</li></ul><h2 id="分点总结如下："><a href="#分点总结如下：" class="headerlink" title="分点总结如下："></a>分点总结如下：</h2><h3 id="童年生活"><a href="#童年生活" class="headerlink" title="童年生活"></a>童年生活</h3><p>他母亲17岁时生的贝佐斯，4岁到16岁跟祖父在牧场生活。<br>祖父动手能力很强（修好几乎报废的推土机），对他影响最大。<br>干完牧场各种活儿，下午和祖父一起看肥皂剧《The Days of Our Lives》（我们的生活） </p><h3 id="自我认知"><a href="#自我认知" class="headerlink" title="自我认知"></a>自我认知</h3><p>来自斯里兰卡天才同学Yosanta，用10s解出困扰他和另外一个同学3小时的难题。（Youtube评论区此同学大神现身）<br>这让贝索斯意识到，即使再努力，他未来也只是一名平庸的物理学家，立马转学计算机科学专业。<br>著名传记作家Walter Isaacson 认为贝佐斯在“思想实验”水平上与爱因斯坦一个级别。 而贝佐斯对自己的认知：“我就是一个发明家。我善于观察事物。” </p><h3 id="蓝色起源"><a href="#蓝色起源" class="headerlink" title="蓝色起源"></a>蓝色起源</h3><p>尤里·阿列克谢耶维奇·加加林（Yuri Alekseyevich Gagarin）是苏联的一名宇航员，也是人类历史上第一个进入太空的人。 他在1961年4月12日进行了一次为期108分钟的太空旅行，以此完成了对地球的一次全轨道飞行。 加加林据说在太空看到地球时，说： “my God, it’s blue.” 贝佐斯的火箭公司“Blue Origin”的名字由来于此。 </p><ul><li>🚀 太空竞赛对人类历史产生了巨大影响，激发了人们对太空探索的兴趣。</li><li>🚀 Bezos希望未来数千年人类能在太阳系中居住，实现太空殖民和资源开发。</li><li>🚀 他认为建造巨大的太空站是实现这一愿景的关键。</li><li>🚀 Bezos强调了太空探索对保护地球的重要性，认为太空旅行是保护地球的一种途径。</li><li>🚀 Blue Origin正在研发新的火箭，计划在2024年进行首次发射。</li><li>🚀 Bezos对加快Blue Origin的发展进程充满期待，希望成为世界上最果断的公司之一。</li></ul><h3 id="Day-One思想"><a href="#Day-One思想" class="headerlink" title="Day One思想"></a>Day One思想</h3><blockquote><p>贝佐斯的Day One思想，应该是被新bd直接copy了。 </p></blockquote><p>核心理念：每天都像公司成立的第一天那样，带着重新开始的创业精神，快速迭代和革新，不被过往路径依赖或自我一致性限制，保持开放思维，与时俱进。<br>如何避免Day two（停滞&#x2F;衰退）： </p><ol><li>保持对客户的痴迷； </li><li>批判看待代理变量（不被过时的运营指标束缚）；积极重用外部新趋势；</li><li>保持高速决策（150w人的亚马逊，行动依然迅速）</li><li>六页纸开会 贝索斯在Amazon和Blue Origin开会，都使用6页纸memo，为什么不用PPT？ 用PPT开会的问题： <ol><li>PPT是一种说服工具，不利于“寻求真理”。</li><li>只给要点，容易藏匿模糊的思考。 </li><li>对演示者友好，对听众困难。 </li><li>中途容易打断提问，讨论低效。<br>六页纸开会好处：</li></ol></li><li>写6页memo需要投入大量时间和精力，迫使作者做系统思考。</li><li>memo以逻辑叙述方式展开，思考更明晰和严谨，不能藏匿思维漏洞。 </li><li>开会前阅读或开会时一起读，确保与会人在同一个起点，真正讨论问题、激发思考。 </li><li>部分疑问能随着阅读Memo得到解答，避免无效提问，节省时间。</li></ol><blockquote><p>新bd的“飞阅会”，也源于亚马逊的这套方法论，确实好用！</p></blockquote><blockquote><p>wangxin是最早的c2c之王，但是zhangyiming显然在短暂的共事过程中，学得更深入，找到了更好的赛道应用jeff 这一套。</p></blockquote><h3 id="决策技巧"><a href="#决策技巧" class="headerlink" title="决策技巧"></a>决策技巧</h3><p>贝佐斯非常善于决策，比如蓝色起源的目标是成为世界上”最具决策力的公司”。 最出名的是“单向门”和“双向门”决策：难逆转的重大决策是“单向门”，慎重决策；大多数决策是“双向门”决策，要快速决策。<br>除此之外，还有很多有趣的原则：</p><ol><li>“不同意但执行”原则：当团队成员意见跟他不一致，他会说自己不同意，但全力支持执行。</li><li>不要妥协，要寻求真理：妥协带不来真知，决策尽可能追求事物的本质真理。</li><li>当数据和叙事不一致时，相信叙事。（如客户抱怨时，即使数据正常，也要相信客户视角）</li></ol><p>最后一条原则，有个小故事： 亚马逊指标显示客服电话平均等待时间少于60秒，但客户抱怨明显要久得多。 在一次业务回顾会上，贝佐斯当场打客服电话，全场高管沉默等待，发现等待时间远超10分钟。 </p><p>另外，贝佐斯提到人是社会动物，而非理性动物。真相难听，但组织高绩效需要truth telling机制，明确告诉员工这不舒服很正常，鼓励大家直言不讳。 他一般在会议中最后发言，让大家客观表达自己的观点，不会被他的意见所影响。</p><h3 id="Papercut问题"><a href="#Papercut问题" class="headerlink" title="Papercut问题"></a>Papercut问题</h3><p>“papercut”指微小但令人烦恼的问题或困扰。就像一个纸割伤虽然看起来不大，却能引起不成比例的疼痛或不适，一些看似微不足道的问题或困扰也可能给人带来相当大的困扰或不便。 贝佐斯的做法：安排专门团队致力于修复小的缺陷（Papercut问题），其他人专注于大的改进。 </p><h3 id="他对AI的一些观点"><a href="#他对AI的一些观点" class="headerlink" title="他对AI的一些观点"></a>他对AI的一些观点</h3><p>贝佐斯认为 ChatGPT 这样的大语言模型更像是”发现”而不是”发明”。 AI模型不是设计完成的工程对象。我们仍不断被它们的新能力所惊讶。 他对AI 模型更有可能帮助人类而不是伤害我们持乐观态度。</p>]]></content>
    
    
    <categories>
      
      <category>Investment</category>
      
    </categories>
    
    
    <tags>
      
      <tag>amazon</tag>
      
      <tag>blue origin</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>特斯拉的故事波折与重新估值-2023年11月-枫叶洞见EP02</title>
    <link href="/2023/12/26/Tesla%20in%20November%202023-Story%20twists%20and%20turns-Just%20In%20Vent%20EP02/"/>
    <url>/2023/12/26/Tesla%20in%20November%202023-Story%20twists%20and%20turns-Just%20In%20Vent%20EP02/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s.draftai.cn/vent/20231226004310.png" alt="cybertruck"></p><h2 id="开篇的话"><a href="#开篇的话" class="headerlink" title="开篇的话"></a><strong>开篇的话</strong></h2><p>Aswath Damodaran教授在最近（2023年11月）对特斯拉进行重新估值。他把特斯拉拆解成4个部分，分别是<strong>汽车业务</strong>、<strong>能源业务</strong>、<strong>软件业务</strong>和<strong>无人出租车业务</strong>。根据这四个部分的业务的营收和增长情况，他给特斯拉的估值是<strong>每股价值约为180美元</strong>。</p><h2 id="作者简介"><a href="#作者简介" class="headerlink" title="作者简介"></a><a href="https://www.blogger.com/profile/12021594649672906878">作者简介</a></h2><ul><li><p>阿斯瓦斯·达莫达兰（Aswath Damodaran）教授是一位著名的金融学者和教育家，主要在纽约大学斯特恩商学院（New York University Stern School of Business）任教。他以其在投资估值（investment valuation）、公司金融（corporate finance）和股票投资策略（stock investment strategies）方面的专业知识而闻名。</p></li><li><p>他出版了多本关于估值和金融的书籍，这些书籍被广泛用作学术和实践领域的教材和参考资料。</p></li><li><p>他经常在Twitter和YouTube上发布关于金融市场和投资的见解。他的教学风格通俗易懂，能够将复杂的金融理论简化为容易理解的概念，这使他在学生和金融专业人士中都非常受欢迎。</p></li></ul><h2 id="特斯拉近况"><a href="#特斯拉近况" class="headerlink" title="特斯拉近况"></a><strong>特斯拉近况</strong></h2><p>我的最后一次对特斯拉的估值还不到十个月，虽然在日历时间上不算长，但对于特斯拉这支股票来说，感觉就像是一段漫长的时间。更新公司估值的前导是从股价开始，股价是市场对公司健康状况的晴雨表。这支股票在年初出现了一段低迷，但在上半年迅速恢复，巅峰时期股价接近每股300美元。</p><p><img src="https://s.draftai.cn/vent/20231226001712.png" alt="image.png"></p><p>过去的四个月对股票进行了考验，它已经回撤了今年收益的相当一部分，股价在2023年10月30日跌破了200美元。由于盈利报告通常被视为动力转变的催化剂，我突出了2023年期间的四份盈利报告，并将其与预期进行了比较。2023年1月的第一份盈利报告是唯一一次超出预期的，4月的报告与预期相符，而7月和10月的报告则落后于预期。</p><p>每股收益的关注点忽略了特斯拉的很多故事，深入挖掘财务报表并检视公司在更广泛的运营指标上的表现是很有启示性的。</p><p><img src="https://s.draftai.cn/vent/20231226001749.png" alt="image.png"></p><p>在截至2023年9月的12个月内，特斯拉报告了107亿美元的营业利润，收入为959亿美元；这使得他们的收入远远超过了我2013年预测的650亿美元，尽管营业利润率为11.18%，低于我预计的12.5%。这使得特斯拉成为全球第十一大汽车公司，以收入计算，并且在该名单上排名第七，利润最高，这使得那些对其认为它只是一个过时的潮流的人越来越难以辩驳。从财务报告中分析这一消息，以下是报告所揭示的业务分组情况：</p><ul><li>汽车业务：特斯拉的汽车业务在2020年至2022年之间的火爆增长势头放缓，第三季度同比收入增长率降至个位数，但考虑到汽车行业销售平稳和电动汽车市场疲软，它仍然是一个亮点。更令人失望的数字是汽车利润率从2022年水平下降，尽管特斯拉在第三季度的17.42%毛利率对于特斯拉来说令人失望，但对于几乎任何竞争对手来说，这都是一个值得庆祝的原因。</li><li>能源业务：特斯拉的能源业务在2016年收购Solar City后得到了发展，今年表现强劲，从2022年的公司收入的4.8%上升到截至2023年9月的12个月内的6.2%。与此同时，该业务的盈利能力在过去的12个月也大幅增长，尽管其中一部分增长将平均化，但其中一部分可以归因于从能源发电转向储能解决方案（电池组和其他）的重点转变。</li></ul><p>简而言之，特斯拉的财务报告展示了期望对市场对其中的新闻反应的影响。特斯拉在疫情后的收入和盈利激增导致了对该公司在本十年内能够做到什么的不切实际的高期望，而特别是在过去两个季度中的数字则起到了现实检验的作用。</p><ol><li>价格下调：在2023年期间，特斯拉多次降低其产品的价格，最近一次是在本月早些时候。Model 3的1250美元降价将使其价格降至约39000美元，使其在美国大众汽车市场上即使仅从价格角度来看也具有竞争力。部分价格下调是战术性的，是对竞争的回应，无论是当前的竞争还是预测的竞争，但其中一部分可能反映了该公司业务模式的转变。</li><li>全自动驾驶（FSD）：作为一家公司，特斯拉将其全自动驾驶技术推向了前沿，尽管特斯拉在自动驾驶技术方面领先于竞争对手的程度存在分歧，以及自动驾驶的长期前景。其新颖性和新闻价值使其成为辩论的核心主题，特斯拉的粉丝和批评者利用其成功和失败来撰写社交媒体帖子。虽然特斯拉的车辆配备了自动驾驶功能作为标准配置，但其提供的FSD软件仍处于测试版，提供了一个增强的自动驾驶模型，但价格为12,000美元。FSD的新闻报道还重新引发了关于特斯拉机器人出租车业务的讨论，公司泄露了一款专门针对该业务的25,000美元的车辆。</li><li>Cybertruck：经过多年的等待，特斯拉Cybertruck终于来了，它也因其独特的设计和特斯拉进入传统汽车公司仍然主导的市场而引起了过多的关注。虽然对于这款产品是一个小众产品还是会改变卡车市场仍然存在争议，但它无疑吸引了人们对该公司的关注。事实上，该公司的预订追踪器记录了超过两百万个预订（带有定金），尽管根据历史经验，实际销量将远远低于这些数字。</li></ol><p>这就是特斯拉的基本盘，关于这家公司还有许多其他故事，但这是常有的事。我们将重点关注这三个故事，因为它们有可能颠覆或改变特斯拉的叙事，并进而影响其价值。</p><h2 id="故事和估值：重新审视和重新评估"><a href="#故事和估值：重新审视和重新评估" class="headerlink" title="故事和估值：重新审视和重新评估"></a><strong>故事和估值：重新审视和重新评估</strong></h2><p>在我对特斯拉的估值中，我将其视为一家汽车公司，其他业务的价值都包含在总收入中，而不是单独列出。这并不意味着它们对价值的贡献很大，而是价值的增加被混在了一个价值输入中，而不是单独估计。在我对2023年初的估值中，我估计特斯拉的运营利润率为16%，远高于汽车行业的平均水平，因为我相信软件和&#x2F;或机器出租车业务除了带来额外的收入外，还会增加运营利润率，因为它们是高利润的业务。</p><p>今年关于特斯拉的新闻报道让我重新评估了这一观点，因为它们符合特斯拉的叙事，即<strong>特斯拉不仅相信软件和自动驾驶出租车业务作为独立业务具有重要的价值潜力</strong>，而且正在采取相应的行动。为了理解其中的原因，让我分别介绍这三个新闻故事，并将它们融入我的特斯拉叙事中。</p><ol><li><p><strong>Cybertrucks</strong>：将Cybertruck效应融入特斯拉叙事中最容易的新闻项目。如果预订量是潜在需求的指标，并且Cybertruck代表了一个迄今未开发的市场延伸，那么它确实增加了特斯拉的收入增长潜力。有两个潜在的负面因素需要考虑，马斯克在最近的盈利电话会议中提到了它们。首先，即使通过巧妙的设计选择，按照传闻的定价，这些卡车的利润率将低于高端产品。另一个是Cybertruck很可能需要专门的生产设施，从而增加了再投资的需求。如果Cybertruck销售火爆，需求强劲，积极因素将超过消极因素，但如果热度消退，它可能会成为一个分散注意力、降低价值的小众产品。Cybertrucks的附加值也将部分取决于购买者的身份，如果销售来自其他公司的卡车买家，而不是特斯拉汽车买家，特斯拉将获得更多收益，但这将 cannibalize 自己的销售。</p></li><li><p><strong>FSD</strong>：当我看到关于特斯拉FSD研究的竞争论点时，我觉得双方都有一定道理。从积极的一面来看，特斯拉在这条道路上显然比其他任何公司都更进一步，不仅从技术角度，还从商业模式和营销角度。虽然我不认为以12000美元的附加费来销售FSD会创造一个大市场，但降低价格将不仅为特斯拉驾驶员提供软件销售的机会，甚至可能为其他汽车制造商打开大门。此外，对我来说，特斯拉的无人出租车业务现在已经从可能性转变为可行性，因此值得认真对待。从消极的一面来看，我确实同意世界还没有完全准备好迎接无人驾驶汽车，而急于将产品推向市场可能会带来灾难性后果。</p></li><li><p><strong>价格下调</strong>：特斯拉的价格下调导致特斯拉支持者之间出现分歧，一些人认为这是特斯拉最近定价困境的原因，而另一些人则认为这是推动其全球统治使命的一招高招。为了确定哪一方的观点更为现实，我决定看一下价格下调对一家普通公司的价值产生的影响。价格下调的第一阶段效应是负面的，因为降低价格会降低利润和利润率，这很容易计算。而第二阶段效应则比较棘手，我在下图中列出了可能性及其价值后果。<img src="https://s.draftai.cn/vent/20231226002234.png" alt="image.png"></p><p> 简而言之，降价可以并且通常会改变销售数量，可能抵消部分降价的不利影响（战术性），使竞争对手难以跟上或进入你的业务（战略性），并扩大附带或补充业务蓬勃发展的潜力（协同效应）。这个数据解释了关于特斯拉降价的分歧，悲观主义者认为电动汽车需求对于能够弥补较低利润率的销量增加来说过于不弹性，而乐观主义者认为较低利润率带来的价值损失将被特斯拉市场份额的长期增加以及他们的软件和机器人出租车业务的价值增加所抵消。</p></li></ol><p>全面回顾一下，我将特斯拉分为四个业务领域 - <strong>汽车业务</strong>、<strong>能源业务</strong>、<strong>软件业务</strong>和<strong>无人出租车业务</strong>。我知道会有特斯拉的乐观主义者认为特斯拉还可以进入其他业务，包括保险和机器人，但目前我认为公司已经忙得不可开交。我在下面的图片中观察这些业务的市场潜力和盈利能力，以及特斯拉在每个业务领域的地位。<br><img src="https://s.draftai.cn/vent/20231226002401.png" alt="image.png"></p><p>请注意，就收入潜力而言，汽车业务是迄今为止最大的，但在盈利能力方面落后于其他业务，尤其是软件和无人驾驶出租车业务，其中单位经济效益较好，利润率更高。还要注意，对于无人驾驶出租车和汽车软件业务的未来估计是不确定的，因为它们仍处于初创阶段，我们还有很多不了解的地方。我对每个业务的特斯拉故事如下，包括收入和盈利假设，按业务细分。<br><img src="https://s.draftai.cn/vent/20231226002438.png" alt="image.png"></p><p>有了这些故事，我估计了企业的收入、盈利和现金流，总结出了公司的收入、盈利和现金流，并利用这些现金流估计了公司每股的价值。</p><p>总的来说，根据我对特斯拉业务的细分和考虑到不同业务的增长和盈利能力的差异，<strong>我得出的每股价值约为180美元。</strong><br>这比我年初的估计要高，其中一部分增长来自于附加业务的更高盈利潜力，以及对每个业务的更大收益预期。</p><p>请注意，这些企业都是相互关联的，因为驱动汽车和能源业务的电池技术是共享的，而全自动驾驶软件销售将与汽车销售挂钩。因此，您将无法以至少这些估值的方式拆分或出售这些企业，但这确实提供了投资者在该公司中应该关注的方向。因此，由于软件和机器人出租车与全自动驾驶系统(FSD)相关联，FSD前线的任何进展(失败)都将对价值产生影响。</p><p>当你审查我的故事和数字时，你对特斯拉的未来肯定会有非常不同的看法。你可以<a href="https://s.draftai.cn/vent/Tesla2023OctDIY.xlsx">直接下载excel</a>并根据你自己的认知进行预测。</p><p>所以，如果你认为我严重低估了机器人出租车业务的规模，请自行判断它有多大，但请注意，扩大这个业务将使你的汽车和软件业务变小。毕竟，如果每个人都乘坐机器人出租车，汽车销量应该会下降，现有车主可能不太愿意为全自动驾驶套件额外支付费用。</p>]]></content>
    
    
    <categories>
      
      <category>Tech News</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tesla</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024年我们投资什么以及2023年总结-枫叶洞见EP03</title>
    <link href="/2023/12/26/How%20We&#39;re%20Investing%20in%202024-Just%20In%20Vent%20EP03/"/>
    <url>/2023/12/26/How%20We&#39;re%20Investing%20in%202024-Just%20In%20Vent%20EP03/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s.draftai.cn/vent/202312261919320.png"></p><h2 id="开篇的话"><a href="#开篇的话" class="headerlink" title="开篇的话"></a><strong>开篇的话</strong></h2><p><a href="https://www.youtube.com/@TickerSymbolYOU">Ticker Symbol: YOU</a> 是油管的投资频道，有28万订阅用户。主要关注AI领域的研发进展和投资。<br>理念是：Investing in innovation starts with understanding it. 投资创新始于对其理解。</p><blockquote><p>我叫Alex。我在麻省理工学院担任火箭科学家已有8年之久。我拥有电气工程学士学位，系统工程与优化硕士学位，以及图像处理与计算机视觉方面的博士学位的一半（😭）。现在，我全职从事人工智能和先进技术的研究，投资其中，并在这个频道上进行全方位报道。</p></blockquote><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h3><p>在2023年，与芯片和人工智能相关的事物取得了令人难以置信的进展。无论是在软件方面，如ChatGPT和微软的合作伙伴，还是在硬件方面，如英伟达的GPU和其他竞争对手，以及股票市场方面，科技股从七大巨头到人工智能软件和硬件领域的小型成长股都经历了疯狂的涨势。在这一期节目中，将回顾整个年度发生的事情，谈谈我们最喜欢的时刻以及它们对我们的资金和投资的意义。</p><h3 id="核心观点"><a href="#核心观点" class="headerlink" title="核心观点"></a><strong>核心观点</strong></h3><ul><li>💥 2023年是芯片和人工智能领域的非凡一年，涉及软件和硬件方面的重大突破和进展。Open AI以惊人的速度推出了ChatGPT和DALL·E等产品，推动了整个市场的发展。</li><li>🆚 谷歌和微软之间的竞争加剧，微软CEO实现了大象跳舞，AI领域暂时领先谷歌。谷歌意识到必须加快创新步伐，迎头赶上。</li><li>📈 英伟达在GPU数据中心领域表现出色，股票市值飙升，23年已经涨了2倍。但面临供应短缺问题。AMD推出了自己的芯片MI30系列，能够在庞大市场占有一席之地。</li><li>🎯 Ticker Symbol You认可ARK Invest的行业研究能力，但是不认可他们的投资能力，因为ARK他们不太认可巨头在AI上获益，比如英伟达、微软等。</li><li>🌟 Ticker Symbol You频道专注于人工智能软件和硬件领域，大幅持仓英伟达和微软，取得了显著的成功。</li><li>🚀 AI时代变化很快投资者需要持续学习和进化，2024年Ticker Symbol You会更多覆盖在利率下行背景下，寻找更多小型股票、增长机会更大的股票。领域主要集中在,人工智能软件、服务型软件公司、半导体公司、增强现实和虚拟现实公司。</li></ul><p><img src="https://s.draftai.cn/vent/202312261914955.png"></p><hr><h3 id="全文"><a href="#全文" class="headerlink" title="全文"></a>全文</h3><p>(00:00) 2023年对于所有与芯片和AI相关的事物来说都是不可思议的一年。每周似乎都有关于AI的重大新公告,无论是软件方面的ChatGPT、Microsoft的copilot,还是硬件方面的nvidia GPU加速器及其所有竞争对手,或者是从全息7到更小的成长型股票在AI软件和硬件领域的狂飙。所以我们认为我们应该制作一期节目,坐下来消化一下今年所发生的一切,讨论一下我们最喜欢的时刻,以及它们对我们的投资意味着什么。你的时间很宝贵,所以让我们直接进入主题。</p><p>(00:34) 第一件事,我们必须从你的圣诞毛衣开始。我不知道大家能不能看到,这实际上是NASA的丑圣诞毛衣,这里是火箭。所以你知道,我明年的计划是买同一件毛衣,但是SpaceX的。对于大西洋那边的所有人来说,谢谢你,这是一件很棒的圣诞毛衣。是的,你做得很好。</p><p>(01:05) 难以相信,就在12个月前,它被宣布对吧?那是2022年11月,Chat GPT第一次进入市场。我的意思是,今年节奏如此之快,我们看到新模型的发布,GPT 4、GPT 4 Turbo,它们在3月和11月。我们看到被发布的插件,我们看到了一个应用商店的发布,你可以创建自己的定制GPT代理。这个软件以如此快的速度在移动,与此同时,我们看到混乱出现在Sam Altman被开除的问题上,首先他被董事会解雇,接着你知道他加入了微软,然后又是董事会被解雇和替换,他又回来了。发生了很多事。感觉我们需要时间来消化所有这一切。</p><p>(01:42) 那么当尘埃落定,2023年末,OpenAI作为一家公司的状况你觉得如何?你是如何看待的?我认为OpenAI做了一些对整个市场非常重要的事情。我不认为我们曾经看到过一个组织能像OpenAI的ChatGPT和DALL-E那样快速推出产品更新升级。我认为接下来的一年和未来会发生的是,我们会看到其他软件公司和其他AI公司放低一些界限,更快地推出产品。</p><p>(02:20) 为了强调你关于发布速度的观点,感觉在他们年初的首次公告之后,就有其他公司试图建立在Chat GPT的基础之上,利用它,但是到Dev day发布的时候,他们发布的一些内容基本上就像杀死了那些公司。几乎就像如果你不创新和快速迭代的话,你会死,因为他们的创新速度会比你快得多。我认为你是对的,尤其是在开始阶段,建立在Chat GPT基础之上的公司基本上就是利用高级提示编程的包装器。随着Chat GPT在理解提示方面的提高,以及提示编程的民主化甚至商品化,这些公司很快就死了。</p><p>(02:51) 我认为我们现在在大型语言模型方面正处于一个几乎你可以相信的不会在第二天就倒闭的阶段,如果你今天启动你的业务的话。现在我们看到谷歌有Bard,我们有Anthropic的Claude,我们不得不保持更新的节奏,关于这些模型能做什么,我们在UI和UX方面添加的新功能,不仅仅是模型本身,我们需要快速发布,因为我们的竞争对手也在快速发布,最先进的技术也在不断发展。</p><p>(03:29) 我们后面也会谈到,英伟达的芯片也正在加快节奏。过去A100和H100 GPU之间有两年的间隔,现在只有6个月的周期,H200、H300等等。我的观点是速度会更快。这意味着落后6个月与过去的意义大不相同。就像,我的天,OpenAI 移动得如此之快,这表明公司可以移动得如此之快。</p><p>(04:04) 就是这样一个巨大的规模,从仅有770名员工的OpenAI到Microsoft的上万名员工,他们都能够快速做出重要决策,无论是在部署软件还是治理和公司结构方面。这是一个极端的公司大小和需要做出的重大决策的完整范围,无论是在产品还是公司方面,事情都会变得更快。</p><p>(04:43) 是的,在那些混乱的日子里,在Sam被解雇和恢复之间的某个时候,曾经有一段时间人人都要加入微软,我记得有人说我们会作为一个孵化器运行它,作为我们公司中的一家公司,我们在这样做而不扼杀创新方面有着良好的记录。我认为他谈论的就是你所说的,那就是如何将一家像OpenAI这样的公司带入你的组织,而不会因官僚主义而放慢速度,仍然以它需要主导的速度运行。我认为即使只有几分钟的时间考虑将它吸收为一家公司,他也急于保持创新步伐,因为这是他们的关键优势之一。</p><p>(05:14) 是的,它推动了许多其他甚至庞大的公司意识到,我们必须行动迅速。另一个很好的例子是谷歌。谷歌可能是最大的一个例子,一个有创新者困境的公司,他们在搜索方面占据垄断地位,但在这里,他们终于承认,好吧,这种模式在10年内可能行不通,10个蓝色链接,我们需要站在它的颠覆最前沿,通过诸如Gemini和Bard之类的东西,否则别人会这么做。现在他们也在加快速度行动。我认为这在2024年只会变得更加明显,因为现在Sam Altman回到了OpenAI,他们可能会行动得更快,因为董事会不会成为部署新开发的障碍。</p><p>(05:44) 听起来你的意思是所有关于微软继续在下一年保持领导地位的一切都是非常看好的,你对这方面有什么看法?你认为谷歌能赶上来吗?我的意思是,谷歌是AI的代名词,看到他们现在处于二流地位,落后于微软,真的很奇怪。我认为总的来说,由于OpenAI、Sam Altman和萨提亚•纳德拉展示了,不管公司规模有多大,从仅有770名员工的OpenAI到Microsoft数十万名员工,他们都能够快速做出重要决策,无论是部署软件还是治理和公司结构。这就是我们谈到的整个规模范围,以及你需要做出的重大决策,无论是产品还是公司方面,事情都会变得更快。</p><p>(06:25) 我同意。我认为2023年对我来说最大的惊喜之一是微软能够保持并可以说是扩大了他们在这一领域的领先地位。我本以为谷歌赶上只是时间问题,但有趣的是,在他们最近的历史中,这是第二次演示他们搞砸了演示,并导致了大量尴尬的报道,但股票也因此受到了打击。谷歌并不缺少理解AI的聪明人,他们拥有一些世界上最大和最好的AI人才。他们也不缺钏,可以解决像这样的问题。所以我也希望谷歌能做些什么,最终我认为消费者将是赢家。因为这场军备竞赛,如果你想把它称为AI,将意味着我们将看到更多创新产品进入市场,更快地进入你早先的观点。我认为我们会在明年这个时候经历的AI将是一个相对于我们目前拥有的巨大进步。我认为你知道这对每个人来说都很令人兴奋。</p><p>(07:01) 那么,根据你刚才所说的,你认为投资者今后应该如何考虑这一点呢?我认为最大的收获,或者我这里的重点是速度和波动性。速度的意思是说新闻会传播得更快,会有更多的产品发布,更多的服务发布,更多Chat GPT和Bard等的功能和能力,会有更多的用例,更多的垂直领域等等。结果就是我们会看到速度带来像Bard那样的失误,然后是与Gemini Chat相关的失误。GPT本身也有不少在最初推出时的批评。</p><p>(07:38) 所以我认为这将提高这些股票的波动性。我们会看到股票遭受损失,然后会有人出现解决问题,然后股票会反弹,然后下一个事物就会出问题,然后它会遭受损失,这将是一个波动性的循环,我认为人们并不真正为此做好准备。这不是宏观经济波动性,这实际上是技术进步的波动性。现在理解和评估技术风险,理解支撑股票的科学,这是我们这里的专长,它将变得越来越有意义,因为我认为股价的越来越多将由技术的真正进步驱动。</p><p>(08:14) 让我根据你刚才说的进行后续,你仍然认为OpenAI与其他任何你选择的大型语言模型相比具有巨大优势吗?无论是人类学的Claude还是谷歌的Gemini。你认为他们的领先优势缩小了多少?或者你认为它根本没有收缩,现在只是有了其他竞争对手?你怎么看?</p><p>是的,这是一个棘手的问题。我会分享我的历程,我认为这可能很典型。我从Chat GPT开始,因为那是第一个出现的东西,每个人都在使用它,然后我了解到,如果你使用Microsoft搜索,如果你使用必应,它就能进行一些生成式AI,同时结合实时搜索结果。所以然后我开始使用类似必应聊天之类的东西。然后我了解到人类学的Claude可以接受更大的提示比任何人都多,我就像,好吧,我可以给它提供整个YouTube文字稿,让它来总结,这是我在Chat GPT上做不到的。 所以我就移动着,一直在这些工具之间移动,看哪个工具为我提供最佳功能。但是Chat GPT推出了订阅版本,可以做所有那些事情以及更多其他功能。 所以就像是一个移动的目标,总是有新的出现。</p><p>(09:30) 所以它确实感觉像Chat GPT,现在它可以接受更大的提示,它已升级到2023年4月的知识,它已经与必应的实时搜索集成在一起,所以对我来说,就我们录制这个的时候,这仍然是我总是使用的解决方案。但这是一个非常快速变化的空间,在接下来的几周内可能会出现某些东西来打败它!</p><p>(09:55) 对我来说,我认为竞争对手不断提高最小可接受性能的下限,但Chat GPT,尤其是GPT 4、GPT 4 Turbo、GPT 4.5,似乎总是在提高天花板,然后每个人都在快速弥合这一差距。所以我确实认为,也许Gemini现在是最前沿的模型,但总的来说,前沿模型和我们称之为其他竞争对手平均水平之间的距离似乎正在变得越来越小。这似乎意味着在未来的一年或者18个月左右,这些大型语言模型将得到完全普及和商品化,无论你如何想象它。</p><p>(10:28) 基于你刚才说的,让我后续跟进一下,你是否仍然认为Open AI在任何你选择的大型语言模型方面与微软相比具有巨大优势?无论是人类学的Claude还是谷歌的Gemini。我认为领先优势已经缩小了多少?或者它根本没有收缩?</p><p>是的,这是一个棘手的问题。我会分享我的历程,我认为这可能很典型。我从Chat GPT开始,因为那是第一个出现的东西,每个人都在使用它,然后我了解到,如果你使用微软搜索,如果你使用必应,它就能进行一些生成式AI,同时结合实时搜索结果。所以然后我开始使用类似必应聊天之类的东西。然后我了解到人类学的Claude可以接受更大的提示比任何人都多,我就像,好吧,我可以给它提供整个YouTube文字稿,让它来总结,这是我在Chat GPT上做不到的。所以我就在这些工具之间移动,看哪个为我提供最好的功能。 但是Chat GPT推出了订阅版本,可以做所有那些事情以及更多其他功能。 对我来说,就我们录制这个的时候,这仍然是我总是使用的解决方案。</p><p>(11:00) 但是它是一个非常快速变化的空间,在接下来的几周内可能会出现某些东西来打败它!对我来说,我认为竞争对手不断提高最小可接受性能的下限,但Chat GPT,尤其是GPT 4、GPT 4 Turbo和GPT 4.5似乎总是在提高最高限度,然后每个人都在快速弥合这一差距。 因此,我确实认为也许Gemini现在是最前沿的模型,但总的来说,前沿模型和我们称之为其他竞争对手的平均水平之间的距离似乎正在变得越来越小。 这似乎意味着在未来的一年左右,这些大型语言模型将得到完全普及和商品化,无论你如何想象它。</p><p>(11:44) 我同意。我认为竞争对手不断提高最小可接受的性能下限,但ChatGPT,尤其是GPT-4、GPT-4 Turbo和GPT-4.5似乎总是在提高最高限度,然后每个人都在快速弥合这一差距。因此,我确实认为也许Gemini现在是最前沿的模型,但总的来说,前沿模型和我们称之为其他竞争对手平均水平之间的距离似乎正在变得越来越小。这似乎意味着在未来的一年左右,这些大型语言模型将被完全普及和商品化,无论你如何想象它。</p><p>(12:13) 我认为这是一种伟大的思考方式。这就是大多数人真正关心的,并不是你最近推出了多少十亿参数,而是我能用它做什么,它是否给我带来了竞争优势,如果与其他东西相比我使用它。</p><p>(12:29) 完全同意,完全同意。非常好,那么我们已经讨论了软件方面,现在让我们谈谈硬件,这同样爆炸性增长,至少从股市波动来看更大。我的意思是,在录制本播客时,英伟达的股票刚刚突破510美元大关,是不是很疯狂。英伟达和AI真的是今年的主题。他们在数据中心领域取得了创纪录的收入和硬件公司难以置信的利润率。他们的H100芯片似乎唯一的限制是实际上更快地拿出H100,这似乎更多的是台积电产能不足的问题,而不是其他任何事情。在2023年,他们基本上是毫无竞争的。</p><p>但是AMD在几天前刚刚发布了他们的MI300X,英特尔也举行了人工智能无处不在的活动。那么你如何看待这个市场呢?这是一个赢家通吃的市场吗?在GPU数据中心领域?你有什么看法?</p><p>(13:26) 我认为这不是一个赢家通吃的市场,这里的大不同iator是数据中心实际上是技术组合。当它涉及到为许多这些新工作负载提供服务时,一个解决方案在绝对上优于其他所有解决方案,仅仅是因为其他解决方案实际上并不存在。就像A100基本上是无可争议的GPU,H100是一个几乎无可争议的GPU,在GPU不重要的时刻,直到它们变得重要。然后,所有的AI工作负载,它们只有一个芯片可供选择。</p><p>(13:58) 所以,我刚才说的事情实际上在几年前就开始了,当OpenAI与微软合作建立类似AI Azure之类的东西,并开始为GPT和GPT-2等提供动力时,它是幕后的,没有人真正在谈论它。大型语言模型确实需要GPU才能起飞,谁是GPU制造商?是英伟达。现在这个市场已经存在,我们看到了英特尔的Gaudi 2和Gaudi 3芯片等公司的加入,我们看到了AMD的MI300X和MI300A。现在的问题是,这个由AI驱动的新市场是否是一个赢家通吃的市场?因为过去只有一家公司能做到这一点,现在多家公司都能做到,会发生什么?</p><p>(14:32) 是的,我仍然认为答案是否定的,因为这将是谁做得最好,我们如何只让最符合成本效益的工作负载运行在英伟达的GPU上,然后从那里反向计算出这些工作负载将占据数据中心所有工作负载的百分比。这就是你如何决定谁将赢得这个市场。</p><p>(14:56) 我确实认为每个人都还在沉睡,不了解人工智能和生成式人工智能工作负载在未来18个月内会有多大。这是我喜欢你如何简洁地阐述它的方式——这原本不是一个市场,只有一家玩家,我们没有所谓的杀手级应用。</p><p>(15:17) 是的,我还记得当GPU市场崩溃时,你知道,人们会说,英伟达之所以成功,是因为加密挖矿和GPU市场,但其收入正在下降——它们的PC GPU收入确实在下降——游戏一直是他们的大事。他们在AI方面做了这档子事,我们知道他们是领导者,但并没有真正的大规模推广,然后我们迎来了ChatGPT时刻,这真的颠覆了整个形势。</p><p>(15:43) 你认为AMD来晚了吗?有人觉得当MI 300X推出时,他们已经错过了船。我不这么认为。我有两个原因认为落后并不意味着太晚。首先,它可以很好地完成H100能做的一些事情,这意味着你可以将部分本应直接放在H100上的工作负载卸载到MI30上,如果你不介意像运行时间更长这样的代价的话,你会得到类似的价格性能比。如果基于工作负载你不在乎,那么使用更便宜的芯片是可以的。这不像自动驾驶汽车,一个用户倾向于购买一辆车,所以赢家变成了一个通吃的市场,因为购买最便宜、最好的汽车,在这种情况下是特斯拉,没有理由不这样做。</p><p>(16:24) 在这种情况下,如果有更好的芯片,他们不会删除服务器机架,他们会说,好的,我的下一次扩张我将购买更多这些,我将改变我的投资组合比例。我将朝着一个新的目标前进,而不是卖掉旧车买新车。我仍然拥有一辆汽车。</p><p>(16:42) 关于英伟达,你应该知道的一件事是,如果你现在想要一个H100,显然需要12个月的交货时间。因此,如果AMD能在更短的时间内为你提供一个不太好但几乎一样好的东西,你可能会选择它,如果你想购买一些并运行自己的业务。这就是为什么我也觉得AMD在2024年将从中分一杯羹,他们不需要是最快的,也不需要是最节能或最便宜的,他们在这个市场上有一席之地,他们将从这个市场中占据一定的份额。</p><p>(17:13) 如果AMD只是说,你是对的,英伟达的芯片比我们的好50%,而我们的芯片比他们的便宜50%.这显然没有那么简单,因为这里有功耗方面的考虑,还有数据中心的物理占用空间,这里还有定价战略元素,而不仅仅是计算性能元素。</p><p>(17:33) 我想指出的另一件很有意思的事情是,在今年我们看到了多少紧急演示,就像苹果都被迫在高通演示之后进行紧急演示,然后在英伟达演示之后,我们看到AMD和英特尔的紧急演示。 我认为这简直疯了,这在这个行业里以前不是这样的。就像这些事件过去需要花费数月的时间来计划、准备,人们会排练他们的演讲,他们会掌握每一个词。今年的速度实在是太快了。</p><p>(18:06)这又回到了之前关于一切速度的论点。正如你所说的,这对2024年似乎是一个很好的配方。</p><p>当我们结束这一年时,我认为我们要涵盖的最后一个领域是频道本身。回顾一下今年的频道,我们进行了一些改变,你令人难以置信地与ARK Invest分手,我们将重点缩小到AI软件公司、半导体公司,这是频道的一个关键重点。我们还推出了不同的格式,比如我们现在的播客,更多的是关于特定事件的精华剪辑。关于改变方向,你真的希望tiemba你的观众和社区带走什么,你能谈谈这种变化吗?</p><p>(18:44) 与ARK Invest分手不是预先计划的事情。实际上发生的是,我和你决定坚持我们的核心竞争力,而不是Mirror和评论另一个投资组合经理的投资组合。在我看来,ARK Invest确实拥有市场上最好的研究,但在我看来,他们在将该研究转化为投资决策方面并不出色。我们非常具体地指出了这些投资决策,例如在股价翻两番之前抛售英伟达,或没能认识到来自AI的创新。</p><p>(19:15) 似乎有很多这种偏见,这似乎是因为他们只是想与更大的指数保持不相关,但这实际上开始影响他们对整体创新的描述,以至于与我和你以及我们的背景不符。这才真正促使我们远离ARK。同时,我确实想坚持我们的核心竞争力。我认为人工智能软件和硬件方面,我和你一起做得非常出色。</p><p>(19:40) 我确实认为,当ARK投资与我们所在的空间重叠时,特别是Ark K、Ark W和Ark Q这三只基金,我确实想多涵盖他们的研究。我一直说,我认为他们的研究整体来说是市场上最好的。我个人不一定同意他们如何将研究转化为投资决策。我认为我们今年与ARK Invest分手后,我们的表现已经验证了这一点。我们最大的股票选择是英伟达,它的价格涨幅超过两倍。我们的第二大股票选择是微软,Ark Invest根本不持有任何微软股票,他们还抛售了很多英伟达股票。</p><p>(20:13) 所以我们的决策与他们有很大不同。我认为你知道,对于那些希望我们涵盖更多隐藏小型股票、增长机会更大的股票的人,我真的理解和欣赏这种想法。随着2024年和2025年利率计划下降,你可以期待我们做更多这方面的工作,但肯定是在我们已经涵盖的领域,人工智能软件、服务型软件公司、半导体公司、增强现实和虚拟现实公司。</p><p>(20:40) 随着该频道接近30万订阅者,确保人们获得好的信息比兴奋的信息更重要。我认为我们成为频道和内容创作者成长的一部分是我们变得更加成熟,我们更关心为观众增加价值,而不是感到自己很有价值。我认为今年真正的情况是,对观众最有益的是什么,我们如何最好地为观众服务,什么想法对观众真的有意义,这似乎是非常关注人工智能的技术,它们以破纪录的速度改变着我们的生活。</p><p>(21:13) 我认为该频道在真正超级关注这一件事情上做得非常出色,将我们的研究转化为我们相信的投资决策。结果,如果您查看2021年和2022年部分时间内的股票及其表现类型以及我们调整后的表现,那是截然不同的。所以你知道,对于那些寻找隐藏小型股票的人,在人工智能软件和硬件领域,你会发现他们,我们会更多地报道他们,但我们的目标始终是我认为从去年开始考虑观众第一,而不仅仅是为了酷和激动人心。</p><p>(21:44) 我认为曾几何时,当利率非常低时,人人都在寻找增长型股票,因为人人都在寻找下一个大事件,他们都在努力发掘这些隐藏的小宝石,这不仅仅是他们,风险投资公司也在这些公司中注入了数百万美元,试图从他们的钱中获得回报。那是不同的时代,新冠病毒只会加重这种情况。</p><p>(22:10) 但真正有趣的是,你刚才谈到的关于ARK投资和创新的内容。有一個主题,他们非常看好的就是人工智能无所不在,所以他们应该从过去的12个月中受益匪浅。为什么他们没有?一是因为他们不相信巨头能从人工智能中获得利益,我想这一点他们相当教条。就像你所说的,你不会从传统股票和大公司中获得曝光,你会获得不同的东西,你会获得创新,创业公司。</p><p>(22:38) 而且我认为所有的好投资者都有一个共同点,那就是随着新数据的出现,他们愿意改变主意。无论是新利率,还是崭新的技术出现,如果你跟10年前的投资者一样,你可能不是一个好的投资者,你可能需要做一些自我反思,如果你认为在2020年和2021年表现不错的东西,会在2024年和2025年,甚至2030年和2031年同样表现不错。</p><p>(23:05) 正如这整个播客剧集充分点出的那样,世界正在以破纪录的速度移动,投资者也需要学习如何快速移动。所以我非常期待我们的下一集,我们会谈论2024年的预测。我敢肯定,当这一集在1月底出来时,到了1月底,所有内容都会完全错误。但其中的想法是,事情是否正在以太快的速度前进,以至于我们甚至无法预测。所以我也非常期待那一集。在此之前,这里是Tiker符号,我是Alex,我和Arif一起提醒您,您可以做出的最佳投资是投资于您自己。</p>]]></content>
    
    
    <categories>
      
      <category>Investment</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Google</tag>
      
      <tag>ARK</tag>
      
      <tag>Nvdia</tag>
      
      <tag>AMD</tag>
      
      <tag>MSFT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Epic Games起诉Google -ARK Brainstorm -枫叶洞见EP01</title>
    <link href="/2023/12/25/Epic%20Games%20vs%20Google%20-ARK%20Brainstorm-Just%20In%20Vent%20EP01/"/>
    <url>/2023/12/25/Epic%20Games%20vs%20Google%20-ARK%20Brainstorm-Just%20In%20Vent%20EP01/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s.alidraft.com/vent/vent1924_A_depiction_of_various_app_stores_and_digital_platform_70f4ed0e-8a4f-4381-91d8-4e8c66bcf46c.png" alt="various_app_stores"></p><h3 id="开篇的话"><a href="#开篇的话" class="headerlink" title="开篇的话"></a>开篇的话</h3><ul><li>后续机器史学开启一个新的栏目频道：枫叶洞见。</li><li>主要关注科技投资相关的高质量输出。初期主要是包括对顶级企业家、投资人的访谈节目的翻译、总结、理解、分析。</li><li>希望能够帮助大家一起学习，解读科技和商业发展和进步。</li><li>欢迎大家留言反馈，给出宝贵的建议和意见。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Epic Games起诉Google的案件背景和最新进展，涉及反垄断诉讼和App Store的高额佣金。</p><p><img src="https://s.alidraft.com/vent/vent1924_A_courtroom_with_a_judge_jury_and_attorneys_representi_3fc664c6-6f73-442e-9458-c7e91f4b620a.png" alt="courtroom"></p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><ul><li>[⚖️] Epic Games在2021年输掉了反垄断诉讼与苹果的案件，但最近赢得了针对Google的类似案件。</li><li>[💰] Epic Games主张Google和苹果对应用内购买和订阅收取高达30%的佣金，导致消费者和开发者受到损害。</li><li>[🔒] Google和苹果辩称高额佣金是为了确保App Store的安全和规范，但Epic Games认为这是垄断行为。</li><li>[🤖] AI技术可能改变应用商店的未来，但也可能增加对应用商店的监管需求。</li></ul><hr><h2 id="时间线指导"><a href="#时间线指导" class="headerlink" title="时间线指导"></a>时间线指导</h2><h3 id="00-00-音乐和开场"><a href="#00-00-音乐和开场" class="headerlink" title="00:00 - 音乐和开场"></a>00:00 - 音乐和开场</h3><ul><li>🎵 <strong>音乐和开场</strong>：介绍了Brainstorm第28集，由Andrew Kim主持。</li></ul><h3 id="00-00-案件背景"><a href="#00-00-案件背景" class="headerlink" title="00:00 - 案件背景"></a>00:00 - 案件背景</h3><ul><li>🌍 <strong>案件背景</strong>：Epic Games在2021年反垄断案件中输给了Apple，但在最近的类似案件中赢了Google。</li></ul><h3 id="00-57-陪审团判决"><a href="#00-57-陪审团判决" class="headerlink" title="00:57 - 陪审团判决"></a>00:57 - 陪审团判决</h3><ul><li>⚖️ <strong>陪审团判决</strong>：九人陪审团完全支持Epic，认定Google在其Google Play应用商店中实施反竞争行为。</li></ul><h3 id="01-48-关于应用商店的讨论"><a href="#01-48-关于应用商店的讨论" class="headerlink" title="01:48 - 关于应用商店的讨论"></a>01:48 - 关于应用商店的讨论</h3><ul><li>📱 <strong>关于应用商店的讨论</strong>：Tim Sweeney表达了对Apple和Google应用商店高佣金的不满，这些佣金影响了消费者和开发者。</li></ul><h3 id="03-23-安全和监管辩护"><a href="#03-23-安全和监管辩护" class="headerlink" title="03:23 - 安全和监管辩护"></a>03:23 - 安全和监管辩护</h3><ul><li>🛡️ <strong>安全和监管辩护</strong>：Apple和Google主张他们的应用商店提供更安全、更受监管的环境，但存在反驳意见，指出其他平台也能提供良好监管。</li></ul><h3 id="04-55-公众感知与司法决定"><a href="#04-55-公众感知与司法决定" class="headerlink" title="04:55 - 公众感知与司法决定"></a>04:55 - 公众感知与司法决定</h3><ul><li>👥 <strong>公众感知与司法决定</strong>：Epic在反Google案中的胜利可能反映了公众对应用商店权力和影响的看法。</li></ul><h3 id="06-28-国际趋势"><a href="#06-28-国际趋势" class="headerlink" title="06:28 - 国际趋势"></a>06:28 - 国际趋势</h3><ul><li>🌐 <strong>国际趋势</strong>：欧盟越来越倾向于接受第三方应用商店和支付服务，显示了全球对开放生态系统的趋势。</li></ul><h3 id="07-59-移动计算的未来"><a href="#07-59-移动计算的未来" class="headerlink" title="07:59 - 移动计算的未来"></a>07:59 - 移动计算的未来</h3><ul><li>🎮 <strong>移动计算的未来</strong>：讨论了Roblox等平台可能成为新型应用商店的潜力，以及移动应用商店未来可能的沉浸式和三维化发展。</li></ul><h3 id="09-16-AI和内容筛选的未来"><a href="#09-16-AI和内容筛选的未来" class="headerlink" title="09:16 - AI和内容筛选的未来"></a>09:16 - AI和内容筛选的未来</h3><ul><li>🔮 <strong>AI和内容筛选的未来</strong>：AI的发展可能会改变Apple和Google在移动生态系统中的角色，同时内容和应用创作的增长强化了内容筛选的重要性。</li></ul><h3 id="10-42-封闭与开源趋势"><a href="#10-42-封闭与开源趋势" class="headerlink" title="10:42 - 封闭与开源趋势"></a>10:42 - 封闭与开源趋势</h3><ul><li>📈 <strong>封闭与开源趋势</strong>：探讨了封闭生态系统与开源趋势，及AI在内容创作中的作用。</li></ul><h3 id="11-20-AI的影响"><a href="#11-20-AI的影响" class="headerlink" title="11:20 - AI的影响"></a>11:20 - AI的影响</h3><ul><li>🤖 <strong>AI的影响</strong>：讨论了AI如何改变用户对特定生态系统的感知和安全性的看法，以及苹果和谷歌在此中的角色。</li></ul><h3 id="12-04-算法推荐的作用"><a href="#12-04-算法推荐的作用" class="headerlink" title="12:04 - 算法推荐的作用"></a>12:04 - 算法推荐的作用</h3><ul><li>🔍 <strong>算法推荐的作用</strong>：AI和算法推荐系统可能会影响小型、恶意应用的可见性，使得它们更难获得关注。</li></ul><h3 id="13-23-应用商店的多样性"><a href="#13-23-应用商店的多样性" class="headerlink" title="13:23 - 应用商店的多样性"></a>13:23 - 应用商店的多样性</h3><ul><li>🔀 <strong>应用商店的多样性</strong>：指出除了苹果和谷歌的应用商店外，还有其他中介如Steam和Epic提供类似服务。</li></ul><h3 id="14-08-用户关注点与选择"><a href="#14-08-用户关注点与选择" class="headerlink" title="14:08 - 用户关注点与选择"></a>14:08 - 用户关注点与选择</h3><ul><li>👁️ <strong>用户关注点与选择</strong>：随着内容和应用数量的增加，用户可能会更倾向于使用少数主要平台，而非频繁切换应用商店。<br><img src="https://s.alidraft.com/vent/vent1924_An_illustration_of_diverse_people_using_different_mobi_569796cc-d22c-4e9c-b756-653cc702057d.png" alt="diverse_people_using_different_mobi"></li></ul><h3 id="14-51-应用商店聚合器"><a href="#14-51-应用商店聚合器" class="headerlink" title="14:51 - 应用商店聚合器"></a>14:51 - 应用商店聚合器</h3><ul><li>🔄 <strong>应用商店聚合器</strong>：讨论了应用商店聚合器的概念，类似于视频流领域的Roku，为用户提供更广泛的应用选择。</li></ul><h3 id="15-34-聚合理论"><a href="#15-34-聚合理论" class="headerlink" title="15:34 - 聚合理论"></a>15:34 - 聚合理论</h3><ul><li>🌍 <strong>聚合理论</strong>：探讨了数字经济中聚合者的影响力，可能预示着应用商店领域的未来聚合趋势。</li></ul><h3 id="16-15-AI的持续影响"><a href="#16-15-AI的持续影响" class="headerlink" title="16:15 - AI的持续影响"></a>16:15 - AI的持续影响</h3><ul><li>🤔 <strong>AI的持续影响</strong>：讨论了AI如何成为一个重要且持续的话题，特别是在移动计算和应用商店的未来发展中。</li></ul><h3 id="结尾和节目计划"><a href="#结尾和节目计划" class="headerlink" title="结尾和节目计划"></a>结尾和节目计划</h3><ul><li>🎉 <strong>结尾和节目计划</strong>：最后讨论了节目的结尾和下周的节目计划。</li></ul><hr><h2 id="完整译文"><a href="#完整译文" class="headerlink" title="完整译文"></a>完整译文</h2><p>(00:00) [音乐] 大家好，欢迎来到Brainstorm的第28集，我今天来做介绍。我们遇到了几次技术问题，这个环节已经是我们第六或第七次重新录制了，但我们仍在追求完美。今天我们有Andrew Kim和我们在一起，他将为我们分析Epic对Google的最新消息。Andrew，请开始。</p><p>Andrew：当然。首先，我想稍微介绍一下背景。2021年，Epic Games在一起反垄断诉讼中输给了Apple，这个决定主要由第九巡回上诉法院维持。然而，Epic最近在一起类似的案件中赢了Google。这周，九人陪审团完全支持Epic，认为Google在其Google Play应用商店中从事反竞争行为，以此来施加垄断权力。我们还不知道实际的惩罚会是什么，因为Epic并没有提出经济损失赔偿要求，双方下个月都有上诉的机会。我们也在等待最高法院对Apple和Epic在2021年和2023年早期审判中的上诉做出决定。</p><p>(01:48) 你知道，Tim Sweeney在过去几年里一直在表达对苹果和谷歌应用商店高佣金的巨大挫败感。谷歌和苹果都会对应用内购买和订阅等收取高达30%的佣金。消费者因为支付过高的数字商品费用而受到伤害，这是Epic的观点。开发者也受到伤害，因为这影响了消费者，同时他们也只能拿回一小部分收入，还要面临如何分发应用、选择第三方支付方式等方面的僵化问题。Epic并不是为了经济赔偿而提起诉讼，他们只是希望通过规则来稍微平衡一下竞争环境，使移动应用生态系统更公平。</p><p>(03:23) Apple和Google在自己的辩护中主要围绕安全问题展开。例如，Apple提出，如果允许第三方应用商店存在，可能会导致较少监管的应用分发给消费者，可能含有恶意代码等。因此，他们主要围绕安全问题和缺乏监管进行辩护。但反驳的观点是，像Epic和Steam这样的成功应用商店证明了除了苹果和谷歌的移动应用商店生态系统之外，还可以有良好的监管。</p><p>(04:55) 关于Epic在反苹果案中败诉而在反谷歌案中胜诉的情况，这可能表明公众对应用商店在移动应用生态系统中的地位以及苹果和谷歌所持有的权力有怎样的看法。还讨论了与此同时进行的另一场战斗，即Apple和Google试图阻止在其生态系统中出现其他应用商店。</p><p>(06:28) 欧盟已经显示出对第三方应用商店和支付服务的接受趋势。例如，因Spotify对Apple的高佣金投诉</p><p>，欧盟在2024年开始实施一些措施，要求大型科技公司遵守新规定，允许第三方支付服务和应用商店的存在。</p><p>(07:59) 讨论了Roblox和其他平台是否可以构成应用商店的问题，以及用户如何在这些平台上从一个体验跳转到另一个体验。这可能预示着移动计算的未来阶段，其中应用商店将变得更加沉浸式和三维化。</p><p>(09:16) 最后，讨论了Apple和Google在移动生态系统中的角色以及AI如何可能改变这一格局。同时，指出内容和应用程序创作的爆炸性增长将使内容筛选变得更加重要。</p><p>(10:42) 讨论了封闭生态系统与开源的趋势，以及AI在内容和应用创作中的潜在影响。苹果和谷歌辩称，他们在筛选和保护内容方面具有经验，这在内容创作迅猛增长的情况下尤为重要。AI的发展可能会使得内容筛选变得更加困难，对于较小的平台来说，这尤其具有挑战性。</p><p>(11:20) 讨论了AI如何改变对特定生态系统的建设感受和安全性的看法。如果AI驱动的搜索变得更为普遍，可能会导致用户更加依赖某些中央权威或监管机构。这可能会加强苹果和谷歌在移动生态系统中的角色，尽管也存在其他可信赖的应用商店。</p><p>(12:04) 讨论了AI和算法推荐系统如何影响应用商店的运营和用户体验。随着算法推荐变得更加精准，小型、恶意或不安全的应用可能越来越难以获得足够的关注。</p><p>(13:23) 讨论了应用商店体验的多样性，指出除了苹果和谷歌的商店外，还有像Steam和Epic这样的小型中介能够提供类似的服务。这表明在应用商店领域，存在多种可能性和不同的运营模式。</p><p>(14:08) 最后，讨论了用户关注点的分散以及人们如何选择和使用不同的应用商店。随着内容和应用的增加，用户可能会倾向于使用少数几个主要的平台，而不是频繁切换到不同的商店。</p><p>(14:51) 讨论了“应用商店聚合器”的概念。这种聚合器可能会整合多个不同的应用商店，为用户提供更广泛的选择。比如在视频流领域，Roku就充当了类似的角色，聚合了Netflix、Prime等多个视频服务。</p><p>(15:34) 进一步探讨了聚合理论，即在数字经济中，聚合者拥有巨大的影响力。这可能意味着在应用商店领域，存在着一个或多个大型聚合者。</p><p>(16:15) 最后，讨论了AI如何在每周的讨论中成为一个持续出现的话题，以及它可能如何影响移动计算和应用商店的未来。同时，提到了假期安排和下周的节目计划，包括对即将到来的节日的祝福，以及对下一期节目的期待和规划。</p>]]></content>
    
    
    <categories>
      
      <category>Investment</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Epic Games</tag>
      
      <tag>Google</tag>
      
      <tag>ARK</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo 版本升级</title>
    <link href="/2023/12/25/hexo-update/"/>
    <url>/2023/12/25/hexo-update/</url>
    
    <content type="html"><![CDATA[<h3 id=""><a href="#" class="headerlink" title=""></a></h3><ul><li>以下指令均在Hexo目录下操作，先定位到Hexo目录  </li><li>查看当前版本，判断是否需要升级  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">hexo version<br></code></pre></td></tr></table></figure></li></ul><h3 id="全局升级hexo-cli"><a href="#全局升级hexo-cli" class="headerlink" title="全局升级hexo-cli"></a>全局升级hexo-cli</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm i hexo-cli -g <br></code></pre></td></tr></table></figure><h3 id="再次查看版本，看hexo-cli是否升级成功"><a href="#再次查看版本，看hexo-cli是否升级成功" class="headerlink" title="再次查看版本，看hexo-cli是否升级成功"></a>再次查看版本，看hexo-cli是否升级成功</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">hexo version<br></code></pre></td></tr></table></figure><h3 id="安装npm-check，若已安装可以跳过"><a href="#安装npm-check，若已安装可以跳过" class="headerlink" title="安装npm-check，若已安装可以跳过"></a>安装npm-check，若已安装可以跳过</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install -g npm-check <br></code></pre></td></tr></table></figure><h3 id="检查系统插件是否需要升级"><a href="#检查系统插件是否需要升级" class="headerlink" title="检查系统插件是否需要升级"></a>检查系统插件是否需要升级</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm-check  <br></code></pre></td></tr></table></figure><h3 id="安装npm-upgrade，若已安装可以跳过"><a href="#安装npm-upgrade，若已安装可以跳过" class="headerlink" title="安装npm-upgrade，若已安装可以跳过"></a>安装npm-upgrade，若已安装可以跳过</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm install -g npm-upgrade  <br></code></pre></td></tr></table></figure><h3 id="更新package-json"><a href="#更新package-json" class="headerlink" title="更新package.json"></a>更新package.json</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm-upgrade  <br></code></pre></td></tr></table></figure><h3 id="更新全局插件"><a href="#更新全局插件" class="headerlink" title="更新全局插件"></a>更新全局插件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm update -g  <br></code></pre></td></tr></table></figure><h3 id="更新系统插件"><a href="#更新系统插件" class="headerlink" title="更新系统插件"></a>更新系统插件</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">npm update --save  <br></code></pre></td></tr></table></figure><h3 id="再次查看版本，判断是否升级成功"><a href="#再次查看版本，判断是否升级成功" class="headerlink" title="再次查看版本，判断是否升级成功"></a>再次查看版本，判断是否升级成功</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">hexo version<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>从零开始打造更强的私有GPT大模型- RAG教程01</title>
    <link href="/2023/12/25/rag-101/"/>
    <url>/2023/12/25/rag-101/</url>
    
    <content type="html"><![CDATA[<p><img src="https://s.alidraft.com/vent/vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png" alt="vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png"></p><h2 id="RAG理论"><a href="#RAG理论" class="headerlink" title="RAG理论"></a>RAG理论</h2><h3 id="1-什么是RAG"><a href="#1-什么是RAG" class="headerlink" title="1. 什么是RAG"></a>1. 什么是RAG</h3><p>众所周知，大模型基于海量的数据来训练，它具备非常强大的智能，能够回答各种问题。但是我们在使用过程中发现，通用大模型在某些专业领域能力还不够强，很多<strong>领域相关问题很难回答得上来</strong>。通常，预训练（pre-train）的大模型只懂得它训练时用的数据，对于训练集之外的新信息（比如网络搜索新数据或特定行业的知识）就不太清楚。</p><p>那么怎么构建一个私有的GPT大模型呢？方法有很多种，包括 1. 重新训练<strong>私有领域数据</strong>的大模型，2. 基于已有大模型做专有数据的<strong>微调</strong>(FineTuning) 3. 通过RAG技术，优化大模型基础能力。4. 通过Prompt 工程把私有数据在对话中给到大模型。</p><p><strong>RAG</strong>: Retrieval Augmented Generation，检索增强生成技术。RAG由FaceBook AI实验室 于2020年提出，他们的论文<a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>， 提供了一种通过给大模型提供向量数据来增强模型能力的方法。</p><p>快速来看一下文章摘要：</p><blockquote><ol><li><strong>背景与挑战</strong>：这些大型预训练语言模型，因其存储了大量事实知识和在 NLP 任务中的出色表现而闻名。但它们在精确获取和处理知识方面存在局限，尤其是在知识要求高的任务中。这导致它们在特定任务的架构上表现不佳。此外，如何提供决策依据和更新模型中的知识仍是一个挑战。</li></ol></blockquote><blockquote><ol start="2"><li><strong>检索增强生成（RAG）方法</strong>：文章提出了一种 RAG 模型的微调方案，这些模型巧妙地结合了预训练的参数记忆（例如 seq2seq 模型）和非参数记忆（例如维基百科的密集向量索引）。通过预训练的神经检索器，这种组合被赋予了新的生命。RAG 模型的目标是利用这两种记忆类型，使语言生成更加生动和具有创造力。</li></ol></blockquote><blockquote><ol start="3"><li><strong>两种RAG形式</strong>：研究精心比较了两种 RAG 形式。一种在整个生成过程中使用相同的检索段落，另一种则可以为每个词汇使用不同的段落。。</li></ol></blockquote><blockquote><ol start="4"><li><strong>性能与评估</strong>：在多种知识密集型 NLP 任务中，RAG 模型经过微调和评估后，创造了三个开放领域问答任务的新纪录，超越了传统的参数型 seq2seq 模型和专门的检索-提取架构。在语言生成方面，与仅使用参数的 seq2seq 模型相比，RAG 模型能生成更加具体、多样和真实的语言。</li></ol></blockquote><p>整体看一下：论文对RAG技术的理解。 主要由 <strong>Retriever</strong>和<strong>Generator</strong>两大部分组成。<br><img src="https://s.zhangguiyi.cn/vent/202312181613600.png" alt="image.png"></p><p>随着时间的推移，尤其是大模型的进步，RAG的架构有些变化，更加组件化和清晰化。<br>如果希望了解更多关于RAG的历史和发展，推荐仔细阅读综述文章： <strong><a href="https://arxiv.org/pdf/2312.10997.pdf">Retrieval-Augmented Generation for Large Language Models: A Survey”</a></strong> </p><p><img src="https://s.alidraft.com/vent/202312211120837.png" alt="image.png"></p><h3 id="2-进一步理解RAG"><a href="#2-进一步理解RAG" class="headerlink" title="2. 进一步理解RAG"></a>2. 进一步理解RAG</h3><p>第一节相对学术化一些，我们来看一个更好理解的图：</p><p><img src="https://docs.llamaindex.ai/en/stable/_images/basic_rag.png"></p><p>从上图可以看到<strong>RAG</strong>的系统核心,由User(Query),(Vector) Index , LLM 三大组件组成。</p><h4 id="2-1-三大组件"><a href="#2-1-三大组件" class="headerlink" title="2.1 三大组件"></a>2.1 三大组件</h4><ol><li>**用户发起的查询(User-&gt;query)**。这种查询一般都是自然语言的，用户不再需要学习类似于之前搜索引擎的DSL或者数据库的SQL。这样大幅降低这类系统的使用门槛。</li><li><strong>模型所需的外部数据(Index -&gt; prompt)<strong>。可以看到，RAG系统的核心工作其实在这个组件。<br> 1. <strong>索引</strong>：将不同类似的用户数据，比如结构化的关系数据库、非结构化的文本、甚至是可编程的API，通过向量嵌入(Vector embedding)方法来将它们变成向量数据。更多关于向量数据库的文章，可以访问我之前的博客。涉及到</strong>向量化(Vector embedding)方法</strong>和<strong>切片(Chunk)方法</strong>。<br> 2. <strong>召回</strong>，向量数据的召回，本质上通过余弦相似度来找到最匹配的多个向量。目的是从大量数据中快速筛选出与查询最相关的文档子集，为后续的更详细检索过程提供一个更专注的候选集。这种方法旨在提高检索过程的效率和效果，减少计算资源的需求，并加速响应时间。<br> 3. <strong>查询</strong>。这里查询方法有很多种，涉及到向量数据库的相关度计算与评估，不同的查询策略。</li><li>**大模型(LLM)**。这里面的大模型可以是开源的Llama2&#x2F;Mistral 等，也可以是闭源的GPT3.5&#x2F;4等。</li></ol><h4 id="2-2-实现RAG的五个步骤"><a href="#2-2-实现RAG的五个步骤" class="headerlink" title="2.2 实现RAG的五个步骤"></a>2.2 实现RAG的五个步骤</h4><p>重复总结一下，实现RAG中有五个关键步骤，如下图所示：<br><img src="https://docs.llamaindex.ai/en/stable/_images/stages.png"></p><ul><li><strong>加载</strong>：指将各种文本文件、PDF、其他网站、数据库还是API等数据，导入我们工作流的步骤。</li><li><strong>索引</strong>：和普通关系数据库无本质差异，在于通过索引加速查询。不同的是，具体的索引算法。</li><li><strong>存储</strong>：存储索引以及其他元数据，以避免重新索引</li><li><strong>查询</strong>：对于任何给定的索引，可以进行多种查询，包括子查询、多步查询和混合策略。</li><li><strong>评估</strong>：提供了关于查询的响应有多准确、快速的客观衡量。</li></ul><h3 id="3-RAG的进化"><a href="#3-RAG的进化" class="headerlink" title="3. RAG的进化"></a>3. RAG的进化</h3><p><img src="https://s.alidraft.com/vent/202312211123188.png" alt="image.png"></p><ol><li><p><strong>初级RAG（Naive RAG）</strong>：这是RAG的最初形式，包括基本的索引、检索和生成过程。它以简单的方式将检索到的信息与生成任务相结合，但可能存在准确性和效率的问题。</p></li><li><p><strong>高级RAG（Advanced RAG）</strong>：在初级RAG的基础上，高级RAG引入了预检索和后检索处理方法，优化了索引和检索流程。这种范式致力于提高检索内容的质量和相关性，以及提升生成任务的效果。</p></li><li><p><strong>模块化RAG（Modular RAG）</strong>：这种范式通过引入多样的模块，如搜索模块、记忆模块和额外的生成模块，提供了更多的灵活性和定制化能力。模块化RAG允许根据特定问题的上下文重新配置或替换模块，实现更复杂和高效的检索增强生成任务。</p></li></ol><p>再次推荐阅读综述文章： <strong><a href="https://arxiv.org/pdf/2312.10997.pdf">Retrieval-Augmented Generation for Large Language Models: A Survey”</a></strong> </p><h3 id="4-RAG能干嘛？"><a href="#4-RAG能干嘛？" class="headerlink" title="4. RAG能干嘛？"></a>4. RAG能干嘛？</h3><p>RAG的LLM应用的用例无穷无尽，但大致可以分为三类：</p><p><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html"><strong>查询引擎</strong></a>: 查询引擎允许您对您的数据提出问题。它接收自然语言查询，并返回响应，以及检索并传递给LLM的参考上下文。</p><p><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html"><strong>聊天引擎</strong></a>： 聊天引擎用于与您的数据进行<strong>多轮对话</strong>。</p><p><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html"><strong>Agent(代理)</strong></a>: Agent由LLM驱动，能够实现自动决策。可以采取任意数量的步骤来完成给定的任务，动态地决定最佳行动方案。Agent某种意义上来讲是一种AGI。</p><h3 id="5-给我也搞一个"><a href="#5-给我也搞一个" class="headerlink" title="5. 给我也搞一个"></a>5. 给我也搞一个</h3><p><strong>可以！</strong><br>接下来我们基于<a href="https://www.llamaindex.ai/">Llama Index</a>库来实现一个网页数据的Q&amp;A机器人。</p><h2 id="RAG实战"><a href="#RAG实战" class="headerlink" title="RAG实战"></a>RAG实战</h2><p>2023 年以来，出现了大量的开源 &amp; 闭源LLM大模型，基本上都能够在上面构建 RAG 系统。<br>最常见的方式包括： </p><ul><li>GPT-3.5&#x2F;4 + RAG（闭源方案） </li><li>Llama 2 &#x2F; Mistral + RAG（开源方案）</li></ul><h3 id="基于LLama-Index-和-GPT3-5-来构建"><a href="#基于LLama-Index-和-GPT3-5-来构建" class="headerlink" title="基于LLama-Index 和 GPT3.5 来构建"></a>基于LLama-Index 和 GPT3.5 来构建</h3><p>我们基于来LLama-Index 和 GPT3.5 来构建一个RAG系统，它能够访问你指定的<strong>网页数据</strong>，你可以提问关于这个网页的<strong>任何内容</strong>。</p><blockquote><ol><li>Llama-Index是一个简单灵活的数据框架，用于连接自定义数据源到大型语言模型（LLM）。</li><li>它提供了API和入门教程，方便用户进行数据的摄取和查询。</li><li>Llama-Index可以作为桥梁，连接自定义数据和大型语言模型。</li><li>通过Llama-Index，用户可以轻松构建应用程序，并访问私有或特定领域的数据。</li></ol></blockquote><ul><li>复习一遍流程：加载、索引、存储、查询、评估</li></ul><h3 id="加载库和数据"><a href="#加载库和数据" class="headerlink" title="加载库和数据"></a>加载库和数据</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment"># 安装所需的库</span><br>!pip install llama-index transformers<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> llama_index.readers <span class="hljs-keyword">import</span> BeautifulSoupWebReader<br><br>  <br><span class="hljs-comment"># 访问智写AI的官网博客</span><br>url = <span class="hljs-string">&quot;https://www.draftai.cn/2023/12/19/chatonce-support-chat-with-file/&quot;</span><br><span class="hljs-comment"># 通过BeautifulSoupWebReader 来加载数据</span><br>documents = BeautifulSoupWebReader().load_data([url])<br><br></code></pre></td></tr></table></figure><h3 id="索引-存储"><a href="#索引-存储" class="headerlink" title="索引&amp;存储"></a>索引&amp;存储</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment">## index </span><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> openai<br><br><span class="hljs-comment">#设置你在openai的密钥</span><br>os.environ[<span class="hljs-string">&#x27;OPENAI_API_KEY&#x27;</span>] = <span class="hljs-string">&quot;sk-&quot;</span><br><br>openai.api_key = os.environ[<span class="hljs-string">&#x27;OPENAI_API_KEY&#x27;</span>]<br><span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI<br><br>  <br><span class="hljs-comment">## 指定GPT3.5模型，记得要用gpt-3.5-turbo-1106，更便宜</span><br>llm = OpenAI(model=<span class="hljs-string">&quot;gpt-3.5-turbo-1106&quot;</span>, temperature=<span class="hljs-number">0</span>)<br><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> ServiceContext<br><br>  <br><span class="hljs-comment">## 向量化，采用BAAI的向量库，开源免费，比用OpenAI的embbeding便宜。</span><br>service_context = ServiceContext.from_defaults(llm=llm, embed_model=<span class="hljs-string">&quot;local:BAAI/bge-small-zh-v1.5&quot;</span>) <span class="hljs-comment">#BAAI/bge-small-zh-v1.5. BAAI/bge-small-en-v1.5</span><br><br></code></pre></td></tr></table></figure><h3 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> llama_index.response.notebook_utils <span class="hljs-keyword">import</span> display_response<br><span class="hljs-keyword">import</span> logging<br><br><span class="hljs-keyword">import</span> sys<br><br>  <br><span class="hljs-comment"># 打印日志组件</span><br>logging.basicConfig(stream=sys.stdout, level=logging.INFO)<br><br>logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))<br>query_engine = vector_index.as_query_engine(response_mode=<span class="hljs-string">&quot;compact&quot;</span>)<br><br>  <br><span class="hljs-comment"># 简单查询问题</span><br>response = query_engine.query(<span class="hljs-string">&quot;智写AI能干嘛?它最新的功能是什么？&quot;</span>)<br><br>  <br><span class="hljs-comment"># 展示返回结果</span><br>display_response(response)<br></code></pre></td></tr></table></figure><h3 id="不同的查询策略和效果"><a href="#不同的查询策略和效果" class="headerlink" title="不同的查询策略和效果"></a>不同的查询策略和效果</h3><p><img src="https://s.alidraft.com/vent/202312211901720.png" alt="image.png"></p><p><img src="https://s.alidraft.com/vent/202312211900917.png" alt="image.png"></p><p><img src="https://s.alidraft.com/vent/202312211903829.png" alt="image.png"></p><h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p>可以复制我在Colab的<a href="https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing">notebook </a>直接运行。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: <em><a href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a></em></p><p>[2]<strong>查询引擎</strong>: <em><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html">https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html</a></em></p><p>[3]<strong>聊天引擎</strong>: <em><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html">https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html</a></em></p><p>[4]<strong>Agent(代理)</strong>: <em><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html</a></em></p><p>[5]Llama Index: <em><a href="https://www.llamaindex.ai/">https://www.llamaindex.ai/</a></em></p><p>[6]notebook : <em><a href="https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing">https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing</a></em></p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>vector databases</tag>
      
      <tag>RAG</tag>
      
      <tag>vector embeedings</tag>
      
      <tag>Llama Index</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>风下之乡</title>
    <link href="/2023/12/25/wind/"/>
    <url>/2023/12/25/wind/</url>
    
    <content type="html"><![CDATA[<p>我喜欢婆罗洲那些黑黢黢的夜晚。空气中弥漫着树芽和湿漉漉的树叶的气味，仅有的声音是来自于青蛙的呱噪和夜鹰的咕咕，唯一的陪伴来自我们自己，仅有的只言片语也我们自己的。 我喜欢那些独自在家的安静日子，那些懒散、无所事事、不值一提的日子。只有时间和孤独，才能让人认识到自己的存在。这些，是我们在婆罗洲所拥有的。</p><p>我也喜欢那些丛林中的旅行，认识了那些我一直想要认识的事物— 阴云惨淡，如地狱般的尼泊湿地；模糊暧昧的绿色河流，美丽刺激的险滩；阳光照射下，色泽醇厚的裸露的脊背。</p>]]></content>
    
    
    <categories>
      
      <category>读书笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Non Fiction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Pinecone和OpenAI的图像搜索系统入门教程</title>
    <link href="/2023/12/18/Introduction%20to%20Image%20Search%20System%20Using%20Pinecone%20and%20OpenAI:%20A%20Tutorial/"/>
    <url>/2023/12/18/Introduction%20to%20Image%20Search%20System%20Using%20Pinecone%20and%20OpenAI:%20A%20Tutorial/</url>
    
    <content type="html"><![CDATA[<p>本教程将向您详细介绍基于Pinecone和OpenAI的图像搜索系统的工作原理，并提供示例代码帮助您实现关键功能。让我们开始吧!</p><h2 id="Pinecone和OpenAI简介"><a href="#Pinecone和OpenAI简介" class="headerlink" title="Pinecone和OpenAI简介"></a>Pinecone和OpenAI简介</h2><p>Pinecone是一个高性能的向量数据库，可以存储和搜索大量图片及其特征向量。OpenAI API提供了强大的语言理解模型，可以分析文本的语义。结合两者，我们可以实现智能的语义图片搜索。</p><h3 id="Pinecone的优点"><a href="#Pinecone的优点" class="headerlink" title="Pinecone的优点"></a>Pinecone的优点</h3><ul><li>海量图片存储和毫秒级检索</li><li>数据易导入，支持各种机器学习框架</li><li>提供便捷的Python SDK</li><li>过滤和排序功能强大、灵活</li></ul><h3 id="OpenAI-API的优势"><a href="#OpenAI-API的优势" class="headerlink" title="OpenAI API的优势"></a>OpenAI API的优势</h3><ul><li>提供多种预训练语言模型可直接使用</li><li>模型支持关键词提取、分类、语义搜索等能力 </li><li>使用简单的API调用即可获得结果</li><li>持续优化模型性能</li></ul><h2 id="快速上手Pinecone"><a href="#快速上手Pinecone" class="headerlink" title="快速上手Pinecone"></a>快速上手Pinecone</h2><p>首先需要准备图片数据集，提取特征向量，并导入Pinecone中。这些特征向量可以帮助我们在高维空间中表示和搜索图片。</p><h3 id="安装库"><a href="#安装库" class="headerlink" title="安装库"></a>安装库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">pip install pinecone torchvision<br></code></pre></td></tr></table></figure><h3 id="提取图片向量"><a href="#提取图片向量" class="headerlink" title="提取图片向量"></a>提取图片向量</h3><p>使用预训练的模型从图片中提取特征向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model = torchvision.models.inception_v3()<br><br>imgs = load_images() <br>vectors = extract_vectors(model, imgs)<br></code></pre></td></tr></table></figure><h3 id="创建索引和导入数据"><a href="#创建索引和导入数据" class="headerlink" title="创建索引和导入数据"></a>创建索引和导入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pinecone<br><br>index = pinecone.Index(<span class="hljs-string">&#x27;image-search&#x27;</span>)<br>index.upsert(ids, vectors)<br></code></pre></td></tr></table></figure><h3 id="执行搜索"><a href="#执行搜索" class="headerlink" title="执行搜索"></a>执行搜索</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results = index.query(query_vector)<br></code></pre></td></tr></table></figure><h2 id="用Python实现图片搜索"><a href="#用Python实现图片搜索" class="headerlink" title="用Python实现图片搜索"></a>用Python实现图片搜索</h2><p>下面我们使用Pinecone Python SDK进行图片搜索。初始化客户端是与Pinecone服务建立连接的第一步。</p><h3 id="初始化客户端"><a href="#初始化客户端" class="headerlink" title="初始化客户端"></a>初始化客户端</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pinecone<br><br>client = pinecone.Client()<br>index = client.init_index(<span class="hljs-string">&#x27;image-search&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="向量搜索"><a href="#向量搜索" class="headerlink" title="向量搜索"></a>向量搜索</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">results = index.query(image_vector)  <br></code></pre></td></tr></table></figure><h3 id="多向量搜索"><a href="#多向量搜索" class="headerlink" title="多向量搜索"></a>多向量搜索</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">vectors = [vec1, vec2, vec3]<br>results = index.query(vectors, top_k=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h3 id="过滤搜索"><a href="#过滤搜索" class="headerlink" title="过滤搜索"></a>过滤搜索</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">filters = &#123;<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;cat&#x27;</span>&#125;<br>results = index.query(vector, <span class="hljs-built_in">filter</span>=filters)  <br></code></pre></td></tr></table></figure><h2 id="利用OpenAI理解搜索意图"><a href="#利用OpenAI理解搜索意图" class="headerlink" title="利用OpenAI理解搜索意图"></a>利用OpenAI理解搜索意图</h2><p>通过OpenAI API分析查询语义，转换为搜索语句。这样，我们可以更准确地理解用户的搜索意图，并提供更相关的搜索结果。</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> openai <br><br>openai.api_key = <span class="hljs-string">&#x27;YOUR_API_KEY&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="解析查询意图"><a href="#解析查询意图" class="headerlink" title="解析查询意图"></a>解析查询意图</h3><p>根据用户输入的文本，使用OpenAI API提取关键词或主题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">text = <span class="hljs-string">&quot;Find cute cats&quot;</span><br>response = openai.Completion.create(prompt=text)<br>keywords = response.keywords <br></code></pre></td></tr></table></figure><h3 id="构造搜索语句"><a href="#构造搜索语句" class="headerlink" title="构造搜索语句"></a>构造搜索语句</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">image_query = &#123;<br>  <span class="hljs-string">&#x27;vector&#x27;</span>: cat_vector,<br>  <span class="hljs-string">&#x27;filter&#x27;</span>: &#123;<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-string">&#x27;attributes&#x27;</span>: keywords&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="搜索示例"><a href="#搜索示例" class="headerlink" title="搜索示例"></a>搜索示例</h3><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-string">&quot;Cute kittens and puppies&quot;</span><br></code></pre></td></tr></table></figure><p>返回<code>kittens</code>和<code>puppies</code>作为关键词，构造多向量搜索。</p><h2 id="实现个性化推荐"><a href="#实现个性化推荐" class="headerlink" title="实现个性化推荐"></a>实现个性化推荐</h2><p>最后，我们借助OpenAI实现基于用户的个性化图片推荐。通过分析用户的搜索历史和行为，我们可以为他们提供更相关的图片推荐。</p><h3 id="收集用户数据"><a href="#收集用户数据" class="headerlink" title="收集用户数据"></a>收集用户数据</h3><p>跟踪用户搜索查询、点击等行为数据。</p><h3 id="生成用户向量"><a href="#生成用户向量" class="headerlink" title="生成用户向量"></a>生成用户向量</h3><p>根据用户的行为数据，使用OpenAI生成一个代表用户偏好的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">user_data = [queries, clicks, ...] <br><br>embedding = openai.Embedding.create(<span class="hljs-built_in">input</span>=user_data)<br>user_vector = embedding[<span class="hljs-string">&#x27;data&#x27;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;embedding&#x27;</span>]<br></code></pre></td></tr></table></figure><h3 id="个性化推荐"><a href="#个性化推荐" class="headerlink" title="个性化推荐"></a>个性化推荐</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">results = index.query(user_vector, top_k=<span class="hljs-number">30</span>)<br>ranked_images = rerank(results, user_profile)<br></code></pre></td></tr></table></figure><p>返回用户偏好的图片。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过组合Pinecone和OpenAI，我们可以实现智能的语义图片搜索和个性化推荐。这个指南向您展示了主要的实现流程和代码示例。希望它可以帮助您快速上手构建自己的图片搜索应用程序!</p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>大语言模型部署应用与基础设施成本优化--风爷推荐-E08-Week31 2023</title>
    <link href="/2023/12/18/Large%20Language%20Model%20Deployment%20Application%20and%20Infrastructure%20Cost%20Optimization-E08-Week31%202023/"/>
    <url>/2023/12/18/Large%20Language%20Model%20Deployment%20Application%20and%20Infrastructure%20Cost%20Optimization-E08-Week31%202023/</url>
    
    <content type="html"><![CDATA[<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h1><p>ChatGPT、LLaMa、Bard 等大语言模型(LLMs)取得了非常巨大突破，迅速在公众领域流行起来。LLMs所展现的强大文本生产能力让用户惊叹不已，属于划时代的产品。这些模型拥有数十亿甚至数千亿个参数，因而这些模型通常的部署和维护成本都惊人的高昂。这类大模型的的训练和推理都需要大量的计算资源和内存资源，企业需要投入海量的<strong>基础设施成本</strong>（不管是云服务还是自建机房都非常贵)，来保证大模型能够稳定提供服务。</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-1.png"></p><p>那么有没有办法花小钱办大事呢？<br><em>当然有。</em></p><p>本文旨在提供一些策略、提示和技巧，您可以在部署基础架构时应用这些策略、提示和技巧来优化基础架构。我们将重点探讨这些内容：</p><ul><li><ol><li>大模型部署与应用时将会面临的基础架构挑战</li></ol></li><li><ol start="2"><li>如何降低大模型部署与应用的成本</li></ol></li><li><ol start="3"><li>其他的一些有用的策略</li></ol></li></ul><h1 id="2-大模型部署与应用的挑战"><a href="#2-大模型部署与应用的挑战" class="headerlink" title="2. 大模型部署与应用的挑战"></a>2. 大模型部署与应用的挑战</h1><p>LLMs遵循规模效应，也就是说参数越大，效果越好。因此它们一般需要海量GPU计算资源才能获得最佳性能。通常会面临以下的挑战：</p><h2 id="2-1-高计算量"><a href="#2-1-高计算量" class="headerlink" title="2.1 高计算量"></a>2.1 高计算量</h2><p>部署 LLM 是一个充满挑战的任务，因为它们需要大量计算资源来执行推理，尤其是模型用于实时应用程序（例如聊天机器人或虚拟助手）为甚。<br>以 ChatGPT 为例，大多数情况下它能够在几秒钟内处理和响应用户查询。尤其是繁忙时段瞬间涌入海量用户会使得推理时间会变长。还有其他因素可能会延迟推理，例如问题的复杂性、生成响应所需的信息量等等。总而言之，大模型要提供实时服务，它必须能够实现<strong>高吞吐量</strong>和<strong>低延迟</strong>。</p><h2 id="2-2-大存储量"><a href="#2-2-大存储量" class="headerlink" title="2.2 大存储量"></a>2.2 大存储量</h2><p>由于模型参数规模从数百万到数千亿，LLM 的存储也是一个充满挑战的问题。由于大模型规模太大，所以无法直接将整个模型存储在单个存储设备。<br>例如，OpenAI 的 <strong>GPT-3</strong> 模型有 <strong>1750亿</strong> 个参数，仅其权重参数存储就需要超过 <strong>300GB</strong> 的存储空间。<br>此外，它还需要至少具有 16GB 显存的 GPU 才能高效运行（意味着起码是T4级别以上的N卡)。<br>因此，在单个设备上存储和运行如此大的模型对于许多用户场景来说是不切实际的。整体来说，<br>LLM 的存储容量存在三个主要问题：</p><ul><li><strong>内存限制</strong> : LLMs需要大量内存，因为它们要处理大量信息。部署此类模型的一种方法是使用分布式系统，模型分布在多个服务器节点上。这种系统允许将推理任务切分分配到多台服务器上，实现负载均衡和推理加速。这类系统通常架构都比较复杂，需要大量的专业知识来设置和维护这些分布式机器。模型越大，需要的服务器就越多，这也增加了部署成本。还有一种复杂的场景就是，如何将大模型部署在手机等内存较小的设备上。</li><li><strong>模型规模</strong>  : 如果输入查询又长又复杂，即便运行在大内存显卡上的模型推理过程中也很容易耗尽内存。即使对于 LLM 的基本推理，也需要多个加速器或多节点计算集群（例如多个 Kubernetes Pod）。</li><li><strong>可扩展性</strong>  : 大模型通常使用<strong>模型并行化</strong>（<strong>MP</strong>）进行扩展，这涉及将模型分成更小的部分并将其分布在多台机器上。每台机器处理模型的不同部分，并将结果组合起来产生最终输出。该技术有助于大模型训练，但也需要仔细考虑机器之间的通信开销。</li></ul><h2 id="2-3-网络带宽"><a href="#2-3-网络带宽" class="headerlink" title="2.3 网络带宽"></a>2.3 网络带宽</h2><p>如上所述，LLM 必须使用 MP 进行扩展。但我们发现的问题是，模型并行化 在单节点集群中是有较好效果，但在多节点集群中，由于网络通讯开销，导致推理效率不高。</p><h2 id="2-4-成本与能耗"><a href="#2-4-成本与能耗" class="headerlink" title="2.4  成本与能耗"></a>2.4  成本与能耗</h2><p>如上文所述，部署和使用 LLM 的成本可能很高，包括硬件和基础设施的成本，尤其是在推理过程中使用 GPU 或 TPU 等资源来实现低延迟和高吞吐量时。对小公司和个人来说，这是一个非常大的挑战。</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-2.png"><br>LLMs的费用估算以及碳足迹｜ <a href="https://sunniesuhyoung.github.io/files/LLM.pdf">来源</a></p><p>根据 <a href="https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/">NVIDIA</a>的说法，80-90% 的机器学习工作负载是<strong>推理带来的</strong>。同样，根据  <a href="https://aws.amazon.com/blogs/aws/amazon-ec2-update-inf1-instances-with-aws-inferentia-chips-for-high-performance-cost-effective-inferencing/">AWS</a> 的数据，<strong>推理占云中机器学习需求</strong>的 90%。</p><p>在22年12月份，chatGPT 的运行成本约为每天 100,000 美元或每月 300 万美元。随着ChatGPT的大获成功，GPT-4的推出等，估计现在(23年7月)估计要比当时(22年12月)高出一个数量级了</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-3.png"></p><p>关于ChatGPT成本的推文 | <a href="https://twitter.com/tomgoldsteincs/status/1600196995389366274?lang=en">来源</a>  </p><h1 id="3-优化大模型基础设施成本的策略"><a href="#3-优化大模型基础设施成本的策略" class="headerlink" title="3. 优化大模型基础设施成本的策略"></a>3. 优化大模型基础设施成本的策略</h1><p>在本节中，我们将探讨并讨论前一节中讨论的挑战的可能解决方案和技术。</p><p>首先以AWS作为云供应商，来实现大模型推理的工作流作为例子：<br><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-4.png"></p><p>AWS上的大模型推理的工作流 | <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">来源</a>  </p><p>您可以按照以下的步骤尽可能高效地部署大模型。</p><h2 id="3-1-云计算预算评估与规划"><a href="#3-1-云计算预算评估与规划" class="headerlink" title="3.1 云计算预算评估与规划"></a>3.1 云计算预算评估与规划</h2><p>使用云计算服务可以提供动态、按需使用包括CPU,GPU,TPU在内的各种强大的计算资源。云计算服务灵活且可扩展性强，但是在使用云服务的时候，首先你需要为自己的项目制定一个项目预算，这样能够让你的基础设施投入更加合理可控。<br>云服务提供商如AWS、Azure和google cloud提供了一系列部署LLM的产品，包括虚拟机、容器和<strong>无服务器计算</strong>。但是尽管如此，建议还是需要根据自己业务情况进行研究和计算，选择更加合理的云服务解决方案。例如，你必须核实以下三个方面信息：</p><ol><li>模型尺寸</li><li>关于要使用的硬件的详情</li><li>合理的推理产品方案<br>根据上述三个方面的信息，可以计算出你需要多少加速计算能力，从而规划并执行适合你自身业务的大模型部署。</li></ol><p><a href="https://neptune.ai/blog/mlops-tools-for-nlp-projects"> 大模型的MLOps工具</a></p><h3 id="3-1-1-计算模型大小"><a href="#3-1-1-计算模型大小" class="headerlink" title="3.1.1 计算模型大小"></a>3.1.1 计算模型大小</h3><p>您可以根据以下表格，折算自己模型大概需要多少多少FLOPs算力，从而确定要在云平台上找到相相应的GPU。</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-5-1.png"><br> <a href="https://arxiv.org/pdf/2203.15556.pdf">预估计算FLOPs</a><br>另外<a href="https://www.lesswrong.com/posts/HvqQm6o8KnwxbdmhZ/estimating-training-compute-of-deep-learning-models">这个工具</a>也可以帮你计算模型在训练和推理过程中所需的FLOPs。</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-6.png"></p><p> <a href="https://www.lesswrong.com/posts/HvqQm6o8KnwxbdmhZ/estimating-training-compute-of-deep-learning-models">一个用于计算训练和推理所需的FLOPs的工具</a>  </p><h3 id="3-1-2-选择合适的硬件"><a href="#3-1-2-选择合适的硬件" class="headerlink" title="3.1.2 选择合适的硬件"></a>3.1.2 选择合适的硬件</h3><p>当你计算出所需的FLOPs，就可以继续选择GPU。确保你了解GPU所提供的功能。例如，查看下面的图片以了解情况。可以参考以下A100的GPU规格，选择符合预算的显卡。最新的H100芯片可以访问<a href="https://www.nvidia.com/en-sg/data-center/h100/">这个链接</a></p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-7.png"></p><p><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">NVIDIA提供的GPU规格清单</a>  </p><h3 id="3-1-3-选择正确的推理产品"><a href="#3-1-3-选择正确的推理产品" class="headerlink" title="3.1.3 选择正确的推理产品"></a>3.1.3 选择正确的推理产品</h3><p><a href="https://aws.amazon.com/pm/sagemaker/?trk=b92e5bb5-847d-49ae-bd86-21239cc9ac5e&sc_channel=ps&ef_id=CjwKCAjwzo2mBhAUEiwAf7wjkpU15inCoHRO6RFplymcFt_4vahy0S3NV44y0bZjEymm-0nDJCjK4BoCIPAQAvD_BwE:G:s&s_kwcid=AL!4422!3!532425958059!e!!g!!amazon%20sagemaker!11543056255!112002968389">Amazon SageMake</a>r是一个机器学习云服务产品，提供多种推理选项，以适应不同的工作负载。例如，如果您需要：</p><ol><li><p><strong>实时推理</strong>, 适用于低延迟或高吞吐量的在线推理，支持最大6 MB的负载大小和60秒的处理时间。</p></li><li><p><strong>无服务器推理</strong>,适用于间歇性或不可预测的流量模式，并支持高达4 MB的负载大小和60秒的处理时间。在无服务器推理中，模型根据流量弹性伸缩，自动扩容或者缩容。</p></li><li><p><strong>批量处理</strong> ，适用于大型数据集的离线处理，适合以GB为单位的负载大小和以天为单位的处理时间的场景。</p></li><li><p><strong>异步推理</strong> ，适用于排队具有大负载和长处理时间的请求，支持最大1 GB的负载和最长一小时的处理时间. 。同样支持弹性伸缩。</p></li></ol><p>为了更好地理解并满足您的要求，请查看下方的图片。</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-8.png"></p><p><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">选择模型部署类型</a>  </p><p>当满足以上所有要点时，您可以将模型部署在任何云服务上。</p><h3 id="3-1-4-小结："><a href="#3-1-4-小结：" class="headerlink" title="3.1.4 小结："></a>3.1.4 小结：</h3><ul><li><strong>1 设定预算</strong></li><li><strong>2  计算模型的大小</strong></li><li><strong>3 计算模型所需的FLOPs</strong></li><li><strong>4 找到合适的GPU</strong></li><li><strong>5 选择适当的推理类型</strong></li><li><strong>6 研究各个云计算平台提供的定价</strong></li><li><strong>7 找到适合您需求和预算的服务</strong></li><li><strong>8 部署</strong></li></ul><h2 id="3-2-优化模型以提供服务"><a href="#3-2-优化模型以提供服务" class="headerlink" title="3.2 优化模型以提供服务"></a>3.2 优化模型以提供服务</h2><p>在上一节中，我们讨论了不同规模的LLM的规模如何部署。如果当我们的模型过大时，可以采用模型编译、模型压缩和模型分片等策略。这些技术可以在保持准确性的同时减小模型的大小，部署起来更容易，与之同时会显著降低相关费用。</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-9.png"><br> <a href="https://d1.awsstatic.com/events/Summits/reinvent2022/AIM405_Train-and-deploy-large-language-models-on-Amazon-SageMaker.pdf">优化LLMs以进行部署的不同技术或策略</a>  </p><h3 id="3-2-1-模型压缩"><a href="#3-2-1-模型压缩" class="headerlink" title="3.2.1 模型压缩"></a>3.2.1 模型压缩</h3><p>模型压缩的目标是通过利用硬件特定优化，如减少内存占用、改善计算并行性和减少延迟，来提高LLM推理的性能和效率。模型压缩能够帮助你尝试不同的技术组合，为各种任务设定性能基准，并找到适合预算的方案。模型压缩一般涉及几个步骤：</p><ol><li><strong>计算图优化(Graph optimization)</strong>: 使用<strong>剪枝</strong>和<strong>量化</strong>等优化技术对高级LLM图进行转换和优化，以降低模型的计算复杂度和内存占用。这样，模型变得更小，同时保持其准确性。</li><li><strong>硬件感知优化(Hardware-specific optimization)</strong>: 在优化过的LLM计算图基础上进一步实现硬件优化。Amazon Sagemaker为各种流行的ML框架提供了基于硬件优化的模型服务容器以及SDK，包括XGBoost，scikit-learn，PyTorch，TensorFlow和Apache MXNet。</li></ol><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-10.png"></p><p> <a href="https://aws.amazon.com/sagemaker/neo/">AWS Sagemaker Neo的工作原理</a>  </p><p>以下是一些必须了解的模型压缩技术。</p><h4 id="3-2-1-1-模型量化"><a href="#3-2-1-1-模型量化" class="headerlink" title="3.2.1.1 模型量化"></a>3.2.1.1 模型量化</h4><p>模型量化（MQ）是一种减少机器学习模型大小和计算复杂性的技术。在模型量化中，我们将模型的权重和激活函数从浮点数（例如32位）转换为更小的数据类型（例如8位整数）。这样做的原因是，更小的数据类型需要更少的存储空间和计算资源。</p><p>这个过程可以类比为我们在生活中的经验。比如，你有一张非常详细的地图，这张地图上的每一条街道、每一棵树都画得非常清楚。但是，这张地图非常大，你无法把它放进口袋里。于是，你决定制作一张简化版的地图，只标注主要的街道和地标。这样，你的地图就变小了，可以放进口袋里，但是它仍然包含了你需要的主要信息。</p><p>在模型量化中，我们也是这样做的。我们将模型的权重和激活函数简化，使其变小，但仍然尽可能地保留了原始模型的信息。</p><p>然而，这种简化过程确实可能会导致一些信息的丢失，这就是所谓的量化误差。这种误差可能会对模型的精度产生影响。但是，通过一些技术，如重新校准（re-calibration）和量化感知训练（quantization-aware training），我们可以在一定程度上减小这种影响。这些技术可以帮助模型在量化过程中适应信息的丢失，从而在减小模型大小的同时，尽可能地保持模型的精度。<br>PyTorch 提供了模型量化的能力。PyTorch 提供了一套完整的量化工具，包括动态量化（Dynamic Quantization）、静态量化（Static Quantization）以及量化感知训练（Quantization Aware Training）。</p><ol><li><p><strong>动态量化</strong>：这种方法只量化模型的权重，而不量化激活。这种方法的优点是实现简单，不需要对模型进行任何修改，但可能不如其他方法减少模型大小和提高性能。</p></li><li><p><strong>静态量化</strong>：也称为（<strong>Post-training quantization</strong>)。这种方法在训练后进行，它包括权重和激活的量化，以及模型的重校准。这种方法可以进一步减小模型大小和提高性能，但需要更多的步骤来实现。</p></li><li><p><strong>量化感知训练</strong>：也称为(<strong>Hybrid quantization</strong>). 这种方法在训练过程中引入量化，使模型能够适应量化带来的误差。这种方法可以在减小模型大小和提高性能的同时，保持或提高模型的精度。</p></li></ol><p>PyTorch 提供了一系列的 API 和教程来帮助开发者实现模型量化。这些工具可以帮助你将模型量化，使其更适合在资源有限的设备上运行。</p><p>虽然模型量化有很多优点，如减小模型大小、减少计算需求、提高推理速度等，但是它也有一些潜在的缺点：</p><ol><li><p><strong>精度下降</strong>：量化过程可能会导致一些信息的丢失，这可能会对模型的精度产生影响。虽然有一些技术可以减小这种影响，但在某些情况下，精度的下降可能是无法避免的。</p></li><li><p><strong>实现复杂</strong>：实现模型量化可能需要对模型进行一些修改，这可能会增加实现的复杂性。例如，你可能需要进行量化感知训练，或者对模型进行重校准。</p></li><li><p><strong>硬件兼容性</strong>：虽然模型量化可以帮助模型在资源有限的设备上运行，但并非所有的硬件都支持量化模型。在某些硬件上，运行量化模型可能不会带来预期的性能提升。</p></li><li><p><strong>模型兼容性</strong>：并非所有的模型都适合量化。某些模型可能在量化后的性能下降较大，或者无法进行有效的量化。</p></li></ol><p>因此，虽然模型量化是一种强大的工具，但在使用它时，你需要考虑到这些潜在的问题，并根据你的具体需求和条件来决定是否使用模型量化。</p><h4 id="3-2-1-2-模型剪枝"><a href="#3-2-1-2-模型剪枝" class="headerlink" title="3.2.1.2 模型剪枝"></a>3.2.1.2 模型剪枝</h4><p>模型剪枝（Model Pruning）是一种优化技术，它的目标是通过移除模型中的一部分参数（例如神经网络中的神经元或连接）来减小模型的大小和计算需求，同时尽可能地保持模型的性能。</p><p>模型剪枝的基本思想是，模型中的一些参数对模型的性能贡献很小，因此可以安全地移除它们。例如，在神经网络中，我们可以移除那些权重很小的连接，因为它们对模型的输出影响很小。</p><p>模型剪枝有很多种方法，包括：</p><ol><li><p><strong>权重剪枝</strong>：这种方法移除那些权重值小于某个阈值的连接。这种方法的优点是实现简单，但可能会导致模型的结构变得不规则，从而影响硬件的优化。</p></li><li><p><strong>单位剪枝</strong>：这种方法移除整个神经元或者卷积核。这种方法可以保持模型的结构规则，从而更适合硬件的优化，但可能会对模型的性能产生更大的影响。</p></li><li><p><strong>结构剪枝</strong>：这种方法移除模型中的一部分结构，例如卷积层或者全连接层。这种方法可以大大减小模型的大小，但需要更复杂的算法来确定应该移除哪些结构。</p></li><li><p><strong>稀疏剪枝</strong>：这种方法试图在保持模型性能的同时，最大化模型的稀疏性。这通常通过一些优化算法来实现，例如L1正则化。</p></li><li><p><strong>动态剪枝</strong>：这种方法在模型的训练过程中动态地进行剪枝。这种方法的优点是可以根据模型的训练情况来调整剪枝的策略，但实现起来可能比较复杂。</p></li></ol><p>下面是一个使用 PyTorch 进行权重剪枝的简单例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.utils.prune <span class="hljs-keyword">as</span> prune<br><br><span class="hljs-comment"># 创建一个简单的线性模型</span><br>model = nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 打印模型的权重</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;原始权重:&#x27;</span>)<br><span class="hljs-built_in">print</span>(model.weight)<br><br><span class="hljs-comment"># 使用 L1Norm 方法进行剪枝，剪掉 30% 的权重</span><br>prune.l1_unstructured(model, name=<span class="hljs-string">&quot;weight&quot;</span>, amount=<span class="hljs-number">0.3</span>)<br><br><span class="hljs-comment"># 打印剪枝后的权重</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;剪枝后的权重:&#x27;</span>)<br><span class="hljs-built_in">print</span>(model.weight)<br></code></pre></td></tr></table></figure><p>在这个例子中，首先创建了一个简单的线性模型。然后，使用 <code>prune.l1_unstructured</code> 方法进行剪枝，剪掉了 30% 的权重。最后，打印出了剪枝后的权重。</p><p>这只是一个非常简单的例子，实际的剪枝过程可能会更复杂。例如，可能需要在剪枝后重新训练模型，以恢复模型的性能，也可能需要使用更复杂的剪枝方法，例如单位剪枝或结构剪枝。</p><p>你可以通过PyTorch的这个<a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb">Colab</a>笔记本来更好地了解MP。</p><h4 id="3-2-1-3-模型蒸馏"><a href="#3-2-1-3-模型蒸馏" class="headerlink" title="3.2.1.3 模型蒸馏"></a>3.2.1.3 模型蒸馏</h4><p>模型蒸馏是一种模型压缩技术，它的主要步骤如下：</p><ol><li><p><strong>训练教师模型</strong>：首先，需要训练一个大的模型，这个模型通常被称为教师模型。教师模型通常是一个深度的、复杂的模型，它可以在训练数据上达到很高的性能。</p></li><li><p><strong>获取教师模型的输出</strong>：然后，使用教师模型对训练数据进行预测，获取教师模型的输出。这些输出通常包括类别的预测，以及预测的概率或者置信度。</p></li><li><p><strong>训练学生模型</strong>：接着，训练一个小的模型，这个模型通常被称为学生模型。学生模型通常是一个浅度的、简单的模型，它的目标是学习教师模型的输出。在训练过程中，不仅要最小化学生模型的输出和数据的标签之间的差异，还要最小化学生模型的输出和教师模型的输出之间的差异。</p></li><li><p><strong>评估学生模型</strong>：最后，评估学生模型的性能。如果学生模型的性能达到了我们的要求，那么就可以使用学生模型来替代教师模型了。</p></li></ol><p>以BERT为例，模型蒸馏工作流程请参见下方的图片。</p><p><img src="https://s.zhangguiyi.cn/vent/optimizing-infrastructure-costs-for-deploying-large-nlp-models-11.png"></p><p><a href="https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a">DistilBERT的蒸馏过程</a>  </p><p>模型蒸馏的具体方法有很多种，以下是一些常见的方法：</p><ol><li><p><strong>软标签蒸馏</strong>：这是最常见的模型蒸馏方法，也是模型蒸馏的基础。在这种方法中，教师模型的输出（通常是概率分布）被用作学生模型的目标。这种方法可以帮助学生模型学习到教师模型的知识，包括类别之间的关系和对于某些难以分类的样本的不确定性。</p></li><li><p><strong>特征蒸馏</strong>：在这种方法中，我们不仅要让学生模型学习到教师模型的输出，还要让学生模型学习到教师模型的中间特征。这种方法可以帮助学生模型学习到更深层次的知识，从而提高模型的性能。</p></li><li><p><strong>关系蒸馏</strong>：在这种方法中，我们让学生模型学习到教师模型的输出之间的关系。例如，我们可以让学生模型学习到教师模型对于一对样本的相对预测。这种方法可以帮助学生模型学习到更复杂的知识，从而提高模型的性能。</p></li><li><p><strong>注意力蒸馏</strong>：在这种方法中，我们让学生模型学习到教师模型的注意力分布。这种方法可以帮助学生模型学习到教师模型的注意力机制，从而提高模型的性能。</p></li></ol><p>我们用一个使用 PyTorch 实现模型蒸馏的代码示例来加深理解。<br>在这个例子中，我们首先创建了一个教师模型和一个学生模型，然后使用自定义的损失函数进行模型蒸馏。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># 创建教师模型和学生模型</span><br>teacher = nn.Sequential(nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">1200</span>), nn.ReLU(), nn.Linear(<span class="hljs-number">1200</span>, <span class="hljs-number">10</span>))<br>student = nn.Sequential(nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">800</span>), nn.ReLU(), nn.Linear(<span class="hljs-number">800</span>, <span class="hljs-number">10</span>))<br><br><span class="hljs-comment"># 定义损失函数和优化器</span><br>criterion = nn.CrossEntropyLoss()<br>optimizer = optim.SGD(student.parameters(), lr=<span class="hljs-number">0.01</span>, momentum=<span class="hljs-number">0.9</span>)<br><br><span class="hljs-comment"># 定义蒸馏的温度和权重</span><br>temperature = <span class="hljs-number">2.0</span><br>alpha = <span class="hljs-number">0.5</span><br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):  <span class="hljs-comment"># 进行 100 个训练周期</span><br>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(trainloader, <span class="hljs-number">0</span>):<br>        inputs, labels = data<br><br>        <span class="hljs-comment"># 计算教师模型的输出</span><br>        teacher_outputs = teacher(inputs)<br>        teacher_probs = torch.nn.functional.softmax(teacher_outputs / temperature, dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># 计算学生模型的输出</span><br>        student_outputs = student(inputs)<br>        student_probs = torch.nn.functional.softmax(student_outputs / temperature, dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># 计算损失</span><br>        loss1 = criterion(student_outputs, labels)<br>        loss2 = criterion(student_probs.log(), teacher_probs)<br>        loss = alpha * loss1 + (<span class="hljs-number">1</span> - alpha) * loss2<br><br>        <span class="hljs-comment"># 反向传播和优化</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br></code></pre></td></tr></table></figure><p>在这个例子中，首先创建了一个教师模型和一个学生模型。然后，定义了损失函数和优化器。还定义了蒸馏的温度和权重，这两个参数用于控制蒸馏的强度。</p><p>在训练过程中，首先计算教师模型的输出，然后将这些输出转换为概率分布。接着，我们计算学生模型的输出，然后将这些输出转换为概率分布。然后，计算两种损失：一种是学生模型的输出和数据标签之间的损失，另一种是学生模型的输出和教师模型的输出之间的损失。这两种损失的权重由 alpha 参数来控制。最后，进行反向传播和优化。</p><p>可以在这个链接中找到完整的代码和更详细的解释：<a href="https://towardsdatascience.com/model-distillation-and-compression-for-recommender-systems-in-pytorch-5d81c0f2c0ec">Towards Data Science: Introduction to PyTorch Model Compression Through Teacher-Student Knowledge Distillation</a>。</p><h3 id="3-2-2-LLM的硬件优化"><a href="#3-2-2-LLM的硬件优化" class="headerlink" title="3.2.2 LLM的硬件优化"></a>3.2.2 LLM的硬件优化</h3><ol><li><p><strong>选择适合LLM需求的硬件</strong>：大型语言模型（LLMs）通常需要大量的计算资源和存储空间。例如，如果我们模型是一个大型的Transformer模型，需要具有大量RAM和多个GPU的高性能服务器。另一方面，如果我们模型是一个小型的RNN模型，可能只需要一个具有适量RAM和一个GPU的普通服务器。选择合适的硬件可以确保我们模型能够有效地运行，同时避免浪费资源。</p></li><li><p><strong>利用专用硬件</strong>：专用硬件，如TPU（张量处理单元），是专门为深度学习任务设计的硬件加速器。TPU在处理张量运算（这是深度学习模型的基础）方面非常高效。例如，Google的BERT模型就是在TPU上训练的。此外，推理时可以考虑使用加速线性代数（XLA）。XLA是一种优化编译器，可以将TensorFlow的计算图优化为高效的机器代码，从而提高性能。</p></li><li><p><strong>使用优化的库</strong>：优化的库，如TensorFlow、PyTorch或JAX，可以利用硬件特性来加速计算。例如，TensorFlow可以自动地将计算任务分配到多个GPU上，从而提高性能。PyTorch则提供了一种动态计算图的特性，可以更灵活地构建和修改模型。JAX则提供了一种函数转换的特性，可以更容易地实现复杂的优化算法。</p></li><li><p><strong>调整批次大小</strong>：在推理过程中，适当调整批次大小可以最大化硬件利用率并提高推理速度。例如，如果GPU有足够的内存，可以增大批次大小，这样可以让GPU同时处理更多的数据，从而提高性能。但是，如果批次大小太大，可能会导致内存溢出。因此，我们需要根据硬件条件和模型需求来调整批次大小。</p></li><li><p><strong>持续监控和优化</strong>：在部署过程中，需要持续监控模型的性能，包括推理速度、内存使用情况、GPU利用率等。如果发现性能有问题，可能需要调整硬件配置，例如增加RAM、增加GPU、升级存储设备等。也可能需要优化模型或代码，例如减小模型大小、优化计算图、减少数据传输等。</p></li></ol><h3 id="3-3-成本效益-Cost-Efficient-CE-的可扩展性"><a href="#3-3-成本效益-Cost-Efficient-CE-的可扩展性" class="headerlink" title="3.3 成本效益(Cost  Efficient,CE) 的可扩展性"></a>3.3 成本效益(Cost  Efficient,CE) 的可扩展性</h3><p>以下是我们可以在控制成本的同时扩展大型自然语言处理模型的方法：</p><ol><li><p><strong>选择合适的推理选项</strong>：例如，我们可以选择使用AWS SageMaker或Google Cloud AI Platform，这些服务可以根据需求动态分配资源，从而在需求较少时降低部署成本。</p></li><li><p><strong>优化推理性能</strong>：我们可以通过使用硬件加速，如GPU或TPU，以及优化推理代码来提高推理性能。例如，我们可以使用TensorRT或OpenVINO这样的库来优化我们的模型，使其能够更有效地在GPU或TPU上运行。</p></li><li><p><strong>使用缓存</strong>：如果我们的模型需要处理大量的重复请求，我们可以使用缓存来提高性能和降低成本。例如，我们可以使用Redis或Memcached这样的缓存服务来存储我们的模型的推理结果。当我们收到一个相同的请求时，我们可以直接从缓存中获取结果，而不需要再次运行模型。这样可以显著减少我们的计算需求，从而降低成本。</p></li></ol><h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h2><p>我们总结一下全文的流程</p><ul><li><strong>设定预算</strong>：明确我们的财务预算，以便在部署大型语言模型时做出明智的决策。</li><li><strong>计算模型大小</strong>：理解我们的模型的规模，以便选择合适的硬件和服务。</li><li><strong>使用模型压缩技术</strong>：通过修剪、量化和蒸馏等技术，可以减少部署所需的内存和计算资源。</li><li><strong>利用云计算服务</strong>：AWS、Google Cloud和Microsoft Azure等云服务提供了经济高效且可扩展的解决方案。</li><li><strong>采用无服务器计算</strong>：无服务器计算提供了按使用付费的模式，可以降低运营成本并实现自动扩展。</li><li><strong>优化硬件加速</strong>：例如，我们可以使用GPU来加速模型的训练和推理，从而提高效率。</li><li><strong>定期监控资源使用</strong>：通过监控，我们可以识别哪些资源被低效使用，或者哪些实例被过度配置，从而找到降低成本的机会。</li><li><strong>持续优化模型和硬件</strong>：我们需要不断地优化我们的模型和硬件配置，以实现高效的推理。</li><li><strong>更新软件和安全补丁</strong>：保持软件和安全补丁的最新状态，以确保我们的系统的安全。</li></ul><p>在本文中，我们探讨了部署大型语言模型时面临的挑战，以及与之相关的基础设施成本。同时，我们也提出了解决这些问题的技术和策略。</p><p>在我们讨论的所有解决方案中，风爷最推荐的是弹性和无服务器推理。虽然模型压缩是一种有效的方法，但是当需求很高时，即使是较小的模型也可能需要大量的资源。因此，需要一个可以根据需求动态调整资源的解决方案，这就是弹性和无服务器推理的优势。</p><p>当然，这些推荐可能并不适合所有的情况，你需要根据你自己的需求和问题来选择最合适的方法。风爷希望这些讨论可以帮助你在部署大型语言模型时降低基础设施成本。</p><p>#AIGC #LLMs #Infra #AWS #大模型 </p><h3 id="References-参考文献"><a href="#References-参考文献" class="headerlink" title="References 参考文献"></a>References 参考文献</h3><ol><li><a href="https://research.aimultiple.com/large-language-model-training/">Large Language Model Training in 2023</a></li><li><a href="https://d1.awsstatic.com/events/Summits/reinvent2022/AIM405_Train-and-deploy-large-language-models-on-Amazon-SageMaker.pdf">https://d1.awsstatic.com/events/Summits/reinvent2022/AIM405_Train-and-deploy-large-language-models-on-Amazon-SageMaker.pdf</a></li><li><a href="https://research.aimultiple.com/ai-chip-makers/">Top 10 AI Chip Makers of 2023: In-depth Guide</a> </li><li><a href="https://www.nvidia.com/en-us/data-center/dgx-a100/">https://www.nvidia.com/en-us/data-center/dgx-a100/</a></li><li><a href="https://arxiv.org/pdf/2302.13971.pdf">LLaMA: A foundational, 65-billion-parameter large language model</a></li><li><a href="https://arxiv.org/pdf/2203.15556.pdf">https://arxiv.org/pdf/2203.15556.pdf</a></li><li><a href="https://huggingface.co/docs/transformers/model_doc">https://huggingface.co/docs/transformers/model_doc</a></li><li><a href="https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast">https://huggingface.co/docs/transformers/model_doc&#x2F;gpt2#transformers.GPT2TokenizerFast</a></li><li><a href="https://sunniesuhyoung.github.io/files/LLM.pdf">https://sunniesuhyoung.github.io/files/LLM.pdf</a></li><li><a href="https://twitter.com/tomgoldsteincs/status/1600196995389366274?lang=en">https://twitter.com/tomgoldsteincs/status/1600196995389366274?lang=en</a></li><li><a href="https://arxiv.org/pdf/1910.02054.pdf">https://arxiv.org/pdf/1910.02054.pdf</a></li><li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html</a></li><li>Jaime Sevilla et al. (2022), “Estimating Training Compute of Deep Learning Models”. Published online at <a href="http://epochai.org/">epochai.org</a>. Retrieved from: ‘<a href="https://epochai.org/blog/estimating-training-compute">https://epochai.org/blog/estimating-training-compute</a>‘ [online resource]</li><li><a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a></li><li><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf</a></li><li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html</a></li><li><a href="https://aws.amazon.com/sagemaker/neo/">https://aws.amazon.com/sagemaker/neo/</a></li><li><a href="https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/7126bf7beed4c4c3a05bcc2dac8baa3c/pruning_tutorial.ipynb">https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads&#x2F;7126bf7beed4c4c3a05bcc2dac8baa3c&#x2F;pruning_tutorial.ipynb</a></li><li><a href="https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a">https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a</a></li><li><a href="https://aws.amazon.com/blogs/machine-learning/train-175-billion-parameter-nlp-models-with-model-parallel-additions-and-hugging-face-on-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/train-175-billion-parameter-nlp-models-with-model-parallel-additions-and-hugging-face-on-amazon-sagemaker/</a></li><li><a href="https://openai.com/blog/improving-language-model-behavior/">Improving Language Model Behavior by Training on a Curated Dataset</a></li><li><a href="https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33">https://towardsdatascience.com/how-to-deploy-large-size-deep-learning-models-into-production-66b851d17f33</a></li><li><a href="https://huggingface.co/blog/large-language-models">https://huggingface.co/blog/large-language-models</a></li><li><a href="https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/">https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/</a></li><li><a href="https://openreview.net/pdf?id=NiEtU7blzN">Large Language Models Can Self-Improve</a></li><li><a href="https://spot.io/resources/cloud-cost/cloud-cost-optimization-15-ways-to-optimize-your-cloud/">https://spot.io/resources/cloud-cost/cloud-cost-optimization-15-ways-to-optimize-your-cloud/</a></li><li><a href="https://dataintegration.info/choose-the-best-ai-accelerator-and-model-compilation-for-computer-vision-inference-with-amazon-sagemaker">https://dataintegration.info/choose-the-best-ai-accelerator-and-model-compilation-for-computer-vision-inference-with-amazon-sagemaker</a></li><li><a href="https://medium.com/data-science-at-microsoft/model-compression-and-optimization-why-think-bigger-when-you-can-think-smaller-216ec096f68b">https://medium.com/data-science-at-microsoft/model-compression-and-optimization-why-think-bigger-when-you-can-think-smaller-216ec096f68b</a></li><li><a href="https://medium.com/picsellia/how-to-optimize-computer-vision-models-for-edge-devices-851b20f7cf03">https://medium.com/picsellia/how-to-optimize-computer-vision-models-for-edge-devices-851b20f7cf03</a></li><li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/parallelism#which-strategy-to-use-when">https://huggingface.co/docs/transformers/v4.17.0/en/parallelism#which-strategy-to-use-when</a></li><li><a href="https://medium.com/@mlblogging.k/9-libraries-for-parallel-distributed-training-inference-of-deep-learning-models-5faa86199c1f">https:&#x2F;&#x2F;medium.com&#x2F;@mlblogging.k&#x2F;9-libraries-for-parallel-distributed-training-inference-of-deep-learning-models-5faa86199c1f</a></li><li><a href="https://towardsdatascience.com/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880">https://towardsdatascience.com/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI Infra</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt 工程进阶之Tree of Thoughts--风爷推荐-E07-Week24 2023</title>
    <link href="/2023/12/18/Prompt%20Engineering%20:%20Tree%20of%20Thoughts-E07-Week24%202023/"/>
    <url>/2023/12/18/Prompt%20Engineering%20:%20Tree%20of%20Thoughts-E07-Week24%202023/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本期和大家一起读篇论文，由Google DeepMind 和普林斯顿大学联合提出的「<a href="https://arxiv.org/pdf/2305.10601.pdf">思维树</a>」框架。该框架是COT(思维链)的泛化模型，它基于人类认知学理论，思维的快慢系统，提出了一种基于树模型的Prompt提示方法。</p><blockquote><p>整体感受，论文思路很好。首先 LLM的COT是激发LLM能力的最重要的策略方法（模型）之一。这篇文章提出了一种创新型的策略叫做TOT，实际上是COT的泛化(从数学上来讲这种泛化工作非常重要）。其次，它证明了这种TOT能够带来LLM模型能力的大幅提升。而且这个TOT有很好的认知科学的理论基础。 整个工作是solid 和 creative的。这篇文章其实很有用，对于做应用层来说，设计好的Prompt框架，能够使得GPT模型发挥更强大的能力。 – 拖延症拉满的小林</p></blockquote><p>文章宣称让思维树框架可以让GPT-4可以自己生成、评估和决策，推理能力最高可提升1750%。论文实验结果显示，ToT显著提高了LLM在三个新任务（24点游戏，创意写作，迷你填字游戏）中的问题解决能力。比如，在24点游戏中，GPT-4只解决了4％的任务，但ToT方法的成功率达到了74％。</p><blockquote><p>实际上，小林跑了下文章附带的代码，无法稳定复现文章宣称的效果，同时对比发现，在24点游戏里面，GPT-4不使用思维树可能效果更好。在GPT-3.5前提下，无论是否使用思维树，都很难求解24点游戏。 – 拖延症拉满的小林</p></blockquote><h1 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h1><h2 id="引文"><a href="#引文" class="headerlink" title="引文"></a>引文</h2><p>LLM，如GPT 和PaLM等大模型已经被证明在需要数学、符号、常识和知识推理的各种任务领域能够具备很好的性能和效果。但是，目前这些模型的所有进展的基础仍然是用于<strong>生成文本的原始自回归机制</strong>，这使得<strong>决策</strong>逐一在左到右的方式下进行。这样简单的机制是否足以构建一个通用问题求解器？如果不是，会有哪些问题挑战当前的范例，应该有哪些替代机制？</p><blockquote><p>这段拗口的话，其实也是杨尚昆怀疑的理论基础。目前所有大模型的基础是 自回归的Transformer，都是从左到右的预测（下一个词&#x2F;向量）。如果不了解从左到右，需要看一下Transformer的原理。 – 拖延症拉满的小林</p></blockquote><p>人类认知的文献提供了一些线索来回答这些问题。对“双过程”模型的研究表明，人们有两种模式来处理决策——快速、自动、无意识模式（“系统1”）和缓慢、深思熟虑、有意识模式（“系统2”）。这两种模式以前已经与机器学习中使用的各种数学模型联系起来。例如，对人类和其他动物进行的强化学习研究探讨了他们进行联想式“无模型”学习或更深思熟虑的“基于模型”的规划的情况。大模型的简单联想令牌级别选择也让人想起了“系统1”，因此可能会从一个更深思熟虑的“系统2”规划过程中获益，该过程（1）维护并探索当前选择的多种不同选择而不只是选择一个，并且（2）评估其当前状态并积极地向前或向后追溯以做出更全局性决策。</p><p><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230608220400.png"></p><p>图1：示意图展示了使用LLMs解决问题的各种方法。每个矩形框代表一个思考过程，它是一个连贯的语言序列，作为解决问题的中间步骤。</p><blockquote><p>从数据结构的角度来看，实际上Tree结构是 COT的泛化版本。在大模型的Prompt工程，主要基于COT和统计采样两种方法来提升大模型输出质量。TOT确实是一个不错的创新。  – 拖延症拉满的小林</p></blockquote><p>为了设计这样一个规划过程，我们回到了人工智能和认知科学的起源，Newell和同事将问题解决描述为对组合问题空间的搜索，该空间表示为一棵树。因此，我们提出了Tree of Thoughts（ToT）框架，用于使用语言模型进行通用问题解决。</p><p>正如图1所示，虽然现有方法（详见下文）会对问题解决进行连续语言序列采样，但ToT则积极维护着一棵思想树，在其中每个思想都是一个连贯的语言序列，作为通向问题解决的中间步骤。这样的高级语义单元允许LM通过一个推理过程来自我评估不同中间思想在解决问题方面所取得的进展，这种通过LM自我评估和思考实现搜索启发式是新颖的，因为以前的搜索启发式要么是编程或学习得到。</p><p>最后，我们将这种基于语言生成和评估多样化思想能力与搜索算法相结合，例如广度优先搜索（BFS）或深度优先搜索（DFS），这些算法通过前瞻和回溯的方式允许系统地探索思想树。</p><blockquote><p>树结构给Prompt工程更灵活的实现路径，但是设计难度也提升了。 – 拖延症拉满的小林</p></blockquote><p>现有方法在使用语言模型解决一般问题时存在的两个主要缺点：1）在局部上，它们不探索思维过程中不同的延续——树的分支。2）在全局上，它们不包括任何类型的规划、向前看或回溯来帮助评估这些不同选项——看起来具有人类问题解决特征的启发式搜索。</p><p>为了解决这些不足，我们引入了“思维树”（Tree of Thoughts，ToT）的范式，使语言模型能够在思想上探索多条推理路径（图1(c)）。ToT将任何问题框架化为在树上进行搜索，其中每个节点都是一个状态 s &#x3D; [x, z1···i]，表示具有输入和迄今为止的思考序列的部分解决方案。ToT的特定实例包括回答四个问题：<br>1.如何将中间过程分解成思考步骤；<br>2.如何从每个状态生成潜在的思想；<br>3.如何启发式地评估状态；<br>4.使用什么搜索算法。</p><p><strong>具体算法实现可以参考原文，这里就不赘述。</strong></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230612231644.png" alt="三种实验"></p><p>我们只看关于创意写作部分。</p><p>接下来，我们设计了一个创意写作任务，其中输入为4个随机句子，输出应为4个段落的连贯文章，分别以这4个输入句子结尾。这样的任务是开放性和探索性的，挑战了创造性思维和高层次的规划能力。</p><p><strong>任务设置</strong>：我们从randomwordgenerator.com中随机选取句子来形成100个输入，对于每个输入限制没有真实的文章参考。由于我们发现GPT-4大多数情况下可以遵循输入限制，因此我们专注于通过两种方式评估文章连贯性：使用GPT-4零-shot提示提供1-10标量分数或使用人类评判比较不同方法生成的输出对。对于前者，我们采样5个分数并对每个任务输出进行平均，并发现这5个分数通常一致，在所有输出中平均标准差约为0.56。对于后者，我们在盲目研究中采用作者子集来比较CoT与ToT生成的文章段落连贯性，其中段落顺序在100个输入上被随机颠倒。</p><p><strong>基线</strong>：鉴于该任务具有创意性质，IO和CoT提示都是zero-shot的。虽然前者提示LM直接根据输入限制生成连贯的文章，而后者提示LM首先制定简要计划，然后编写文章，即计划作为中间思考步骤。我们为每个任务生成10个IO和CoT样本。我们还考虑在每个任务的随机IO样本之上采用迭代细化（k≤5）方法，其中LM受到输入限制和最后生成的段落的影响来决定段落是否已经“完全连贯”，如果没有，则生成一个精细的段落。</p><p><strong>ToT设置</strong>：我们构建了一个深度为2（仅有1个中间思考步骤）的ToT——LM首先生成k&#x3D;5个计划，并投票选出最佳计划（下图的图4），然后类似地基于最佳计划生成k&#x3D;5个段落，然后投票选出最佳段落。这里广度限制b&#x3D;1，因为每一步只保留一个选择。使用简单的零-shot投票提示（“分析下面的选择，然后得出哪一个对指导最有前途”）在两个步骤中采样5个投票。</p><p><strong>结果</strong>。下图的图5（a）显示了100个任务中平均GPT-4分数，其中ToT（7.56）被认为比IO（6.19）和CoT（6.93）平均生成更连贯的段落。虽然这样的自动度量可能存在噪声，但下图的图5（b）通过显示人类在100个段落对中更喜欢ToT而不是CoT来证实这一发现，其中41个对中只有21个对首选CoT（其他38个对被发现“同样连贯”）。最后，迭代细化在这种自然语言任务上更有效，在其中将IO连贯性得分从6.19提高到7.67，并将ToT连贯性得分从7.56提高到7.91。我们认为它可以被视为ToT框架中的第三种思维生成方法，在该方法中，新思想可以从精炼旧思想而不是i.i.d或顺序生成中产生。</p><p><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230613153106.png"></p><p>![[Pasted image 20230613153106.png]]</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p><strong>限制和未来方向</strong>。像ToT这样的有意识搜索可能对于GPT-4已经擅长的许多现有任务并不必要，而作为一个初始步骤，这项工作只探索了三个相对简单的任务，挑战了GPT-4，并呼吁更好的搜索和规划能力与LM结合。然而，随着我们开始将LM用于更多实际的决策应用（例如编码、数据分析、机器人等），可能会出现更复杂的任务，并提供研究这些研究问题的新机会。此外，像ToT这样的搜索方法需要比采样方法更多的资源（例如GPT-4 API成本）才能提高任务性能，但是ToT的模块化灵活性允许用户自定义这种性能成本权衡，并且正在进行的开源努力[29]应该可以在不久的将来降低这些成本。最后，本文聚焦于使用现成LM，并使用类似ToT风格的高级反事实决策制定微调LM可能提供增强LM问题解决能力的机会。</p><p><strong>更广泛的影响</strong>。ToT是一个框架，使LM能够更自主、更智能地做出决策和解决问题。虽然当前任务局限于推理和搜索问题，但涉及与外部环境或人类互动的未来应用可能会带来潜在的危险，例如促进有害的LM使用。另一方面，ToT也提高了模型决策的可解释性和人类对齐的机会，因为所得到的表示是可读的、高级语言推理，而不是隐含的、低级别的令牌值。</p><p><strong>结论</strong>。LM的联想“系统1”可以通过基于搜索可能解决问题路径树的“系统2”进行有益增强。思维树框架提供了一种将经典问题解决见解转化为当代LM可行方法的方式。同时，LM解决了这些经典方法的一个弱点，提供了一种解决不容易形式化的复杂问题（例如创意写作）的方法。我们认为，LM与AI传统方法相交融是未来工作中令人兴奋的方向。</p><h1 id="代码与实现"><a href="#代码与实现" class="headerlink" title="代码与实现"></a>代码与实现</h1><p>坦诚讲，核心代码很少，其实实现的内容也比较简单。<br>Model.py，主要是调用GPT API。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">chatgpt</span>(<span class="hljs-params">messages, model=<span class="hljs-string">&quot;gpt-4&quot;</span>, temperature=<span class="hljs-number">0.7</span>, max_tokens=<span class="hljs-number">1000</span>, n=<span class="hljs-number">1</span>, stop=<span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-built_in">list</span>:<br><br><span class="hljs-keyword">global</span> completion_tokens, prompt_tokens<br><br>outputs = []<br><br><span class="hljs-keyword">while</span> n &gt; <span class="hljs-number">0</span>:<br><br>cnt = <span class="hljs-built_in">min</span>(n, <span class="hljs-number">20</span>)<br><br>n -= cnt<br><br>res = completions_with_backoff(model=model, messages=messages, temperature=temperature, max_tokens=max_tokens, n=cnt, stop=stop)<br><br>outputs.extend([choice[<span class="hljs-string">&quot;message&quot;</span>][<span class="hljs-string">&quot;content&quot;</span>] <span class="hljs-keyword">for</span> choice <span class="hljs-keyword">in</span> res[<span class="hljs-string">&quot;choices&quot;</span>]])<br><br><span class="hljs-comment"># log completion tokens</span><br><br>completion_tokens += res[<span class="hljs-string">&quot;usage&quot;</span>][<span class="hljs-string">&quot;completion_tokens&quot;</span>]<br><br>prompt_tokens += res[<span class="hljs-string">&quot;usage&quot;</span>][<span class="hljs-string">&quot;prompt_tokens&quot;</span>]<br><br><span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><p>Run.py，主要是定义了 生成、评估 和选择的过程。关键部分其实在Prompt 文本里面。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">solve</span>(<span class="hljs-params">args, task, idx, to_print=<span class="hljs-literal">True</span></span>):<br><br><span class="hljs-built_in">print</span>(gpt)<br><br>x = task.get_input(idx) <span class="hljs-comment"># input</span><br><br>ys = [<span class="hljs-string">&#x27;&#x27;</span>] <span class="hljs-comment"># current output candidates</span><br><br>infos = []<br><br><span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(task.steps):<br><br><span class="hljs-comment"># generation</span><br><br><span class="hljs-keyword">if</span> args.method_generate == <span class="hljs-string">&#x27;sample&#x27;</span>:<br><br>new_ys = [get_samples(task, x, y, args.n_generate_sample, prompt_sample=args.prompt_sample, stop=task.stops[step]) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> ys]<br><br><span class="hljs-keyword">elif</span> args.method_generate == <span class="hljs-string">&#x27;propose&#x27;</span>:<br><br>new_ys = [get_proposals(task, x, y) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> ys]<br><br>new_ys = <span class="hljs-built_in">list</span>(itertools.chain(*new_ys))<br><br>ids = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(new_ys)))<br><br><span class="hljs-comment"># evaluation</span><br><br><span class="hljs-keyword">if</span> args.method_evaluate == <span class="hljs-string">&#x27;vote&#x27;</span>:<br><br>values = get_votes(task, x, new_ys, args.n_evaluate_sample)<br><br><span class="hljs-keyword">elif</span> args.method_evaluate == <span class="hljs-string">&#x27;value&#x27;</span>:<br><br>values = get_values(task, x, new_ys, args.n_evaluate_sample)<br><br>  <br><br><span class="hljs-comment"># selection</span><br><br><span class="hljs-keyword">if</span> args.method_select == <span class="hljs-string">&#x27;sample&#x27;</span>:<br><br>ps = np.array(values) / <span class="hljs-built_in">sum</span>(values)<br><br>select_ids = np.random.choice(ids, size=args.n_select_sample, p=ps).tolist()<br><br><span class="hljs-keyword">elif</span> args.method_select == <span class="hljs-string">&#x27;greedy&#x27;</span>:<br><br>select_ids = <span class="hljs-built_in">sorted</span>(ids, key=<span class="hljs-keyword">lambda</span> x: values[x], reverse=<span class="hljs-literal">True</span>)[:args.n_select_sample]<br><br>select_new_ys = [new_ys[select_id] <span class="hljs-keyword">for</span> select_id <span class="hljs-keyword">in</span> select_ids]<br><br>  <br><br><br></code></pre></td></tr></table></figure><h1 id="读后感"><a href="#读后感" class="headerlink" title="读后感"></a>读后感</h1><ul><li>思路很好，大模型有巨大的潜力未被人挖掘，Prompt Engineering 是发掘大模型能力以及理解大模型机理的重要路径之一。</li><li>论文有较好的认知理论基础，通过模仿人类认知的快慢系统（# 思考，快与慢）<ul><li>我们的大脑有快与慢两种作决定的方式。常用的无意识的“系统1”依赖情感、记忆和经验迅速作出判断，它见闻广博，使我们能够迅速对眼前的情况作出反应。但系统1也很容易上当，它固守“眼见即为事实”的原则，任由损失厌恶和乐观偏见之类的错觉引导我们作出错误的选择。有意识的“系统2”通过调动注意力来分析和解决问题，并作出决定，它比较慢，不容易出错，但它很懒惰，经常走捷径，直接采纳系统1的直觉型判断结果。</li></ul></li><li>TOT是COT的泛化模式，复杂问题可以通过TOT来提升大模型答案质量。但是按照目前的代码效果测试来看，大模型本身输出有不稳定性，暂时没有稳定复现。</li><li>原文的<a href="https://github.com/princeton-nlp/tree-of-thought-llm">Github</a> 项目被李代桃僵<br><img src="https://s.zhangguiyi.cn/vent/gua.png" alt="有瓜吃"></li><li>值得大家回味的原文一段话，关于什么是认知。 A genuine problem-solving process involves the repeated use of available information to initiate exploration, which discloses, in turn, more information until a way to attain the solution is finally discovered.—— Newell et al.<blockquote><p>一个真正的问题解决过程涉及重复使用可用信息来启动探索，进而揭示更多信息，直到最终发现达到解决方案的方法。</p></blockquote></li></ul>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Prompt Engineering</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>向量数据库入门教程系列-1-基于pinecone实现语义搜索</title>
    <link href="/2023/12/18/Vector%20Database%20Tutorial%20Series%20-%201%20-%20Implementing%20Semantic%20Search%20with%20Pinecone/"/>
    <url>/2023/12/18/Vector%20Database%20Tutorial%20Series%20-%201%20-%20Implementing%20Semantic%20Search%20with%20Pinecone/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Pinecone简介"><a href="#1-Pinecone简介" class="headerlink" title="1. Pinecone简介"></a>1. Pinecone简介</h1><p>Pinecone是一个<strong>简单的云原生向量数据库</strong>，为高性能的AI应用提供长期记忆。它适用于涉及大模型(LLM)、生成式人工智能(AIGC)和语义搜索(Sematic Search)的应用。使用Pinecone，可以轻松存储和查询Vector ，提供优化的性能和实时分析能力。</p><h2 id="1-1-与Mysql对比"><a href="#1-1-与Mysql对比" class="headerlink" title="1.1 与Mysql对比"></a>1.1 与Mysql对比</h2><table><thead><tr><th>特性&#x2F;数据库</th><th>Pinecone</th><th>MySQL</th></tr></thead><tbody><tr><td><strong>数据类型</strong></td><td>向量数据</td><td>结构化数据（如文本、数字、日期等）</td></tr><tr><td><strong>数据结构</strong></td><td>无模式，数据以高维向量形式存储</td><td>表格结构，数据以行和列的形式组织</td></tr><tr><td><strong>索引</strong></td><td>为高维向量优化的索引</td><td>B树、哈希索引等</td></tr><tr><td><strong>存储方式</strong></td><td>分布式存储，云原生</td><td>可以是本地存储或分布式存储（如MySQL Cluster）</td></tr><tr><td><strong>查询复杂性</strong></td><td>主要用于相似性搜索</td><td>支持复杂的SQL查询，如连接、子查询等</td></tr><tr><td><strong>数据模型</strong></td><td>无模式</td><td>有模式，需要预先定义表、列和关系</td></tr></tbody></table><h2 id="1-2-Pinecone特点"><a href="#1-2-Pinecone特点" class="headerlink" title="1.2 Pinecone特点"></a>1.2 Pinecone特点</h2><ol><li><p>速度快:Pinecone利用了突破性的新算法,可以实现毫秒级的向量搜索,是传统搜索方法的100-1000倍速度。这使得它非常适合需要实时响应的应用。</p></li><li><p>易于集成:Pinecone提供了多种语言的客户端库,包括Python、Java、Go等,可以非常容易地在应用中集成Pinecone。并且支持主流机器学习框架,如TensorFlow、PyTorch等。</p></li><li><p>灵活可扩展:Pinecone使用了可水平扩展的分布式架构,可以根据需求轻松扩展搜索能力。并且提供了细粒度的可调参数,可以针对不同的应用场景进行优化。</p></li><li><p>可管理的SaaS:Pinecone提供了云托管的SaaS版本,开发者可以通过简单的WEB界面来管理索引和查询,无需自行部署和维护服务。</p></li></ol><h1 id="2-初识Pinecone"><a href="#2-初识Pinecone" class="headerlink" title="2. 初识Pinecone"></a>2. 初识Pinecone</h1><p>如前文所说，Pinecone是云原生的应用，它直接登陆网页就能使用。借助传统数据库的概念，我们来快速了解一下Pinecone的数据结构。</p><ul><li><strong>Pod</strong>：它是一个运行Pinecone服务的预配置硬件单元，你可以理解成一个运行Docker上的数据库实例。</li><li><strong>Index</strong>：可以理解成传统关系数据库的表（Table)。每个Index在Pinecone中都是一个独立的数据结构，用于存储和检索高维向量。每个索引都有其自己的数据集和相关的配置。查询和操作是针对特定Index进行的。</li><li><strong>Record</strong>: 可以理解成传统关系数据库的行(Row)。每个记录都有一个唯一的ID或键。</li></ul><p><strong>接下来进入实战：</strong></p><h2 id="2-1-数据库创建"><a href="#2-1-数据库创建" class="headerlink" title="2.1 数据库创建"></a>2.1 数据库创建</h2><p>构建Pinecone数据库是极其简单的，只需要在网页上点击几下，填写一些必要的参数即可。</p><h3 id="2-1-1-登陆注册"><a href="#2-1-1-登陆注册" class="headerlink" title="2.1.1 登陆注册"></a>2.1.1 登陆注册</h3><p><a href="https://app.pinecone.io/organizations">直接访问链接</a></p><p><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230817141430.png" alt="Pasted image 20230817141430"></p><h3 id="2-1-2-容器类型和索引-Pod-Type-Index"><a href="#2-1-2-容器类型和索引-Pod-Type-Index" class="headerlink" title="2.1.2 容器类型和索引(Pod Type&amp; Index )"></a>2.1.2 容器类型和索引(Pod Type&amp; Index )</h3><p>创建一个index，最核心的参数是维数，这个很多时候取决于你的数据大小。Pod Type如前文所言，服务实例的大小，我们免费用户只能选择starter这个机型。<br><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230820085241.png" alt="Pasted image 20230820085241"></p><p>可以看到我们创建了一个名字为test101的index，维度为384维。注意一开始这个vectors应该是零。<br><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230819230858.png" alt="Pasted image 20230819230858"><br>后续我们通过客户端向数据库上传了80000个vector到这个index。</p><h3 id="2-1-3-记录-record"><a href="#2-1-3-记录-record" class="headerlink" title="2.1.3 记录(record)"></a>2.1.3 记录(record)</h3><p>Pinecone索引中的每个记录record包含一个唯一的ID和一个表示密集向量嵌入的浮点数数组，可以看到数据结构实际上这样。</p><p><img src="https://s.zhangguiyi.cn/vent/pinecone_record.png"></p><p>Pinecone基本上是以JSON格式来对数据库做CURD，我们来看看一个最简单Record例子：<br><img src="https://s.zhangguiyi.cn/vent/20230820100515.png" alt="image.png"></p><p>这个Record由4个字段组成，分别是RecordID，可以理解为RowId，唯一标识符。Dense Vector和Sparse Vector都是指我们数据的向量表达方式。MetaData就是元数据。</p><h1 id="3-基于Pinecone实现语义搜索"><a href="#3-基于Pinecone实现语义搜索" class="headerlink" title="3 基于Pinecone实现语义搜索"></a>3 基于Pinecone实现语义搜索</h1><ul><li><strong>安装依赖包</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">!pip install -qU \<br><span class="hljs-string">&quot;pinecone-client[grpc]&quot;</span>==<span class="hljs-number">2.2</span><span class="hljs-number">.1</span> \<br>pinecone-datasets==<span class="hljs-string">&#x27;0.5.0rc11&#x27;</span> \<br>sentence-transformers==<span class="hljs-number">2.2</span><span class="hljs-number">.2</span><br></code></pre></td></tr></table></figure><p>我们跳过数据准备步骤，因为它们非常耗时，直接使用<em>Pinecone Datasets</em>中的预建数据集来进行操作。这次教程中使用的是quora_all-MiniLM-L6-bm25，主要是美国知乎的提问问题。</p><ul><li>*<strong>下载数据集</strong></li></ul><p>Starter这个pod支持写入10万条vector，我们选择8万条数据写入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pinecone_datasets <span class="hljs-keyword">import</span> load_dataset<br><br>dataset = load_dataset(<span class="hljs-string">&#x27;quora_all-MiniLM-L6-bm25&#x27;</span>)<br><span class="hljs-comment"># 删掉这次教程无关的字段</span><br>dataset.documents.drop([<span class="hljs-string">&#x27;metadata&#x27;</span>], axis=<span class="hljs-number">1</span>, inplace=<span class="hljs-literal">True</span>)<br>dataset.documents.rename(columns=&#123;<span class="hljs-string">&#x27;blob&#x27;</span>: <span class="hljs-string">&#x27;metadata&#x27;</span>&#125;, inplace=<span class="hljs-literal">True</span>)<br><span class="hljs-comment">#留下8万条记录</span><br>dataset.documents.drop(dataset.documents.index[<span class="hljs-number">320_000</span>:], inplace=<span class="hljs-literal">True</span>)<br>dataset.documents.drop(dataset.documents.index[:<span class="hljs-number">240_000</span>], inplace=<span class="hljs-literal">True</span>)<br><span class="hljs-comment">#展现10条数据</span><br>dataset.head(n=<span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><p>可以看到前10条数据<br><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230819224843.png" alt="Pasted image 20230819224843"></p><ul><li><p><strong>配置环境</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pinecone<br><br><span class="hljs-comment"># 找到你的PINECONE_API_KEY</span><br>PINECONE_API_KEY = os.environ.get(<span class="hljs-string">&#x27;PINECONE_API_KEY&#x27;</span>) <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;PINECONE_API_KEY&#x27;</span><br><span class="hljs-comment"># 找到你的PINECONE_ENVIRONMENT</span><br>PINECONE_ENV = os.environ.get(<span class="hljs-string">&#x27;PINECONE_ENVIRONMENT&#x27;</span>) <span class="hljs-keyword">or</span> <span class="hljs-string">&#x27;PINECONE_ENVIRONMENT&#x27;</span><br><br>pinecone.init(<br>api_key=PINECONE_API_KEY,<br>environment=PINECONE_ENV<br>)<br></code></pre></td></tr></table></figure><p>如下图所示：第一个红色圆圈为PINECONE_ENVIRONMENT的参数取值。第二个圆圈为PINECONE_API_KEY的参数取值。复制粘贴到上述代码里面替换成字符串即可。<br><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230819225008.png" alt="Pasted image 20230819225008"></p></li><li><p><strong>创建名为test101的index</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">index_name = <span class="hljs-string">&#x27;test101&#x27;</span><br></code></pre></td></tr></table></figure></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 只有index不存在的时候，才会新建</span><br><span class="hljs-keyword">if</span> index_name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> pinecone.list_indexes():<br>pinecone.create_index(<br>name=index_name,<br>dimension=<span class="hljs-built_in">len</span>(dataset.documents.iloc[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;values&#x27;</span>]),<br><span class="hljs-comment">#余弦相似度</span><br>metric=<span class="hljs-string">&#x27;cosine&#x27;</span><br>)<br><br><span class="hljs-comment"># 等一秒钟(wait for it)</span><br>time.sleep(<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 连接index</span><br>index = pinecone.GRPCIndex(index_name)<br></code></pre></td></tr></table></figure><p><strong>批量写入数据：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset.iter_documents(batch_size=<span class="hljs-number">100</span>):<br><br>index.upsert(batch)<br></code></pre></td></tr></table></figure><p> 可以看到写入的性能曲线，有个比较陡峭的峰，感觉性能并没有很稳定。另外，数据图表有个小bug，只显示有75000条，和上面的统计信息有所冲突。<br> <img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230819225744.png" alt="Pasted image 20230819225744"></p><p>Index的数据已经写入，可以开始进行查询了。语义搜索需要将文本转换成向量的形式，也就是我们通常所说的vector embeddings过程。在本教程里，我们采用<a href="https://www.sbert.net/">SentenceTransformer</a>来实现句子vector embeddings(向量嵌入)。<br>SentenceTransformer是一个功能强大的，可以生成句子、文本和图像的embeddings的python库。它提供了许多已经训练好的模型，用户可以使用这些模型进行文本相关任务，例如文本相似度计算、文本分类等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-comment">#默认用GPU</span><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><span class="hljs-comment">#展现下模型</span><br>model = SentenceTransformer(<span class="hljs-string">&#x27;all-MiniLM-L6-v2&#x27;</span>, device=device)<br>model<br></code></pre></td></tr></table></figure><p>可以看到模型的一些默认信息：</p><p><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230819230358.png" alt="Pasted image 20230819230358"></p><ul><li><strong>查询你的问题向量</strong><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">query = <span class="hljs-string">&quot;Who was the best Chinese King?&quot;</span><br><br><span class="hljs-comment"># 创建一个查询的向量</span><br>xq = model.encode(query).tolist()<br><br><span class="hljs-comment"># 查询</span><br>xc = index.query(xq, top_k=<span class="hljs-number">5</span>, include_metadata=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 逐一打印结果</span><br><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> xc[<span class="hljs-string">&#x27;matches&#x27;</span>]:<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">2</span>)&#125;</span>: <span class="hljs-subst">&#123;result[<span class="hljs-string">&#x27;metadata&#x27;</span>][<span class="hljs-string">&#x27;text&#x27;</span>]&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure></li></ul><p>结果如下图所示：数字表示相似度，展现了相似度排名前五的问题向量。<br><img src="https://s.zhangguiyi.cn/vent/Pasted%20image%2020230819230312.png" alt="Pasted image 20230819230312"></p><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><p>本文深入探讨了Pinecone，一款云原生向量数据库，强调了其与传统数据库如MySQL的差异。Pinecone的核心优势在于其高速、易集成、灵活性和云托管能力。通过实际示例，我们展示了如何使用Pinecone和SentenceTransformer进行语义搜索。在接下来的教程中，风爷将进一步探索如何结合向量数据库和大型模型来实现图片的语义搜索，为大家展示向量数据库更多的实际应用场景。</p><p>#blog #vectordb #ai </p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>vector databases</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>向量数据库入门教程系列-2-基于milvus实现图片搜索引擎</title>
    <link href="/2023/12/18/Vector%20Database%20Tutorial%20Series%20-%202%20-%20Building%20an%20Image%20Search%20Engine%20with%20Milvus/"/>
    <url>/2023/12/18/Vector%20Database%20Tutorial%20Series%20-%202%20-%20Building%20an%20Image%20Search%20Engine%20with%20Milvus/</url>
    
    <content type="html"><![CDATA[<blockquote><p>世界纷纷扰扰喧喧闹闹，什么是真实  – 最重要的小事</p></blockquote><p>拖延了一个多月的向量数据库系列终于来啦。这次分享的向量数据库是milvus，基于它我们构建一个以图搜图的搜索引擎。</p><h1 id="1-以图搜图技术流程简介"><a href="#1-以图搜图技术流程简介" class="headerlink" title="1.  以图搜图技术流程简介"></a>1.  以图搜图技术流程简介</h1><p><strong>以图搜图</strong> 是一种帮助你在给定输入图像的情况下搜索相似或相关图像的技术。以图搜图是一种基于内容的图像检索（<a href="https://en.wikipedia.org/wiki/Content-based_image_retrieval">CBIR</a>)）查询技术，它涉及提供一个查询图像给CBIR系统，系统将根据此查询进行搜索。</p><p>整个系统简化下来，基本上可以认为是下图的流程：</p><ul><li>输入一张图片 –&gt;  转化成 向量 –&gt; 写入向量数据库</li><li>输入待查询图片 –&gt; 转化成向量 —&gt; 查询向量数据库 –&gt; 返回最符合结果 –&gt; 展示<br><img src="https://s.zhangguiyi.cn/vent/202309111235417.png" alt="image.png"></li></ul><p>为了构建这样一个图像相似性搜索系统，我们需要下载包含17125张图像的PASCAL VOC图像集，其中包含20个类别。本教程使用YOLOv3进行目标检测和ResNet-50进行图像特征提取。经过这两个机器学习模型处理后，图像将被转换为256维的向量。<br>然后将这些向量存储在Milvus中，并由Milvus自动生成每个向量的唯一ID。然后使用MySQL将向量ID映射到数据集中的图像上。<br>每当您上传一个新的图像到图像搜索系统时，它将被转换为一个新的向量，并与之前存储在Milvus中的向量进行比较。然后Milvus返回最相似向量的ID，并且您可以在MySQL中查询相应的图像。</p><p><img src="https://s.zhangguiyi.cn/vent/20230915220554.png" alt="image.png"></p><h1 id="2-milvus和towhee简介"><a href="#2-milvus和towhee简介" class="headerlink" title="2. milvus和towhee简介"></a>2. milvus和towhee简介</h1><h2 id="2-1-milvus-–-又一个向量数据库"><a href="#2-1-milvus-–-又一个向量数据库" class="headerlink" title="2.1 milvus – 又一个向量数据库"></a>2.1 <strong>milvus – 又一个向量数据库</strong></h2><p>Milvus是一个开源的向量数据库，用于支持嵌入式相似性搜索和人工智能应用程序。Milvus使非结构化数据搜索更加便捷，并提供了一致的用户体验。<br>Milvus 2.0是一个云原生的向量数据库，通过设计将存储和计算分离。它具有毫秒级搜索、简化的非结构化数据管理、可靠的向量数据库等特点。Milvus还支持混合搜索、统一的Lambda结构、社区支持和业界认可等。用户可以通过Zilliz Cloud进行快速部署，也可以根据文档自行安装和使用Milvus。</p><p>和Pinecone不同的是，Milvus不但支持云原生，还是可以私有化部署的开源项目。被互联网大厂广泛采用。</p><p><img src="https://s.zhangguiyi.cn/vent/20230915221603.png" alt="image.png"></p><h2 id="2-2-towhee-–-万物皆向量"><a href="#2-2-towhee-–-万物皆向量" class="headerlink" title="2.2 towhee – 万物皆向量"></a>2.2 <strong>towhee – 万物皆向量</strong></h2><p><img src="https://s.zhangguiyi.cn/vent/towhee.png" alt="image.png"></p><p>Towhee是一个开源的机器学习流水线，可以帮助您将非结构化数据编码为嵌入向量。它具有易于使用的Python API，支持多种数据转换模式，并提供了快速的执行速度。此外，Towhee还提供了700多个预训练的嵌入模型，覆盖了5个领域、15个任务和140多种模型架构。它还与各种生态系统进行了完全集成，支持自定义数据处理流水线</p><h3 id="2-2-1-核心概念"><a href="#2-2-1-核心概念" class="headerlink" title="2.2.1 核心概念"></a>2.2.1 核心概念</h3><p>Towhee 由四个主要模块组成：“算子（Operators）”、“流水线（Pipelines）”、“数据处理 API（DataCollection API）”和“执行引擎（Engine）”。</p><ul><li><p><strong>算子（Operator）</strong>：算子是构成神经网络数据处理水流线(neural data processing pipeline)的“积木块”（基础组件）。这些基础组件按照任务类型进行组织，每种任务类型都具有标准的调用接口。一个算子可以是某种神经网络模型，某种数据处理方法，或是某个 Python 函数。</p></li><li><p><strong>流水线（Pipeline）</strong>：流水线是由若干个算子组成的 DAG（有向无环图）。流水线可以实现比单个算子更复杂的功能，诸如特征向量提取、数据标记、跨模态数据理解等。</p></li><li><p><strong>数据处理 API（DataCollection）</strong>: DataCollection API 是用于描述流水线的编程接口。提供多种数据转换接口：map, filter, flat_map, concat, window, time_window以及window_all，通过这些接口，可以快速构建复杂的数据处理管道，处理视频，音频，文本，图像等非结构化数据。</p></li><li><p><strong>执行引擎（Engine）</strong>: 执行引擎负责实例化流水线、任务调度、资源管理，以及运行期性能优化。面向快速原型构建，Towhee 提供了轻量级的本地执行引擎；面向生产环境需求，Towhee 提供了基于 Nvidia Triton 的高性能执行引擎。</p></li></ul><p>我们将学习如何使用Towhee构建图像搜索引擎。Towhee提供了用于非结构化数据的ETL（抽取、转换和加载）功能，同时集成了各种SOTA机器学习模型。它允许创建数据处理流水线，并提供了用于不同目的的内置算子，例如生成图像嵌入向量、将数据插入到Milvus集合中以及在Milvus集合上进行查询。<br><strong>接下来进入实战：</strong></p><h1 id="3-图片搜索实战"><a href="#3-图片搜索实战" class="headerlink" title="3. 图片搜索实战"></a>3. <a href="https://github.com/ultralytics/ultralytics/blob/main/README.zh-CN.md">图片搜索实战</a></h1><h2 id="3-1-准备"><a href="#3-1-准备" class="headerlink" title="3.1 准备"></a>3.1 准备</h2><p><strong>安装依赖库</strong><br>构建这个图片搜索引擎，我们需要先安装一些python包，下来一下样例数据，然后启动Milvus数据库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">!pip install towhee==<span class="hljs-number">0.5</span><span class="hljs-number">.0</span> opencv-python==<span class="hljs-number">4.5</span><span class="hljs-number">.3</span><span class="hljs-number">.56</span> pillow==<span class="hljs-number">8.2</span><span class="hljs-number">.0</span><br><br></code></pre></td></tr></table></figure><p><strong>下载样例数据</strong><br>接下来，我们需要一些样例的图片数据来做这个实验。通过下列命令下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">!wget https://github.com/tencent/towhee/releases/download/v0<span class="hljs-number">.5</span><span class="hljs-number">.0</span>/towhee_example_data.tar.gz<br>!tar -zxvf towhee_example_data.tar.gz<br></code></pre></td></tr></table></figure><p><strong>启动Milvus向量数据库docker</strong><br>我们用docker来构建我们的服务，这样无论是构建、可持续部署上都更便利。命令如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">!docker run -d --name milvus_cpu_0<span class="hljs-number">.11</span><span class="hljs-number">.0</span> \<br>-p <span class="hljs-number">19530</span>:<span class="hljs-number">19530</span> \<br>-p <span class="hljs-number">19121</span>:<span class="hljs-number">19121</span> \<br>-v /tmp/milvus/db:/var/lib/milvus/db \<br>-v /tmp/milvus/conf:/var/lib/milvus/conf \<br>-v /tmp/milvus/logs:/var/lib/milvus/logs \<br>-v /tmp/milvus/wal:/var/lib/milvus/wal \<br>milvusdb/milvus:<span class="hljs-number">0.11</span><span class="hljs-number">.0</span>-cpu-d030521-6f9e4a<br><br></code></pre></td></tr></table></figure><blockquote><p>可能需要先安装docker</p></blockquote><p>通过上述命令，我们能够把数据库实例启动，接下来我们开始配置数据库，开始代码开发工作。</p><h2 id="3-2-配置"><a href="#3-2-配置" class="headerlink" title="3.2 配置"></a>3.2 配置</h2><p>为了后续使用，我们在开始时导入包并设置参数。您可以根据您的需求和环境更改参数。请注意，嵌入维度<code>DIM</code>应与选择的模型名称<code>MODEL</code>匹配。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> csv<br><br><span class="hljs-keyword">from</span> glob <span class="hljs-keyword">import</span> glob<br><br><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><br><span class="hljs-keyword">from</span> statistics <span class="hljs-keyword">import</span> mean<br><br>  <br><br><span class="hljs-keyword">from</span> towhee <span class="hljs-keyword">import</span> pipe, ops, DataCollection<br><br><span class="hljs-keyword">from</span> pymilvus <span class="hljs-keyword">import</span> connections, FieldSchema, CollectionSchema, DataType, Collection, utility<br><br><br><span class="hljs-comment"># 模型参数，选用Resnet50</span><br><br>MODEL = <span class="hljs-string">&#x27;resnet50&#x27;</span><br><br>DEVICE = <span class="hljs-literal">None</span> <span class="hljs-comment"># None，使用默认设备(有GPU的话会用CUDA)</span><br><br>  <br><br><span class="hljs-comment"># 向量数据库默认参数</span><br><br>HOST = <span class="hljs-string">&#x27;127.0.0.1&#x27;</span><br><br>PORT = <span class="hljs-string">&#x27;19530&#x27;</span><br><br>TOPK = <span class="hljs-number">10</span><br><br>DIM = <span class="hljs-number">2048</span> <span class="hljs-comment"># </span><br><br>COLLECTION_NAME = <span class="hljs-string">&#x27;reverse_image_search&#x27;</span><br><br>INDEX_TYPE = <span class="hljs-string">&#x27;IVF_FLAT&#x27;</span><br><br>METRIC_TYPE = <span class="hljs-string">&#x27;L2&#x27;</span><br><br>  <br><br><span class="hljs-comment"># 搜索路径</span><br><br>INSERT_SRC = <span class="hljs-string">&#x27;reverse_image_search.csv&#x27;</span><br><br>QUERY_SRC = <span class="hljs-string">&#x27;./test/*/*.JPEG&#x27;</span><br></code></pre></td></tr></table></figure><p>默认情况下，本教程选择预训练模型’resnet50’来提取图像嵌入向量。它将<a href="https://milvus.io/docs/v2.0.x/index.md#IVF_FLAT">‘IVF_FLAT’</a>作为索引，并将<a href="https://milvus.io/docs/v2.0.x/metric.md#Euclidean-distance-L2">‘L2’</a>作为Milvus配置的距离度量。<code>TOPK</code>确定返回多少个搜索结果，默认为10个。<br>L2距离的公式，其实就是很简单的欧氏距离：<br><img src="https://s.zhangguiyi.cn/vent/202309210932993.png" alt="image.png"></p><h2 id="3-3-Embedding-流水线"><a href="#3-3-Embedding-流水线" class="headerlink" title="3.3 Embedding 流水线"></a>3.3 Embedding 流水线</h2><p>如前文所述，相似性搜索实际上是针对向量进行的。我们通过使用一个流式读取函数来读取图像所在的路径，将每张图片转换为embeddings向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入数据</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_image</span>(<span class="hljs-params">x</span>):<br><br><span class="hljs-keyword">if</span> x.endswith(<span class="hljs-string">&#x27;csv&#x27;</span>):<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(x) <span class="hljs-keyword">as</span> f:<br><br>reader = csv.reader(f)<br><br><span class="hljs-built_in">next</span>(reader)<br><br><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> reader:<br><span class="hljs-keyword">yield</span> item[<span class="hljs-number">1</span>]<br><br><span class="hljs-keyword">else</span>:<br><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> glob(x):<br><br><span class="hljs-keyword">yield</span> item<br><br><span class="hljs-comment"># 生成Embeding的工作流</span><br><br>p_embed = (pipe.<span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;src&#x27;</span>).flat_map(<span class="hljs-string">&#x27;src&#x27;</span>, <span class="hljs-string">&#x27;img_path&#x27;</span>, load_image)<br><br>.<span class="hljs-built_in">map</span>(<span class="hljs-string">&#x27;img_path&#x27;</span>, <span class="hljs-string">&#x27;img&#x27;</span>, ops.image_decode())<br><br>.<span class="hljs-built_in">map</span>(<span class="hljs-string">&#x27;img&#x27;</span>, <span class="hljs-string">&#x27;vec&#x27;</span>, ops.image_embedding.timm(model_name=MODEL, device=DEVICE)))<br><br></code></pre></td></tr></table></figure><p>[[Vector Database Tutorial Series - 1 - Implementing Semantic Search with Pinecone]]</p><h3 id="核心步骤"><a href="#核心步骤" class="headerlink" title="核心步骤"></a>核心步骤</h3><p>在完成上述工作后，我们准备构建并尝试图像搜索引擎的核心步骤。核心步骤包括3个步骤：</p><ol><li>创建Milvus集合</li><li>将数据插入到集合中</li><li>在数据库中查询图像</li></ol><h3 id="3-3-1-创建-Milvus集合-collection"><a href="#3-3-1-创建-Milvus集合-collection" class="headerlink" title="3.3.1 创建 Milvus集合 (collection)"></a>3.3.1 创建 Milvus集合 (collection)</h3><p>如同在传统关系数据库里面一样，我们需要新建一个数据库。在Mivus里面对应数据库(DataBase)概念的是集合(Collection)。在风爷前面的教程，[[向量数据库入门教程系列-1-基于pinecone实现语义搜索]]中提及到。</p><p>在插入或搜索数据之前，我们需要先创建一个集合。这一步使用上述的配置信息创建一个新的集合。请注意，如果集合已经存在，下面代码会先删除该集合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> milvus<br><br><span class="hljs-comment"># 创建Milvus集合</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_milvus_collection</span>(<span class="hljs-params">collection_name, dim</span>):<br>    <span class="hljs-comment"># 如果集合存在，先删除</span><br>    <span class="hljs-keyword">if</span> utility.has_collection(collection_name):<br>        utility.drop_collection(collection_name)<br><br>    <span class="hljs-comment"># 定义集合的字段</span><br>    fields = [<br>        FieldSchema(name=<span class="hljs-string">&#x27;path&#x27;</span>, dtype=DataType.VARCHAR, description=<span class="hljs-string">&#x27;图片路径&#x27;</span>, max_length=<span class="hljs-number">500</span>,<br>                    is_primary=<span class="hljs-literal">True</span>, auto_id=<span class="hljs-literal">False</span>),<br>        FieldSchema(name=<span class="hljs-string">&#x27;embedding&#x27;</span>, dtype=DataType.FLOAT_VECTOR, description=<span class="hljs-string">&#x27;图片嵌入向量&#x27;</span>, dim=dim)<br>    ]<br><br>    <span class="hljs-comment"># 定义集合的schema</span><br>    schema = CollectionSchema(fields=fields, description=<span class="hljs-string">&#x27;反向图像搜索&#x27;</span>)<br><br>    <span class="hljs-comment"># 创建集合</span><br>    collection = Collection(name=collection_name, schema=schema)<br><br>    <span class="hljs-comment"># 定义索引参数</span><br>    index_params = &#123;<br>        <span class="hljs-string">&#x27;metric_type&#x27;</span>: METRIC_TYPE,<br>        <span class="hljs-string">&#x27;index_type&#x27;</span>: INDEX_TYPE,<br>        <span class="hljs-string">&#x27;params&#x27;</span>: &#123;<span class="hljs-string">&quot;nlist&quot;</span>: <span class="hljs-number">2048</span>&#125;<br>    &#125;<br><br>    <span class="hljs-comment"># 在embedding字段上创建索引</span><br>    collection.create_index(field_name=<span class="hljs-string">&#x27;embedding&#x27;</span>, index_params=index_params)<br><br>    <span class="hljs-keyword">return</span> collection<br><br></code></pre></td></tr></table></figure><h3 id="3-3-2-数据插入到集合中"><a href="#3-3-2-数据插入到集合中" class="headerlink" title="3.3.2 数据插入到集合中"></a>3.3.2 数据插入到集合中</h3><h4 id="连接向量数据库"><a href="#连接向量数据库" class="headerlink" title="* 连接向量数据库"></a>* 连接向量数据库</h4><p>指定数据库地址和端口，还需要指定 集合名 和向量维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 连接到Milvus服务</span><br><br>connections.connect(host=HOST, port=PORT)<br><br><span class="hljs-comment"># 创建集合</span><br><br>collection = create_milvus_collection(COLLECTION_NAME, DIM)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;创建了一个新的集合: <span class="hljs-subst">&#123;COLLECTION_NAME&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><h4 id="数据写入流水线"><a href="#数据写入流水线" class="headerlink" title="* 数据写入流水线"></a>* 数据写入流水线</h4><p>这段代码是一个写入图片数据的流水线，主要用于将数据插入到Milvus集合中。具体实现是通过<code>p_embed</code>流水线中的<code>map</code>算子，将输入的图像路径和嵌入向量传递给<code>ops.ann_insert.milvus_client</code>算子，并指定Milvus数据库的主机、端口和集合名称。最后，通过<code>.output(&#39;mr&#39;)</code>将结果输出为一个标记为’mr’的数据流。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 写入数据</span><br><br>p_insert = (<br>    p_embed<br>    .<span class="hljs-built_in">map</span>((<span class="hljs-string">&#x27;img_path&#x27;</span>, <span class="hljs-string">&#x27;vec&#x27;</span>), <span class="hljs-string">&#x27;mr&#x27;</span>, ops.ann_insert.milvus_client(<br>        host=HOST,<br>        port=PORT,<br>        collection_name=COLLECTION_NAME<br>    ))<br>    .output(<span class="hljs-string">&#x27;mr&#x27;</span>)<br>)<br></code></pre></td></tr></table></figure><p>把所有图片写入这个集合–<code>INSERT_SRC</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 写入数据</span><br>p_insert(INSERT_SRC)<br><br><span class="hljs-comment"># 看看写了多少条数据</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Number of data inserted:&#x27;</span>, collection.num_entities)<br></code></pre></td></tr></table></figure><h3 id="3-3-3-以图搜图"><a href="#3-3-3-以图搜图" class="headerlink" title="3.3.3 以图搜图"></a>3.3.3 以图搜图</h3><p>通过muilvus和Towhee实现以图搜图是非常简单的，只需要构建一个 搜索的流水线(pipeline)即可。本质上是通过对Embeding向量的邻近搜索。下面的代码实现了一个简单的搜索功能，返回查询图像和搜索结果的路径。你可以修改<code>output()</code>函数来返回不同项的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 搜索流水线</span><br><br>p_search_pre = (<br>    p_embed.<span class="hljs-built_in">map</span>(<span class="hljs-string">&#x27;vec&#x27;</span>, (<span class="hljs-string">&#x27;search_res&#x27;</span>), ops.ann_search.milvus_client(<br>        host=HOST, port=PORT, limit=TOPK, collection_name=COLLECTION_NAME))<br>    .<span class="hljs-built_in">map</span>(<span class="hljs-string">&#x27;search_res&#x27;</span>, <span class="hljs-string">&#x27;pred&#x27;</span>, <span class="hljs-keyword">lambda</span> x: [<span class="hljs-built_in">str</span>(Path(y[<span class="hljs-number">0</span>]).resolve()) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> x])<br>    <span class="hljs-comment"># .output(&#x27;img_path&#x27;, &#x27;pred&#x27;)</span><br>)<br><br>p_search = p_search_pre.output(<span class="hljs-string">&#x27;img_path&#x27;</span>, <span class="hljs-string">&#x27;pred&#x27;</span>)<br></code></pre></td></tr></table></figure><ul><li><strong>找条小金鱼： ‘test&#x2F;goldfish&#x2F;*.JPEG’:</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入所需的库</span><br><span class="hljs-keyword">from</span> towhee <span class="hljs-keyword">import</span> collection, p_search, DataCollection<br><br><span class="hljs-comment"># 加载数据集合</span><br>collection.load()<br><br><span class="hljs-comment"># 搜索示例查询图像</span><br>dc = p_search(<span class="hljs-string">&#x27;test/goldfish/*.JPEG&#x27;</span>)<br><br><span class="hljs-comment"># 显示搜索结果的图像路径</span><br>DataCollection(dc).show()<br></code></pre></td></tr></table></figure><p>这段代码使用Towhee进行图像搜索。首先，加载数据集合。然后，使用<code>p_search</code>函数来搜索指定路径下的金鱼图像。最后，使用<code>DataCollection(dc).show()</code>来显示搜索结果的图像路径。</p><ul><li><strong>展示</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cv2  <span class="hljs-comment"># 导入OpenCV库</span><br><span class="hljs-keyword">from</span> towhee.types.image <span class="hljs-keyword">import</span> Image  <span class="hljs-comment"># 导入Towhee中的Image类型</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_images</span>(<span class="hljs-params">img_paths</span>):<br>    imgs = []  <span class="hljs-comment"># 存储读取的图片</span><br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> img_paths:<br>        imgs.append(Image(cv2.imread(p), <span class="hljs-string">&#x27;BGR&#x27;</span>))  <span class="hljs-comment"># 使用OpenCV读取图片，并创建Towhee中的Image对象</span><br>    <span class="hljs-keyword">return</span> imgs<br><br><span class="hljs-comment"># 创建一个数据处理流水线，将&#x27;pred&#x27;列的值映射为&#x27;pred_images&#x27;列的图像对象</span><br>p_search_img = (<br>    p_search_pre.<span class="hljs-built_in">map</span>(<span class="hljs-string">&#x27;pred&#x27;</span>, <span class="hljs-string">&#x27;pred_images&#x27;</span>, read_images)<br>    .output(<span class="hljs-string">&#x27;img&#x27;</span>, <span class="hljs-string">&#x27;pred_images&#x27;</span>)<br>)<br><br><span class="hljs-comment"># 创建一个DataCollection，将路径为&#x27;test/goldfish/*.JPEG&#x27;的图片数据作为输入，并显示结果</span><br>DataCollection(p_search_img(<span class="hljs-string">&#x27;test/goldfish/*.JPEG&#x27;</span>)).show()<br></code></pre></td></tr></table></figure></li></ul><p>具体流程如下：</p><ol><li><p>导入所需的库和模块。</p></li><li><p>定义一个函数<code>read_images</code>，用于读取指定路径下的图像文件并返回图像对象列表。</p></li><li><p>创建一个数据处理流水线<code>p_search_img</code>，使用<code>map</code>算子将’pred’列的值映射为’pred_images’列的图像对象。调用之前定义的<code>read_images</code>函数来实现图像读取和转换。</p></li><li><p>创建一个DataCollection对象，并将路径为’test&#x2F;goldfish&#x2F;*.JPEG’的图像数据作为输入。最后调用<code>.show()</code>方法显示结果。</p></li></ol><h2 id="4-评估系统"><a href="#4-评估系统" class="headerlink" title="4.评估系统"></a>4.评估系统</h2><p>我们通过上面的代码一个图像搜索引擎，实现了图像的写入和检索功能。然而，任何的机器学习相关系统，除了关注其可用性，也需要评估模型本身的性能，比如准确率、召回率等。不同的AI领域有着不同的评估指标，当然评估本身也是比较复杂的一个环节，尤其是大模型时代。本文则采用较为简单的<a href="https://www.educative.io/answers/what-is-the-mean-average-precision-in-information-retrieval">mAP</a>指标，即所有查询的平均精确度，来大致评估图像搜索引擎的效果。</p><h3 id="4-1-评估函数"><a href="#4-1-评估函数" class="headerlink" title="4.1 评估函数"></a>4.1 评估函数</h3><p>对于每个查询图像，我们希望从数据库中得到与之属于同一类别的图像。因此，我们定义了一个函数，根据查询图像的路径返回候选图像路径列表作为基准答案。基准答案应与查询图像具有相同的类别或类别。<br>此外，我们手动定义了一个函数来计算给定预测和期望结果的平均精度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><span class="hljs-keyword">import</span> glob<br><br><span class="hljs-comment"># 通过查询图像的路径获取真实结果</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ground_truth</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-comment"># 获取训练集路径，将&#x27;test&#x27;替换为&#x27;train&#x27;</span><br>    train_path = <span class="hljs-built_in">str</span>(Path(path).parent).replace(<span class="hljs-string">&#x27;test&#x27;</span>, <span class="hljs-string">&#x27;train&#x27;</span>)<br>    <span class="hljs-comment"># 使用glob函数获取训练集中所有图像的路径，并返回一个包含所有路径的列表</span><br>    <span class="hljs-keyword">return</span> [<span class="hljs-built_in">str</span>(Path(x).resolve()) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> glob.glob(train_path + <span class="hljs-string">&#x27;/*.JPEG&#x27;</span>)]<br><br><span class="hljs-comment"># 根据预测结果和真实结果计算平均准确率</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_ap</span>(<span class="hljs-params">pred: <span class="hljs-built_in">list</span>, gt: <span class="hljs-built_in">list</span></span>):<br>    ct = <span class="hljs-number">0</span>  <span class="hljs-comment"># 计数器，记录预测结果中与真实结果相同的数量</span><br>    score = <span class="hljs-number">0.0</span>  <span class="hljs-comment"># 得分，用于计算平均准确率</span><br>    <br>    <span class="hljs-keyword">for</span> i, n <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(pred):<br>        <span class="hljs-keyword">if</span> n <span class="hljs-keyword">in</span> gt:<br>            ct += <span class="hljs-number">1</span><br>        score += (ct / (i + <span class="hljs-number">1</span>))<br>        <br>    <span class="hljs-keyword">if</span> ct == <span class="hljs-number">0</span>:<br>        ap = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">else</span>:<br>        ap = score / ct<br>        <br>    <span class="hljs-keyword">return</span> ap<br></code></pre></td></tr></table></figure><p>代码中定义了两个函数：<code>ground_truth</code>和<code>get_ap</code>。</p><ul><li><p><code>ground_truth</code>函数通过查询图像的路径，返回与之对应的训练集图像路径列表。首先，使用<code>Path</code>将查询图像的路径转换为<code>Path</code>对象，并获取其父目录。然后，将父目录中的’test’替换为’train’，得到训练集图像所在目录。最后，使用<code>glob.glob</code>函数获取训练集目录下所有以’.JPEG’结尾的文件，并将其路径转换为绝对路径，并将所有路径存储在列表中返回。</p></li><li><p><code>get_ap</code>函数根据预测结果和真实结果计算平均准确率。首先，定义一个计数器<code>ct</code>和得分<code>score</code>，分别用于记录预测结果中与真实结果相同的数量和计算得分。然后，使用<code>enumerate</code>遍历预测结果列表，获取每个预测结果及其索引。如果当前预测结果在真实结果列表中，则将计数器加1。然后，计算得分并累加到总得分中。最后，根据计数器的值判断是否存在相同的预测结果和真实结果，并计算平均准确率。</p></li></ul><p>通过上述定义的函数，我们能够基于搜索流水线构建一个评估流水线。它对每个查询图片进行搜索，并将搜索结果与真实结果进行比较。然后，该流水线输出每个查询的平均精确度（AP）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 评估流水线返回平均精确率（Average Precision）</span><br><br>p_eval = (<br>    <span class="hljs-comment"># 通过映射操作获取预测结果、真实标签和基准数据</span><br>    p_search_pre.<span class="hljs-built_in">map</span>(<span class="hljs-string">&#x27;img_path&#x27;</span>, <span class="hljs-string">&#x27;gt&#x27;</span>, ground_truth)<br>    <br>    <span class="hljs-comment"># 通过映射操作计算平均精确率</span><br>    .<span class="hljs-built_in">map</span>((<span class="hljs-string">&#x27;pred&#x27;</span>, <span class="hljs-string">&#x27;gt&#x27;</span>), <span class="hljs-string">&#x27;ap&#x27;</span>, get_ap)<br>    <br>    <span class="hljs-comment"># 输出平均精确率</span><br>    .output(<span class="hljs-string">&#x27;ap&#x27;</span>)<br>)<br></code></pre></td></tr></table></figure><h3 id="4-3-搜索性能"><a href="#4-3-搜索性能" class="headerlink" title="4.3 搜索性能"></a>4.3 搜索性能</h3><p>现在我们可以对所有测试数据运行评估流水线。然后，我们计算所有查询的平均AP值，以获得mAP的最终性能。<br>最后，一张表记录了在我们的环境中测试的一些热门模型的性能和每秒查询率。您可以尝试使用不同的模型和配置。请注意，mAP和qps会受依赖项和设备版本的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-comment"># 导入时间模块</span><br><br><span class="hljs-comment"># 运行评估流水线，用于所有测试数据</span><br>start = time.time()  <span class="hljs-comment"># 记录开始时间</span><br><br>bm = p_eval(<span class="hljs-string">&#x27;test/*/*.JPEG&#x27;</span>)  <span class="hljs-comment"># 调用评估算子，传入测试数据路径</span><br><br>end = time.time()  <span class="hljs-comment"># 记录结束时间</span><br><br><span class="hljs-comment"># 将AP值存储在列表中</span><br>res = DataCollection(bm).to_list()  <span class="hljs-comment"># 将评估结果转换为列表</span><br><br><span class="hljs-comment"># 计算mAP值</span><br>mAP = mean([x.ap <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> res])  <span class="hljs-comment"># 计算平均准确率（mean average precision）</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;mAP@<span class="hljs-subst">&#123;TOPK&#125;</span>: <span class="hljs-subst">&#123;mAP&#125;</span>&#x27;</span>)  <span class="hljs-comment"># 打印mAP@TOPK值</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;qps: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(res) / (end - start)&#125;</span>&#x27;</span>)  <span class="hljs-comment"># 打印查询每秒请求数（query per second）</span><br></code></pre></td></tr></table></figure><p>上述代码是一个简单的示例，运行了一个评估流水线，并计算了平均准确率（mAP）和查询每秒请求数（qps）。其中使用了<code>time</code>模块记录了开始和结束时间，通过调用<code>p_eval</code>函数运行评估算子并传入测试数据路径。然后将评估结果转换为列表，并计算出平均准确率。最后打印出mAP@TOPK值和查询每秒请求数。</p><h4 id="性能表格如下"><a href="#性能表格如下" class="headerlink" title="性能表格如下:"></a>性能表格如下:</h4><table><thead><tr><th>model</th><th>dim</th><th>mAP@10</th><th>qps</th></tr></thead><tbody><tr><td>resnet50</td><td>2048</td><td>0.886</td><td>35</td></tr><tr><td>vgg16</td><td>512</td><td>0.658</td><td>53</td></tr><tr><td>vit_base_patch16_224</td><td>768</td><td>0.962</td><td>40</td></tr><tr><td>tf_efficientnet_b7</td><td>2560</td><td>0.983</td><td>16</td></tr></tbody></table><h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h1><p>整个教程虽然比较简单，但是已经比较完整的构建了以图搜图引擎的全过程。通过Towhee和Milvus构建的从数据预处理到搜索再到评估的机器学习流程，核心的组件就是向量数据库Milvus，当然Towhee封装了很多机器学习过程中的繁琐处理过程，使得构建类似的AI系统变得简单高效。本示例覆盖了图像搜索引擎的主要组成部分,为构建实际生产系统奠定了基础。下期风爷和大家一起探索如何将这个图片搜索引擎部署上线，提供公共服务。</p><p>#blog #vectordb #ai </p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>vector databases</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于自然语言的数据分析神器来了-风爷推荐-E05-Week15 2023</title>
    <link href="/2023/12/18/Natural%20Language-Based%20Data%20Analysis%20Tool-E05-Week15%202023/"/>
    <url>/2023/12/18/Natural%20Language-Based%20Data%20Analysis%20Tool-E05-Week15%202023/</url>
    
    <content type="html"><![CDATA[<p>#风爷推荐 #chatGPT #数据分析<br>许久不见，鸽了多期的风爷推荐又来了，这周依然是基于GPT-4的应用推荐。</p><p>作为数据科学领域从业者来说，这是我目前看到的GPT-4最有用的应用之一：<a href="https://app.akkio.com/">Akkio</a>，<strong>一个基于自然语言就能做数据分析的神器</strong>。</p><p>真是百感交集，即兴奋亢奋又瑟瑟发抖，最早被AI替代的永远是AI从业者。</p><p>老板以后再也不用招那么多数据分析、商分，自己用嘴就能数据分析。AI协助下，老板能够做出足够有模有样的洞察力的数据分析和数据报告，</p><p>不知道有没有一本历史书讲了，纺织工人在纺织机来临之后，去哪里打工了？</p><p>闲话少说，回到主题：</p><h1 id="1-探索数据集"><a href="#1-探索数据集" class="headerlink" title="1. 探索数据集"></a>1. 探索数据集</h1><ul><li>登录页面简单，直接邮箱就可以登录</li></ul><p><img src="https://s.zhangguiyi.cn/vent/202303311149782.png" alt="https://s.zhangguiyi.cn/vent/202303311149782.png"></p><ul><li>登录进来，可以看到如下图的几个样例数据科学工作流：</li></ul><p><img src="https://s.zhangguiyi.cn/vent/202304051954255.png" alt="https://s.zhangguiyi.cn/vent/202304051954255.png"></p><ul><li>点开其中一个，可以看到左侧，整个数据工作流，分成三部分：1. 数据表 2. 预测模型 3. AI服务。</li></ul><p><img src="https://s.zhangguiyi.cn/vent/202304051639206.png" alt="https://s.zhangguiyi.cn/vent/202304051639206.png"></p><p>vvvv</p><p><img src="https://s.zhangguiyi.cn/vent/202304051804565.png" alt="https://s.zhangguiyi.cn/vent/202304051804565.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051805399.png" alt="https://s.zhangguiyi.cn/vent/202304051805399.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051806007.png" alt="https://s.zhangguiyi.cn/vent/202304051806007.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051812711.png" alt="https://s.zhangguiyi.cn/vent/202304051812711.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051813108.png" alt="https://s.zhangguiyi.cn/vent/202304051813108.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051949343.png" alt="https://s.zhangguiyi.cn/vent/202304051949343.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051949241.png" alt="https://s.zhangguiyi.cn/vent/202304051949241.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051950308.png" alt="https://s.zhangguiyi.cn/vent/202304051950308.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051951190.png" alt="https://s.zhangguiyi.cn/vent/202304051951190.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051953076.png" alt="https://s.zhangguiyi.cn/vent/202304051953076.png"></p><p><img src="https://s.zhangguiyi.cn/vent/202304051958770.png" alt="https://s.zhangguiyi.cn/vent/202304051958770.png"></p><p>许久不见As an ML Engineer, this is one of the most useful applications of GPT-4 I’ve seen.</p><p>Chat Explore is a powerful AI-powered data exploration tool.</p><p>Here’s why I am so impressed:</p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Data Science</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AIGC浪潮</title>
    <link href="/2023/12/18/The%20Wave%20of%20AIGC/"/>
    <url>/2023/12/18/The%20Wave%20of%20AIGC/</url>
    
    <content type="html"><![CDATA[<h1 id="1-总览与分类"><a href="#1-总览与分类" class="headerlink" title="1.总览与分类"></a>1.总览与分类</h1><p><img src="https://s.zhangguiyi.cn/vent/202302131706872.png" alt="https://s.zhangguiyi.cn/vent/202302131706872.png"></p><h2 id="1-1-文本生成"><a href="#1-1-文本生成" class="headerlink" title="1.1 文本生成"></a>1.1 文本生成</h2><p><img src="https://s.zhangguiyi.cn/vent/202302211110491.png" alt="https://s.zhangguiyi.cn/vent/202302211110491.png"></p><h2 id="1-2-图像生成"><a href="#1-2-图像生成" class="headerlink" title="1.2 图像生成"></a>1.2 图像生成</h2><p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/97a4c645-30f9-4e37-a9ac-ae5849016f69/Untitled.png" alt="Untitled"></p><h2 id="1-3-音频、视频生成"><a href="#1-3-音频、视频生成" class="headerlink" title="1.3 音频、视频生成"></a>1.3 音频、视频生成</h2><h1 id="2-关键技术"><a href="#2-关键技术" class="headerlink" title="2. 关键技术"></a>2. 关键技术</h1><h2 id="2-1-LLM-以ChatGPT为例"><a href="#2-1-LLM-以ChatGPT为例" class="headerlink" title="2.1 LLM -以ChatGPT为例"></a>2.1 LLM -以ChatGPT为例</h2><p>LLM模型使得生成范式，即(M)LM（(Masked) Language Model），逐渐替代分类范式成为主流。从下图中可以看到，近几年(M)LM模型有逐渐统一语言模型范式的趋势.</p><p>这里的(M)LM主要指的是自回归式的给定前缀生成下一个单词的大规模模型，通常由解码器（decoder）实现，将MLM（Masked Language Model，掩码语言模型）包含进该概念是因为MLM本质也是预测（生成）单词的模型，只是包含了编码器（encoder）结构</p><p><img src="https://s.zhangguiyi.cn/vent/202302220024745.png" alt="https://s.zhangguiyi.cn/vent/202302220024745.png"></p><ul><li><p><strong>In-Context Learning</strong> :</p><p>对于一些LLM没有见过的新任务，只需要设计一些任务的语言描述，并给出几个任务实例，作为模型的输入，即可让模型从给定的情景中学习新任务并给出满意的回答结果。这种训练方式能够有效提升模型小样本学习（few-shot learning）的能力(补充图片)</p><p><img src="https://s.zhangguiyi.cn/vent/202302220021505.png" alt="https://s.zhangguiyi.cn/vent/202302220021505.png"></p></li></ul><p><a href="https://docs.cohere.ai/docs/prompt-engineering">Prompt Engineering</a></p><ul><li><p><strong>Chain-of-Thought，CoT</strong> :</p><p>对于一些逻辑较为复杂的问题，直接向大规模语言模型提问可能会得到不准确的回答，但是如果以提示（prompt）的方式在输入中给出有逻辑的解题步骤（即将复杂问题拆解为多个子问题解决再从中抽取答案）的示例后再提出问题，大模型就能给出正确题解。</p><p>如图所示，直接让模型进行数学题的计算会得到错误的结果，而引入解题过程则可以激发模型的推理能力，从而得到的正确的结果。</p><p><img src="https://s.zhangguiyi.cn/vent/202302220023028.png" alt="https://s.zhangguiyi.cn/vent/202302220023028.png"></p><p><a href="https://github.com/reasoning-machines/pal"></a><a href="https://github.com/reasoning-machines/pal">https://github.com/reasoning-machines/pal</a></p><p><a href="https://learnprompting.org/docs/advanced_applications/mrkl">🟡 LLMs Using Tools | Learn Prompting</a></p><p>有时，甚至不用给示例，在输入后面接一句“Let’s think step by step”，模型的输出就是一步一步“思考”后的各个子问题的结果，再将该输出拼到输入后构造第二次输入数据，大模型就能进一步将上一步的输出整合，得出正确的复杂问题的解。</p><p>目前有研究发现，由于数据集中存在的大量代码数据，得益于代码的强逻辑性，通过将问题中的文本内容替换为编程语言能够进一步提升模型的CoT能力（Program-aided Reasoning）。</p><p>由于CoT技术能够激发大规模语言模型对复杂问题的求解能力，该技术也被认为是打破比例定律的关键。</p><p><img src="https://s.zhangguiyi.cn/vent/202302220024812.png" alt="https://s.zhangguiyi.cn/vent/202302220024812.png"></p></li></ul><p><a href="https://twitter.com/arankomatsuzaki/status/1529278580189908993"></a><a href="https://twitter.com/arankomatsuzaki/status/1529278580189908993">https://twitter.com/arankomatsuzaki/status/1529278580189908993</a></p><ul><li><strong>Learning from Natural Instructions</strong></li></ul><p>这种训练方式会在输前面添加一个“指令（instruction）”，该指令能够以自然语言的形式描述任务内容，从而使得大模型根据输入来输出任务期望的答案。该方式将下游任务进一步和自然语言形式对齐，能显著提升模型对未知任务的泛化能力。</p><p><img src="https://s.zhangguiyi.cn/vent/202302220053512.png" alt="https://s.zhangguiyi.cn/vent/202302220053512.png"></p><p><a href="https://arxiv.org/pdf/2104.08773.pdf"></a><a href="https://arxiv.org/pdf/2104.08773.pdf">https://arxiv.org/pdf/2104.08773.pdf</a></p><p><a href="https://openai.com/blog/chatgpt/">ChatGPT: Optimizing Language Models for Dialogue</a></p><p>第一步：首先从大量的包含人类真实意图的指令集合中采样指令作为输入数据，并聘请专职标注员标注这些指令的输出，这部分相对而言是一个高质量的小数据集。数据收集完成后，使用GPT-3.5在该数据集上进行有监督的微调（supervised fine-tuning）。</p><p>第二步：得到上一步微调好的GPT-3.5之后，再次从指令集合中采样指令作为输入数据，从GPT-3.5得到多个不同的结果，并聘请专职标注员标注这些输出的好坏顺序，例如输出D&gt;输出C&gt;输出A&gt;输出B。由于只需要标注模型不同输出的好坏，这部分标注的成本会降低很多，因此数据规模也会较大。得到这些人工标注的输出顺序之后，可以训练得到一个打分（reward）模型。</p><p>第三步，获得打分模型之后，接着从指令集合中采样一些新的指令作为输入数据，并结合打分模型，使用PPO（强化学习算法）方式来训练得到最终的ChatGPT。</p><p><img src="https://s.zhangguiyi.cn/vent/ChatGPT_Diagram.svg" alt="https://s.zhangguiyi.cn/vent/ChatGPT_Diagram.svg"></p><h2 id="2-2-CLIP-Contrastive-Language-Image-Pre-training"><a href="#2-2-CLIP-Contrastive-Language-Image-Pre-training" class="headerlink" title="2.2 CLIP(Contrastive Language-Image Pre-training)"></a>2.2 CLIP(Contrastive Language-Image Pre-training)</h2><p><img src="/%5B%3Chttps:/s.zhangguiyi.cn/vent/202302211019244.png%3E%5D(%3Chttps:/s.zhangguiyi.cn/vent/202302211019244.png%3E)" alt="Contrastive Language-Image Pre-training"></p><p><a href="https://paperswithcode.com/method/clip"></a><a href="https://paperswithcode.com/method/clip">https://paperswithcode.com/method/clip</a></p><h2 id="2-3-LDM-Latent-Diffusion-Model"><a href="#2-3-LDM-Latent-Diffusion-Model" class="headerlink" title="2.3 LDM(Latent Diffusion Model)"></a>2.3 LDM(Latent Diffusion Model)</h2><h3 id="2-3-1-LoRA"><a href="#2-3-1-LoRA" class="headerlink" title="2.3.1 LoRA"></a>2.3.1 LoRA</h3><h3 id="2-3-2-CriticalNet"><a href="#2-3-2-CriticalNet" class="headerlink" title="2.3.2 CriticalNet"></a>2.3.2 CriticalNet</h3><h1 id="4-场景"><a href="#4-场景" class="headerlink" title="4. 场景"></a>4. 场景</h1><h2 id="4-1-工作提效"><a href="#4-1-工作提效" class="headerlink" title="4.1 工作提效"></a>4.1 工作提效</h2><h3 id="4-1-1-各类写作"><a href="#4-1-1-各类写作" class="headerlink" title="4.1.1 各类写作"></a>4.1.1 各类写作</h3><ul><li>代码</li><li>文档</li><li>文案</li></ul><h3 id="4-1-2-各类绘画"><a href="#4-1-2-各类绘画" class="headerlink" title="4.1.2 各类绘画"></a>4.1.2 各类绘画</h3><h2 id="4-2-个人助手"><a href="#4-2-个人助手" class="headerlink" title="4.2 个人助手"></a>4.2 个人助手</h2><h3 id="4-2-1-辅助决策"><a href="#4-2-1-辅助决策" class="headerlink" title="4.2.1 辅助决策"></a>4.2.1 辅助决策</h3><h3 id="4-2-2-情感诉求"><a href="#4-2-2-情感诉求" class="headerlink" title="4.2.2 情感诉求"></a>4.2.2 情感诉求</h3><h1 id="4-3-The-App"><a href="#4-3-The-App" class="headerlink" title="4.3 The App"></a>4.3 The App</h1><p>取代所有APP，取代大量白领创意性的工作，同时也创造出新的岗位。</p><h1 id="5-应用"><a href="#5-应用" class="headerlink" title="5.应用"></a>5.应用</h1><p>On Google ：Gmail &#x2F;Google Maps</p><p>On Apple : App Store</p><p>On AIGC : Jasper AI &#x2F; StableDiffusionWeb&#x2F;</p>]]></content>
    
    
    <categories>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AIGC</tag>
      
      <tag>OpenAI</tag>
      
      <tag>AGI</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
