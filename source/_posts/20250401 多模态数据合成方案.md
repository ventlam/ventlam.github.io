---
title: 20250401 多模态数据合成方案
categories: AI
date: 2025-04-01
tags:
  - AI
  - Multimodal
  - RAG
---
## 1. MMEvol

一个全新的多模态**指令数据演化**框架。它通过**细粒度感知、认知推理**和交互演化的精细组合，来迭代地提升数据质量，从而赋予多模态大模型更强的能力。

- **指令演化**：在每次生成数据时，随机选择不同层次（感知层、认知推理层或交互层）的演化方式，保证数据分布多样且覆盖面更广。
    
- **指令消除**：为解决在演化过程中少数生成指令存在错误或无效的情况，系统会自动对这些不合格指令进行过滤和剔除。

![](https://s.draftai.cn/vent/202504021117683.png)

#### 论文摘要：

[](https://huggingface.co/papers/2409.05840)[https://arxiv.org/pdf/2409.05840](https://arxiv.org/pdf/2409.05840)[](https://huggingface.co/papers/2409.05840)

> 多模态大型语言模型（MLLMs）的发展已经取得了显著进展。然而，多模态指令数据的数量和质量已成为其进展中的重大瓶颈。手动创建多模态指令数据既耗时又低效，给生成高复杂度的指令带来了挑战。此外，从黑箱商业模型（例如，GPT-4o，GPT-4V）提取指令数据通常会导致简单的指令数据，这限制了性能仅能达到这些模型的水平。策划多样化和复杂的指令数据的挑战仍然相当巨大。我们提出了 MMEvol，一个新颖的多模态指令数据演化框架，结合了细粒度感知演化、认知推理演化和交互演化。这种迭代方法突破了数据质量瓶颈，生成一个复杂且多样的图像-文本指令数据集，从而赋予 MLLMs 增强的能力。 从初始指令集 SEED-163K 开始，我们利用 MMEvol 系统地扩展指令类型的多样性，整合推理步骤以增强认知能力，并从图像中提取详细信息以改善视觉理解和鲁棒性。为了全面评估我们数据的有效性，我们使用进化数据训练 LLaVA-NeXT，并在 13 个视觉语言任务上进行实验。与使用种子数据训练的基线相比，我们的方法在这些任务中平均提高了 3.1 个百分点，并在 9 个任务上达到了最先进的（SOTA）性能。



## 2. STaR(Self-Teaching AI Reasonin)

STaR 旨在提升语言模型在**复杂推理**任务（如数学问题、常识问答）上的能力。

- **基本原理**：通过迭代的方式，利用少量带有推理示例（rationales）的数据，再结合大量**未带推理过程**的大规模语料，来引导模型不断改进复杂推理能力。
    
- **核心循环**：先让模型对少量带推理过程的数据进行学习，进而在大规模无推理标注的数据上推断，再根据推断结果不断微调、修正，形成一个简单高效的迭代流程。
![](https://s.draftai.cn/vent/202504021115058.png)

#### 论文摘要：
https://arxiv.org/pdf/2203.14465
>复杂推理任务（如数学或常识问答）上的表现。然而，目前诱导语言模型生成推理需要构建大量的推理数据集，或者通过仅使用少量示例推理来牺牲准确性。我们提出了一种技术，可以迭代地利用少量推理示例和一个没有推理的大数据集，以启动执行越来越复杂推理的能力。这种技术称为“自我学习推理器”（Self-Taught Reasoner，STaR），依赖于一个简单的循环：生成推理以回答许多问题，使用少量推理示例进行提示；如果生成的答案是错误的，则尝试在给定正确答案的情况下再次生成推理；对所有最终产生正确答案的推理进行微调；重复这一过程。我们展示了 STaR 在多个数据集上的表现显著优于直接预测最终答案的微调模型，并且在 CommensenseQA 上与微调一个大 30 × 的最先进语言模型的表现相当。 因此，STaR 使模型通过学习自身生成的推理来改进自己。

> 1. **自我教学机制**： STaR利用一个简单的循环过程，通过生成推理理由来回答问题。模型首先从少量的推理示例开始，然后在此基础上生成更多的推理，以提高其推理能力[[1](https://arxiv.org/abs/2203.14465)]。
>     
> 2. **链式思维**： STaR强调逐步生成“链式思维”推理，这种方法在处理复杂的推理任务（如数学问题或常识推理）时表现出色。通过这种方式，模型能够更清晰地展示其思考过程，从而提高回答的准确性[[2](https://openreview.net/pdf?id=_3ELRdg2sgI)]。
>     
> 3. **迭代改进**： STaR的一个重要特点是其迭代学习能力。模型在每次生成推理后，会根据之前的错误进行调整和改进，从而不断强化正确的推理路径[[3](https://medium.com/@sahin.samia/star-the-ai-that-teaches-itself-to-reason-a-game-changer-in-ai-development-9f7eba76c93a)]。
>     
> 4. **小样本学习**： STaR能够在仅依赖少量示例的情况下，提升模型的推理能力，而不需要庞大的数据集。这种方法使得模型在学习新任务时更加高效[[6](https://highlearningrate.substack.com/p/teaching-ai-to-think-the-self-taught)]。
>     

## 3. Dyn-VQA(方法 + 数据集 + 评测）

某种程度上说，Dyn-VQA 更偏向一个**Benchmark（**方法 + 数据集 + 评测**）**而非单纯的模型或方法。

为评估多模态 RAG（Retrieval-Augmented Generation）技术在**动态场景**中的表现，阿里团队构建了全新 **Dyn-VQA**：

1. **数据集构建**：Dyn-VQA数据集由1452个问题组成，这些问题被分为三类，旨在测试机器在处理动态问题时的能力[[1](https://arxiv.org/abs/2411.02937)][[9](https://www.themoonlight.io/de/review/benchmarking-multimodal-retrieval-augmented-generation-with-dynamic-vqa-dataset-and-self-adaptive-planning-agent)]。
![](https://s.draftai.cn/vent/202504020925193.png)
2. **动态问题**：与传统的视觉问答（VQA）任务不同，Dyn-VQA的问题需要根据不断变化的知识背景进行回答。这意味着答案可能会随着时间的推移而变化，增加了问题的复杂性[[5](https://arxiv.org/html/2411.02937v3)]。

3. [**多模态检索**](https://github.com/Alibaba-NLP/OmniSearch)：该数据集特别设计用于评估多模态检索增强生成方法的有效性。这些方法结合了文本和图像信息，以生成更准确的答案[[3](https://openreview.net/forum?id=VvDEuyVXkG)]。
![](https://s.draftai.cn/vent/202504020925795.png)





#### 论文摘要：

[https://arxiv.org/pdf/2411.02937](https://arxiv.org/pdf/2411.02937)

> 多模态检索增强生成（mRAG）在缓解多模态大语言模型（MLLMs）固有的“幻觉”问题中发挥着重要作用。尽管前景可期，现有的启发式 mRAG 通常预定义固定的检索过程，这导致了两个问题：（1）非自适应检索查询。（2）过载检索查询。然而，这些缺陷无法通过当前的知识寻求视觉问答（VQA）数据集充分反映，因为所需的知识可以通过标准的两步检索轻松获得。为了弥补数据集的差距，我们首先构建了 Dyn-VQA 数据集，包含三种类型的“动态”问题，这些问题需要在查询、工具和时间上变化的复杂知识检索策略：（1）答案快速变化的问题。（2）需要多模态知识的问题。（3）多跳问题。在 Dyn-VQA 上的实验表明，现有的启发式 mRAG 由于其僵化的检索过程，难以为动态问题提供足够且精确相关的知识。因此，我们进一步提出了首个用于多模态检索的自适应规划代理 OmniSearch。 基本思想是模拟人类在问题解决中的行为，动态地将复杂的多模态问题分解为带有检索动作的子问题链。

## 4. Florence-2

微软研究院曾推出视觉大模型 **Florence**，在图像分类、检测、分割和跨模态检索等方面都取得了不错的效果。**Florence-2** 则是该模型的升级版本，聚焦在以下几点：

- **更大规模的数据**：结合文本、图像、视频等多源数据，进一步扩充了跨模态训练集的规模和多样性。
    
- **改进的视觉骨干网络**：在保证推理速度的前提下，使用更加高效的骨干网络结构，提升对图像/视频内容的理解深度。
    
- **多模态任务统一**：在新版本中，可能将图像标注、图文检索、视觉问答等多模态任务纳入一个更统一的框架，以减少在任务间切换带来的模型适配成本。
    
![](https://s.draftai.cn/vent/202504020921219.png)

#### 论文摘要：

[](https://github.com/kijai/ComfyUI-Florence2)[https://arxiv.org/pdf/2311.06242](https://arxiv.org/pdf/2311.06242) [https://github.com/kijai/ComfyUI-Florence2](https://github.com/kijai/ComfyUI-Florence2)

> Florence-2，这是一种新颖的视觉基础模型，具有统一的基于提示的表示，适用于各种计算机视觉和视觉语言任务。虽然现有的大型视觉模型在迁移学习方面表现出色，但它们在处理简单指令的多样任务时却显得力不从心，这种能力意味着需要处理各种空间层次和语义粒度的复杂性。Florence-2 的设计旨在将文本提示作为任务指令，并生成所需的文本形式结果，无论是图像描述、物体检测、定位还是分割。这种多任务学习设置需要大规模、高质量的标注数据。为此，我们共同开发了 FLD-5B，包含了对 1.26 亿张图像的 54 亿条全面视觉注释，采用了自动图像注释和模型优化的迭代策略。我们采用了序列到序列的结构来训练 Florence-2，以执行多样化和全面的视觉任务。在众多任务上的广泛评估表明，Florence-2 是一种强大的视觉基础模型竞争者，具有前所未有的零样本和微调能力。

## 5. InternVL 2.5

**InternVL**（或 InternVideo/Intern 模型家族）是多模态领域系列研究项目，通常着重于**图像、视频与语言**的统一建模。版本 **2.5** 可能包含：

- **多模态预训练与对齐**：大规模的视觉-语言预训练，强调在图像和文本之间进行更细粒度的特征对齐（如区域级对齐或实体级对齐）。
    
- **跨模态检索与推理优化**：针对检索场景（例如带图片的电商商品检索）做了专项优化，提升在不同场景下的高准确度理解和回答能力。
    
- **模块化设计与可扩展性**：在框架上可能采用模块化设计，便于继续往视频、多语言等更多模态与场景扩充。
    
![](https://s.draftai.cn/vent/202504020918276.png)


#### 论文摘要：

[](https://github.com/kijai/ComfyUI-Florence2)[https://arxiv.org/abs/2412.05271](https://arxiv.org/abs/2412.05271) [https://huggingface.co/OpenGVLab/InternVL2_5-2B](https://huggingface.co/OpenGVLab/InternVL2_5-2B)

> InternVL 2.5，这是一个先进的多模态大型语言模型（MLLM）系列，基于 InternVL 2.0，保持其核心模型架构，同时在训练和测试策略以及数据质量方面引入了显著的增强。在这项工作中，我们深入探讨了模型规模与性能之间的关系，系统地研究了视觉编码器、语言模型、数据集大小和测试时间配置的性能趋势。通过对多种基准的广泛评估，包括多学科推理、文档理解、多图像/视频理解、现实世界理解、多模态幻觉检测、视觉定位、多语言能力和纯语言处理，InternVL 2.5 展现了竞争力的性能，媲美领先的商业模型如 GPT-4o 和 Claude-3.5-Sonnet。值得注意的是，我们的模型是第一个在 MMMU 基准上超过 70%的开源 MLLM，通过链式思维（CoT）推理实现了 3.7 点的提升，并展示了在测试时间扩展方面的强大潜力。

---

### 小结

1. **MMEvol**：强调对多模态指令数据的多层次“演化”与筛选，提高数据质量。
    
2. **STaR**：利用小规模带推理过程数据 + 大规模无推理数据的迭代训练策略，提升模型推理能力。
    
3. **Dyn-VQA**：关注在“动态”知识、上下文实时变化下的多模态问答评测。
    
4. **Florence-2**：微软视觉大模型 Florence 的进阶版本，数据更大、骨干网络更强，致力于统一多模态任务。
    
5. **InternVL 2.5**：多模态预训练与推理框架的新迭代版本，突出图像、视频与语言的对齐和检索能力，并具备良好的可扩展性。
    

通过以上 5 大方案的配合或组合，可以在多模态任务（如视觉问答、图文检索、复杂推理、动态场景问答等）中获得更完善、**更强大的数据合成与模型训练体系**。

它们分别在指令调优、推理能力迭代、动态知识处理以及大规模多模态数据利用等方面提供了可行的思路和实践范式。